[
  {
    "name": "ML-For-Beginners",
    "full_name": "microsoft/ML-For-Beginners",
    "description": "12 weeks, 26 lessons, 52 quizzes, classic Machine Learning for all",
    "url": "https://github.com/microsoft/ML-For-Beginners",
    "stars": 72006,
    "forks": 15544,
    "created_at": "2021-03-03",
    "updated_at": "2025-04-26",
    "topics": [
      "ml",
      "data-science",
      "machine-learning",
      "machine-learning-algorithms",
      "machinelearning",
      "python",
      "machinelearning-python",
      "scikit-learn",
      "scikit-learn-python",
      "r",
      "education",
      "microsoft-for-beginners"
    ],
    "primary_language": "HTML",
    "languages": {
      "HTML": 69312552,
      "Jupyter Notebook": 5468130,
      "Python": 54534,
      "Vue": 3931,
      "JavaScript": 1960,
      "Dockerfile": 1371,
      "CSS": 394
    },
    "readme_content": "[![GitHub license](https://img.shields.io/github/license/microsoft/ML-For-Beginners.svg)](https://github.com/microsoft/ML-For-Beginners/blob/master/LICENSE)\n[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/graphs/contributors/)\n[![GitHub issues](https://img.shields.io/github/issues/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/issues/)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/pulls/)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)\n\n[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/ML-For-Beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/ML-For-Beginners/watchers/)\n[![GitHub forks](https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ML-For-Beginners/network/)\n[![GitHub stars](https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ML-For-Beginners/stargazers/)\n\n[![](https://dcbadge.vercel.app/api/server/ByRwuEEgH4)](https://discord.gg/zxKYvhSnVp?WT.mc_id=academic-000002-leestott)\n\n# Machine Learning for Beginners - A Curriculum\n\n> 🌍 Travel around the world as we explore Machine Learning by means of world cultures 🌍\n\nCloud Advocates at Microsoft are pleased to offer a 12-week, 26-lesson curriculum all about **Machine Learning**. In this curriculum, you will learn about what is sometimes called **classic machine learning**, using primarily Scikit-learn as a library and avoiding deep learning, which is covered in our [AI for Beginners' curriculum](https://aka.ms/ai4beginners). Pair these lessons with our ['Data Science for Beginners' curriculum](https://aka.ms/ds4beginners), as well!\n\nTravel with us around the world as we apply these classic techniques to data from many areas of the world. Each lesson includes pre- and post-lesson quizzes, written instructions to complete the lesson, a solution, an assignment, and more. Our project-based pedagogy allows you to learn while building, a proven way for new skills to 'stick'.\n\n**✍️ Hearty thanks to our authors** Jen Looper, Stephen Howell, Francesca Lazzeri, Tomomi Imura, Cassie Breviu, Dmitry Soshnikov, Chris Noring, Anirban Mukherjee, Ornella Altunyan, Ruth Yakubu and Amy Boyd\n\n**🎨 Thanks as well to our illustrators** Tomomi Imura, Dasani Madipalli, and Jen Looper\n\n**🙏 Special thanks 🙏 to our Microsoft Student Ambassador authors, reviewers, and content contributors**, notably Rishit Dagli, Muhammad Sakib Khan Inan, Rohan Raj, Alexandru Petrescu, Abhishek Jaiswal, Nawrin Tabassum, Ioan Samuila, and Snigdha Agarwal\n\n**🤩 Extra gratitude to Microsoft Student Ambassadors Eric Wanjau, Jasleen Sondhi, and Vidushi Gupta for our R lessons!**\n\n# Getting Started\n\nFollow these steps:\n1. **Fork the Repository**: Click on the \"Fork\" button at the top-right corner of this page.\n2. **Clone the Repository**:   `git clone https://github.com/microsoft/ML-For-Beginners.git`\n\n> [find all additional resources for this course in our Microsoft Learn collection](https://learn.microsoft.com/en-us/collections/qrqzamz1nn2wx3?WT.mc_id=academic-77952-bethanycheum)\n\n\n**[Students](https://aka.ms/student-page)**, to use this curriculum, fork the entire repo to your own GitHub account and complete the exercises on your own or with a group:\n\n- Start with a pre-lecture quiz.\n- Read the lecture and complete the activities, pausing and reflecting at each knowledge check.\n- Try to create the projects by comprehending the lessons rather than running the solution code; however that code is available in the `/solution` folders in each project-oriented lesson.\n- Take the post-lecture quiz.\n- Complete the challenge.\n- Complete the assignment.\n- After completing a lesson group, visit the [Discussion Board](https://github.com/microsoft/ML-For-Beginners/discussions) and \"learn out loud\" by filling out the appropriate PAT rubric. A 'PAT' is a Progress Assessment Tool that is a rubric you fill out to further your learning. You can also react to other PATs so we can learn together.\n\n> For further study, we recommend following these [Microsoft Learn](https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-77952-leestott) modules and learning paths.\n\n**Teachers**, we have [included some suggestions](for-teachers.md) on how to use this curriculum.\n\n---\n\n## Video walkthroughs\n\nSome of the lessons are available as short form video. You can find all these in-line in the lessons, or on the [ML for Beginners playlist on the Microsoft Developer YouTube channel](https://aka.ms/ml-beginners-videos) by clicking the image below.\n\n[![ML for beginners banner](./ml-for-beginners-video-banner.png)](https://aka.ms/m...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "superset",
    "full_name": "apache/superset",
    "description": "Apache Superset is a Data Visualization and Data Exploration Platform",
    "url": "https://github.com/apache/superset",
    "stars": 65914,
    "forks": 14911,
    "created_at": "2015-07-21",
    "updated_at": "2025-04-26",
    "topics": [
      "superset",
      "apache",
      "apache-superset",
      "data-visualization",
      "data-viz",
      "analytics",
      "business-intelligence",
      "data-science",
      "data-engineering",
      "asf",
      "bi",
      "business-analytics",
      "data-analytics",
      "data-analysis",
      "python",
      "react",
      "sql-editor",
      "flask"
    ],
    "primary_language": "Jupyter Notebook",
    "languages": {
      "Jupyter Notebook": 10916999,
      "TypeScript": 10525974,
      "Python": 8815029,
      "JavaScript": 1881652,
      "HTML": 1342551,
      "Shell": 65640,
      "Less": 56942,
      "Dockerfile": 12025,
      "Jinja": 5847,
      "Smarty": 5044,
      "CSS": 4808,
      "Makefile": 4133,
      "Pug": 2969,
      "Mako": 1197
    },
    "readme_content": "<!--\nLicensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n\n  http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n-->\n\n# Superset\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/license/apache-2-0)\n[![Latest Release on Github](https://img.shields.io/github/v/release/apache/superset?sort=semver)](https://github.com/apache/superset/releases/latest)\n[![Build Status](https://github.com/apache/superset/actions/workflows/superset-python-unittest.yml/badge.svg)](https://github.com/apache/superset/actions)\n[![PyPI version](https://badge.fury.io/py/apache_superset.svg)](https://badge.fury.io/py/apache_superset)\n[![Coverage Status](https://codecov.io/github/apache/superset/coverage.svg?branch=master)](https://codecov.io/github/apache/superset)\n[![PyPI](https://img.shields.io/pypi/pyversions/apache_superset.svg?maxAge=2592000)](https://pypi.python.org/pypi/apache_superset)\n[![Get on Slack](https://img.shields.io/badge/slack-join-orange.svg)](http://bit.ly/join-superset-slack)\n[![Documentation](https://img.shields.io/badge/docs-apache.org-blue.svg)](https://superset.apache.org)\n\n<picture width=\"500\">\n  <source\n    width=\"600\"\n    media=\"(prefers-color-scheme: dark)\"\n    src=\"https://superset.apache.org/img/superset-logo-horiz-dark.svg\"\n    alt=\"Superset logo (dark)\"\n  />\n  <img\n    width=\"600\"\n    src=\"https://superset.apache.org/img/superset-logo-horiz-apache.svg\"\n    alt=\"Superset logo (light)\"\n  />\n</picture>\n\nA modern, enterprise-ready business intelligence web application.\n\n[**Why Superset?**](#why-superset) |\n[**Supported Databases**](#supported-databases) |\n[**Installation and Configuration**](#installation-and-configuration) |\n[**Release Notes**](https://github.com/apache/superset/blob/master/RELEASING/README.md#release-notes-for-recent-releases) |\n[**Get Involved**](#get-involved) |\n[**Contributor Guide**](#contributor-guide) |\n[**Resources**](#resources) |\n[**Organizations Using Superset**](https://github.com/apache/superset/blob/master/RESOURCES/INTHEWILD.md)\n\n## Why Superset?\n\nSuperset is a modern data exploration and data visualization platform. Superset can replace or augment proprietary business intelligence tools for many teams. Superset integrates well with a variety of data sources.\n\nSuperset provides:\n\n- A **no-code interface** for building charts quickly\n- A powerful, web-based **SQL Editor** for advanced querying\n- A **lightweight semantic layer** for quickly defining custom dimensions and metrics\n- Out of the box support for **nearly any SQL** database or data engine\n- A wide array of **beautiful visualizations** to showcase your data, ranging from simple bar charts to geospatial visualizations\n- Lightweight, configurable **caching layer** to help ease database load\n- Highly extensible **security roles and authentication** options\n- An **API** for programmatic customization\n- A **cloud-native architecture** designed from the ground up for scale\n\n## Screenshots & Gifs\n\n**Video Overview**\n\n<!-- File hosted here https://github.com/apache/superset-site/raw/lfs/superset-video-4k.mp4 -->\n\n[superset-video-1080p.webm](https://github.com/user-attachments/assets/b37388f7-a971-409c-96a7-90c4e31322e6)\n\n<br/>\n\n**Large Gallery of Visualizations**\n\n<kbd><img title=\"Gallery\" src=\"https://superset.apache.org/img/screenshots/gallery.jpg\"/></kbd><br/>\n\n**Craft Beautiful, Dynamic Dashboards**\n\n<kbd><img title=\"View Dashboards\" src=\"https://superset.apache.org/img/screenshots/slack_dash.jpg\"/></kbd><br/>\n\n**No-Code Chart Builder**\n\n<kbd><img title=\"Slice & dice your data\" src=\"https://superset.apache.org/img/screenshots/explore.jpg\"/></kbd><br/>\n\n**Powerful SQL Editor**\n\n<kbd><img title=\"SQL Lab\" src=\"https://superset.apache.org/img/screenshots/sql_lab.jpg\"/></kbd><br/>\n\n## Supported Databases\n\nSuperset can query data from any SQL-speaking datastore or data engine (Presto, Trino, Athena, [and more](https://superset.apache.org/docs/configuration/databases)) that has a Python DB-API driver and a SQLAlchemy dialect.\n\nHere are some of the major database solutions that are supported:\n\n<p align=\"center\">\n  <img src=\"https://superset.apache.org/img/databases/redshift.png\" alt=\"redshift\" border=\"0\" width=\"200\"/>\n  <img src=\"https://superset.apache.org/img/databases/google-biquery.png\" alt=\"google-biquery\" border=\"0\" width=\"200\"/>\n  <img src=\"ht...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "keras",
    "full_name": "keras-team/keras",
    "description": "Deep Learning for humans",
    "url": "https://github.com/keras-team/keras",
    "stars": 62921,
    "forks": 19575,
    "created_at": "2015-03-28",
    "updated_at": "2025-04-26",
    "topics": [
      "deep-learning",
      "tensorflow",
      "neural-networks",
      "machine-learning",
      "data-science",
      "python",
      "jax",
      "pytorch"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 8800074,
      "Shell": 3904
    },
    "readme_content": "# Keras 3: Deep Learning for Humans\n\nKeras 3 is a multi-backend deep learning framework, with support for JAX, TensorFlow, PyTorch, and OpenVINO (for inference-only).\nEffortlessly build and train models for computer vision, natural language processing, audio processing,\ntimeseries forecasting, recommender systems, etc.\n\n- **Accelerated model development**: Ship deep learning solutions faster thanks to the high-level UX of Keras\nand the availability of easy-to-debug runtimes like PyTorch or JAX eager execution.\n- **State-of-the-art performance**: By picking the backend that is the fastest for your model architecture (often JAX!),\nleverage speedups ranging from 20% to 350% compared to other frameworks. [Benchmark here](https://keras.io/getting_started/benchmarks/).\n- **Datacenter-scale training**: Scale confidently from your laptop to large clusters of GPUs or TPUs.\n\nJoin nearly three million developers, from burgeoning startups to global enterprises, in harnessing the power of Keras 3.\n\n\n## Installation\n\n### Install with pip\n\nKeras 3 is available on PyPI as `keras`. Note that Keras 2 remains available as the `tf-keras` package.\n\n1. Install `keras`:\n\n```\npip install keras --upgrade\n```\n\n2. Install backend package(s).\n\nTo use `keras`, you should also install the backend of choice: `tensorflow`, `jax`, or `torch`.\nNote that `tensorflow` is required for using certain Keras 3 features: certain preprocessing layers\nas well as `tf.data` pipelines.\n\n### Local installation\n\n#### Minimal installation\n\nKeras 3 is compatible with Linux and MacOS systems. For Windows users, we recommend using WSL2 to run Keras.\nTo install a local development version:\n\n1. Install dependencies:\n\n```\npip install -r requirements.txt\n```\n\n2. Run installation command from the root directory.\n\n```\npython pip_build.py --install\n```\n\n3. Run API generation script when creating PRs that update `keras_export` public APIs:\n\n```\n./shell/api_gen.sh\n```\n\n#### Adding GPU support\n\nThe `requirements.txt` file will install a CPU-only version of TensorFlow, JAX, and PyTorch. For GPU support, we also\nprovide a separate `requirements-{backend}-cuda.txt` for TensorFlow, JAX, and PyTorch. These install all CUDA\ndependencies via `pip` and expect a NVIDIA driver to be pre-installed. We recommend a clean python environment for each\nbackend to avoid CUDA version mismatches. As an example, here is how to create a Jax GPU environment with `conda`:\n\n```shell\nconda create -y -n keras-jax python=3.10\nconda activate keras-jax\npip install -r requirements-jax-cuda.txt\npython pip_build.py --install\n```\n\n## Configuring your backend\n\nYou can export the environment variable `KERAS_BACKEND` or you can edit your local config file at `~/.keras/keras.json`\nto configure your backend. Available backend options are: `\"tensorflow\"`, `\"jax\"`, `\"torch\"`, `\"openvino\"`. Example:\n\n```\nexport KERAS_BACKEND=\"jax\"\n```\n\nIn Colab, you can do:\n\n```python\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n\nimport keras\n```\n\n**Note:** The backend must be configured before importing `keras`, and the backend cannot be changed after \nthe package has been imported.\n\n**Note:** The OpenVINO backend is an inference-only backend, meaning it is designed only for running model\npredictions using `model.predict()` method.\n\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/).\n",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "scikit-learn",
    "full_name": "scikit-learn/scikit-learn",
    "description": "scikit-learn: machine learning in Python",
    "url": "https://github.com/scikit-learn/scikit-learn",
    "stars": 61858,
    "forks": 25776,
    "created_at": "2010-08-17",
    "updated_at": "2025-04-26",
    "topics": [
      "machine-learning",
      "python",
      "statistics",
      "data-science",
      "data-analysis"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 12378245,
      "Cython": 728558,
      "C++": 147428,
      "Shell": 47591,
      "C": 41895,
      "Meson": 32407,
      "CSS": 11277,
      "Makefile": 1034
    },
    "readme_content": ".. -*- mode: rst -*-\n\n|Azure| |Codecov| |CircleCI| |Nightly wheels| |Ruff| |PythonVersion| |PyPi| |DOI| |Benchmark|\n\n.. |Azure| image:: https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=main\n   :target: https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&branchName=main\n\n.. |CircleCI| image:: https://circleci.com/gh/scikit-learn/scikit-learn/tree/main.svg?style=shield\n   :target: https://circleci.com/gh/scikit-learn/scikit-learn\n\n.. |Codecov| image:: https://codecov.io/gh/scikit-learn/scikit-learn/branch/main/graph/badge.svg?token=Pk8G9gg3y9\n   :target: https://codecov.io/gh/scikit-learn/scikit-learn\n\n.. |Nightly wheels| image:: https://github.com/scikit-learn/scikit-learn/workflows/Wheel%20builder/badge.svg?event=schedule\n   :target: https://github.com/scikit-learn/scikit-learn/actions?query=workflow%3A%22Wheel+builder%22+event%3Aschedule\n\n.. |Ruff| image:: https://img.shields.io/badge/code%20style-ruff-000000.svg\n   :target: https://github.com/astral-sh/ruff\n\n.. |PythonVersion| image:: https://img.shields.io/pypi/pyversions/scikit-learn.svg\n   :target: https://pypi.org/project/scikit-learn/\n\n.. |PyPi| image:: https://img.shields.io/pypi/v/scikit-learn\n   :target: https://pypi.org/project/scikit-learn\n\n.. |DOI| image:: https://zenodo.org/badge/21369/scikit-learn/scikit-learn.svg\n   :target: https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn\n\n.. |Benchmark| image:: https://img.shields.io/badge/Benchmarked%20by-asv-blue\n   :target: https://scikit-learn.org/scikit-learn-benchmarks\n\n.. |PythonMinVersion| replace:: 3.10\n.. |NumPyMinVersion| replace:: 1.22.0\n.. |SciPyMinVersion| replace:: 1.8.0\n.. |JoblibMinVersion| replace:: 1.2.0\n.. |ThreadpoolctlMinVersion| replace:: 3.1.0\n.. |MatplotlibMinVersion| replace:: 3.5.0\n.. |Scikit-ImageMinVersion| replace:: 0.19.0\n.. |PandasMinVersion| replace:: 1.4.0\n.. |SeabornMinVersion| replace:: 0.9.0\n.. |PytestMinVersion| replace:: 7.1.2\n.. |PlotlyMinVersion| replace:: 5.14.0\n\n.. image:: https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/doc/logos/scikit-learn-logo.png\n  :target: https://scikit-learn.org/\n\n**scikit-learn** is a Python module for machine learning built on top of\nSciPy and is distributed under the 3-Clause BSD license.\n\nThe project was started in 2007 by David Cournapeau as a Google Summer\nof Code project, and since then many volunteers have contributed. See\nthe `About us <https://scikit-learn.org/dev/about.html#authors>`__ page\nfor a list of core contributors.\n\nIt is currently maintained by a team of volunteers.\n\nWebsite: https://scikit-learn.org\n\nInstallation\n------------\n\nDependencies\n~~~~~~~~~~~~\n\nscikit-learn requires:\n\n- Python (>= |PythonMinVersion|)\n- NumPy (>= |NumPyMinVersion|)\n- SciPy (>= |SciPyMinVersion|)\n- joblib (>= |JoblibMinVersion|)\n- threadpoolctl (>= |ThreadpoolctlMinVersion|)\n\n=======\n\nScikit-learn plotting capabilities (i.e., functions start with ``plot_`` and\nclasses end with ``Display``) require Matplotlib (>= |MatplotlibMinVersion|).\nFor running the examples Matplotlib >= |MatplotlibMinVersion| is required.\nA few examples require scikit-image >= |Scikit-ImageMinVersion|, a few examples\nrequire pandas >= |PandasMinVersion|, some examples require seaborn >=\n|SeabornMinVersion| and plotly >= |PlotlyMinVersion|.\n\nUser installation\n~~~~~~~~~~~~~~~~~\n\nIf you already have a working installation of NumPy and SciPy,\nthe easiest way to install scikit-learn is using ``pip``::\n\n    pip install -U scikit-learn\n\nor ``conda``::\n\n    conda install -c conda-forge scikit-learn\n\nThe documentation includes more detailed `installation instructions <https://scikit-learn.org/stable/install.html>`_.\n\n\nChangelog\n---------\n\nSee the `changelog <https://scikit-learn.org/dev/whats_new.html>`__\nfor a history of notable changes to scikit-learn.\n\nDevelopment\n-----------\n\nWe welcome new contributors of all experience levels. The scikit-learn\ncommunity goals are to be helpful, welcoming, and effective. The\n`Development Guide <https://scikit-learn.org/stable/developers/index.html>`_\nhas detailed information about contributing code, documentation, tests, and\nmore. We've included some basic information in this README.\n\nImportant links\n~~~~~~~~~~~~~~~\n\n- Official source code repo: https://github.com/scikit-learn/scikit-learn\n- Download releases: https://pypi.org/project/scikit-learn/\n- Issue tracker: https://github.com/scikit-learn/scikit-learn/issues\n\nSource code\n~~~~~~~~~~~\n\nYou can check the latest sources with the command::\n\n    git clone https://github.com/scikit-learn/scikit-learn.git\n\nContributing\n~~~~~~~~~~~~\n\nTo learn more about making a contribution to scikit-learn, please see our\n`Contributing guide\n<https://scikit-learn.org/dev/developers/contributing.html>`_.\n\nTesting\n~~~~~~~\n\nAfter installation, you can launch the test suite from outside the source\ndirectory (you will need to have ``pytest`` >= |PyTestMinVersion| installed)::\n\n    pytest sklearn\n\nSee th...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "pandas",
    "full_name": "pandas-dev/pandas",
    "description": "Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures similar to R data.frame objects, statistical functions, and much more",
    "url": "https://github.com/pandas-dev/pandas",
    "stars": 45241,
    "forks": 18446,
    "created_at": "2010-08-24",
    "updated_at": "2025-04-26",
    "topics": [
      "data-analysis",
      "pandas",
      "flexible",
      "alignment",
      "python",
      "data-science"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 20925817,
      "Cython": 1388723,
      "HTML": 457849,
      "C": 354589,
      "Shell": 22853,
      "Meson": 11215,
      "Smarty": 8852,
      "CSS": 6804,
      "Dockerfile": 5876,
      "XSLT": 1196
    },
    "readme_content": "<picture align=\"center\">\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://pandas.pydata.org/static/img/pandas_white.svg\">\n  <img alt=\"Pandas Logo\" src=\"https://pandas.pydata.org/static/img/pandas.svg\">\n</picture>\n\n-----------------\n\n# pandas: powerful Python data analysis toolkit\n\n| | |\n| --- | --- |\n| Testing | [![CI - Test](https://github.com/pandas-dev/pandas/actions/workflows/unit-tests.yml/badge.svg)](https://github.com/pandas-dev/pandas/actions/workflows/unit-tests.yml) [![Coverage](https://codecov.io/github/pandas-dev/pandas/coverage.svg?branch=main)](https://codecov.io/gh/pandas-dev/pandas) |\n| Package | [![PyPI Latest Release](https://img.shields.io/pypi/v/pandas.svg)](https://pypi.org/project/pandas/) [![PyPI Downloads](https://img.shields.io/pypi/dm/pandas.svg?label=PyPI%20downloads)](https://pypi.org/project/pandas/) [![Conda Latest Release](https://anaconda.org/conda-forge/pandas/badges/version.svg)](https://anaconda.org/conda-forge/pandas) [![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/pandas.svg?label=Conda%20downloads)](https://anaconda.org/conda-forge/pandas) |\n| Meta | [![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3509134.svg)](https://doi.org/10.5281/zenodo.3509134) [![License - BSD 3-Clause](https://img.shields.io/pypi/l/pandas.svg)](https://github.com/pandas-dev/pandas/blob/main/LICENSE) [![Slack](https://img.shields.io/badge/join_Slack-information-brightgreen.svg?logo=slack)](https://pandas.pydata.org/docs/dev/development/community.html?highlight=slack#community-slack) |\n\n\n## What is it?\n\n**pandas** is a Python package that provides fast, flexible, and expressive data\nstructures designed to make working with \"relational\" or \"labeled\" data both\neasy and intuitive. It aims to be the fundamental high-level building block for\ndoing practical, **real world** data analysis in Python. Additionally, it has\nthe broader goal of becoming **the most powerful and flexible open source data\nanalysis / manipulation tool available in any language**. It is already well on\nits way towards this goal.\n\n## Table of Contents\n\n- [Main Features](#main-features)\n- [Where to get it](#where-to-get-it)\n- [Dependencies](#dependencies)\n- [Installation from sources](#installation-from-sources)\n- [License](#license)\n- [Documentation](#documentation)\n- [Background](#background)\n- [Getting Help](#getting-help)\n- [Discussion and Development](#discussion-and-development)\n- [Contributing to pandas](#contributing-to-pandas)\n\n## Main Features\nHere are just a few of the things that pandas does well:\n\n  - Easy handling of [**missing data**][missing-data] (represented as\n    `NaN`, `NA`, or `NaT`) in floating point as well as non-floating point data\n  - Size mutability: columns can be [**inserted and\n    deleted**][insertion-deletion] from DataFrame and higher dimensional\n    objects\n  - Automatic and explicit [**data alignment**][alignment]: objects can\n    be explicitly aligned to a set of labels, or the user can simply\n    ignore the labels and let `Series`, `DataFrame`, etc. automatically\n    align the data for you in computations\n  - Powerful, flexible [**group by**][groupby] functionality to perform\n    split-apply-combine operations on data sets, for both aggregating\n    and transforming data\n  - Make it [**easy to convert**][conversion] ragged,\n    differently-indexed data in other Python and NumPy data structures\n    into DataFrame objects\n  - Intelligent label-based [**slicing**][slicing], [**fancy\n    indexing**][fancy-indexing], and [**subsetting**][subsetting] of\n    large data sets\n  - Intuitive [**merging**][merging] and [**joining**][joining] data\n    sets\n  - Flexible [**reshaping**][reshape] and [**pivoting**][pivot-table] of\n    data sets\n  - [**Hierarchical**][mi] labeling of axes (possible to have multiple\n    labels per tick)\n  - Robust IO tools for loading data from [**flat files**][flat-files]\n    (CSV and delimited), [**Excel files**][excel], [**databases**][db],\n    and saving/loading data from the ultrafast [**HDF5 format**][hdfstore]\n  - [**Time series**][timeseries]-specific functionality: date range\n    generation and frequency conversion, moving window statistics,\n    date shifting and lagging\n\n\n   [missing-data]: https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html\n   [insertion-deletion]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#column-selection-addition-deletion\n   [alignment]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html?highlight=alignment#intro-to-data-structures\n   [groupby]: https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#group-by-split-apply-combine\n   [conversion]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#dataframe\n   [slicing]: https://pandas.pydata.org/pandas-docs/stable/user_guide/i...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "airflow",
    "full_name": "apache/airflow",
    "description": "Apache Airflow - A platform to programmatically author, schedule, and monitor workflows",
    "url": "https://github.com/apache/airflow",
    "stars": 39826,
    "forks": 14930,
    "created_at": "2015-04-13",
    "updated_at": "2025-04-26",
    "topics": [
      "airflow",
      "apache",
      "apache-airflow",
      "python",
      "scheduler",
      "workflow",
      "automation",
      "dag",
      "data-engineering",
      "data-integration",
      "data-orchestrator",
      "data-pipelines",
      "data-science",
      "elt",
      "etl",
      "machine-learning",
      "mlops",
      "orchestration",
      "workflow-engine",
      "workflow-orchestration"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 40258891,
      "TypeScript": 1816132,
      "JavaScript": 318561,
      "Shell": 219647,
      "Dockerfile": 112836,
      "Jinja": 72760,
      "HTML": 42915,
      "CSS": 15308,
      "Go": 9491,
      "Jupyter Notebook": 7288,
      "HCL": 3786,
      "Mako": 2684,
      "Java": 1443
    },
    "readme_content": "<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n-->\n\n<!-- START Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n# Apache Airflow\n\n[![PyPI version](https://badge.fury.io/py/apache-airflow.svg)](https://badge.fury.io/py/apache-airflow)\n[![GitHub Build](https://github.com/apache/airflow/actions/workflows/ci.yml/badge.svg)](https://github.com/apache/airflow/actions)\n[![Coverage Status](https://codecov.io/gh/apache/airflow/graph/badge.svg?token=WdLKlKHOAU)](https://codecov.io/gh/apache/airflow)\n[![License](https://img.shields.io/:license-Apache%202-blue.svg)](https://www.apache.org/licenses/LICENSE-2.0.txt)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/apache-airflow.svg)](https://pypi.org/project/apache-airflow/)\n[![Docker Pulls](https://img.shields.io/docker/pulls/apache/airflow.svg)](https://hub.docker.com/r/apache/airflow)\n[![Docker Stars](https://img.shields.io/docker/stars/apache/airflow.svg)](https://hub.docker.com/r/apache/airflow)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/apache-airflow)](https://pypi.org/project/apache-airflow/)\n[![Artifact HUB](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/apache-airflow)](https://artifacthub.io/packages/search?repo=apache-airflow)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](https://s.apache.org/airflow-slack)\n[![Contributors](https://img.shields.io/github/contributors/apache/airflow)](https://github.com/apache/airflow/graphs/contributors)\n![Commit Activity](https://img.shields.io/github/commit-activity/m/apache/airflow)\n[![OSSRank](https://shields.io/endpoint?url=https://ossrank.com/shield/6)](https://ossrank.com/p/6)\n\n<picture width=\"500\">\n  <img\n    src=\"https://github.com/apache/airflow/blob/19ebcac2395ef9a6b6ded3a2faa29dc960c1e635/docs/apache-airflow/img/logos/wordmark_1.png?raw=true\"\n    alt=\"Apache Airflow logo\"\n  />\n</picture>\n\n[Apache Airflow](https://airflow.apache.org/docs/apache-airflow/stable/) (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows.\n\nWhen workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.\n\nUse Airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.\n\n<!-- END Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n**Table of contents**\n\n- [Project Focus](#project-focus)\n- [Principles](#principles)\n- [Requirements](#requirements)\n- [Getting started](#getting-started)\n- [Installing from PyPI](#installing-from-pypi)\n- [Official source code](#official-source-code)\n- [Convenience packages](#convenience-packages)\n- [User Interface](#user-interface)\n- [Semantic versioning](#semantic-versioning)\n- [Version Life Cycle](#version-life-cycle)\n- [Support for Python and Kubernetes versions](#support-for-python-and-kubernetes-versions)\n- [Base OS support for reference Airflow images](#base-os-support-for-reference-airflow-images)\n- [Approach to dependencies of Airflow](#approach-to-dependencies-of-airflow)\n- [Contributing](#contributing)\n- [Voting Policy](#voting-policy)\n- [Who uses Apache Airflow?](#who-uses-apache-airflow)\n- [Who maintains Apache Airflow?](#who-maintains-apache-airflow)\n- [What goes into the next release?](#what-goes-into-the-next-release)\n- [Can I use the Apache Airflow logo in my presentation?](#can-i-use-the-apache-airflow-logo-in-my-presentation)\n- [Links](#links)\n- [Sponsors](#sponsors)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n## Project Focus\n\nAirflow works best with workflows that are mostly static and slowly changing. Wh...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "streamlit",
    "full_name": "streamlit/streamlit",
    "description": "Streamlit — A faster way to build and share data apps.",
    "url": "https://github.com/streamlit/streamlit",
    "stars": 39034,
    "forks": 3412,
    "created_at": "2019-08-24",
    "updated_at": "2025-04-26",
    "topics": [
      "python",
      "machine-learning",
      "data-science",
      "deep-learning",
      "data-visualization",
      "streamlit",
      "data-analysis",
      "developer-tools"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 5825838,
      "TypeScript": 4016135,
      "JavaScript": 66360,
      "HTML": 17078,
      "Makefile": 16067,
      "SCSS": 6463,
      "Shell": 3688,
      "Dockerfile": 1121,
      "Batchfile": 676,
      "CSS": 146
    },
    "readme_content": "<br>\n\n<img src=\"https://user-images.githubusercontent.com/7164864/217935870-c0bc60a3-6fc0-4047-b011-7b4c59488c91.png\" alt=\"Streamlit logo\" style=\"margin-top:50px\"></img>\n\n# Welcome to Streamlit 👋\n\n**A faster way to build and share data apps.**\n\n## What is Streamlit?\n\nStreamlit lets you transform Python scripts into interactive web apps in minutes, instead of weeks. Build dashboards, generate reports, or create chat apps. Once you’ve created an app, you can use our [Community Cloud platform](https://streamlit.io/cloud) to deploy, manage, and share your app.\n\n### Why choose Streamlit?\n\n- **Simple and Pythonic:** Write beautiful, easy-to-read code.\n- **Fast, interactive prototyping:** Let others interact with your data and provide feedback quickly.\n- **Live editing:** See your app update instantly as you edit your script.\n- **Open-source and free:** Join a vibrant community and contribute to Streamlit's future.\n\n## Installation\n\nOpen a terminal and run:\n\n```bash\n$ pip install streamlit\n$ streamlit hello\n```\n\nIf this opens our sweet _Streamlit Hello_ app in your browser, you're all set! If not, head over to [our docs](https://docs.streamlit.io/get-started) for specific installs.\n\nThe app features a bunch of examples of what you can do with Streamlit. Jump to the [quickstart](#quickstart) section to understand how that all works.\n\n<img src=\"https://user-images.githubusercontent.com/7164864/217936487-1017784e-68ec-4e0d-a7f6-6b97525ddf88.gif\" alt=\"Streamlit Hello\" width=500 href=\"none\"></img>\n\n## Quickstart\n\n### A little example\n\nCreate a new file named `streamlit_app.py` in your project directory with the following code:\n```python\nimport streamlit as st\nx = st.slider(\"Select a value\")\nst.write(x, \"squared is\", x * x)\n```\n\nNow run it to open the app!\n```\n$ streamlit run streamlit_app.py\n```\n\n<img src=\"https://user-images.githubusercontent.com/7164864/215172915-cf087c56-e7ae-449a-83a4-b5fa0328d954.gif\" width=300 alt=\"Little example\"></img>\n\n### Give me more!\n\nStreamlit comes in with [a ton of additional powerful elements](https://docs.streamlit.io/develop/api-reference) to spice up your data apps and delight your viewers. Some examples:\n\n<table border=\"0\">\n  <tr>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.streamlit.io/develop/api-reference/widgets\">\n        <img src=\"https://user-images.githubusercontent.com/7164864/217936099-12c16f8c-7fe4-44b1-889a-1ac9ee6a1b44.png\" style=\"max-height:150px; width:auto; display:block;\">\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.streamlit.io/develop/api-reference/data/st.dataframe\">\n        <img src=\"https://user-images.githubusercontent.com/7164864/215110064-5eb4e294-8f30-4933-9563-0275230e52b5.gif\" style=\"max-height:150px; width:auto; display:block;\">\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.streamlit.io/develop/api-reference/charts\">\n        <img src=\"https://user-images.githubusercontent.com/7164864/215174472-bca8a0d7-cf4b-4268-9c3b-8c03dad50bcd.gif\" style=\"max-height:150px; width:auto; display:block;\">\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.streamlit.io/develop/api-reference/layout\">\n        <img src=\"https://user-images.githubusercontent.com/7164864/217936149-a35c35be-0d96-4c63-8c6a-1c4b52aa8f60.png\" style=\"max-height:150px; width:auto; display:block;\">\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.streamlit.io/develop/concepts/multipage-apps\">\n        <img src=\"https://user-images.githubusercontent.com/7164864/215173883-eae0de69-7c1d-4d78-97d0-3bc1ab865e5b.gif\" style=\"max-height:150px; width:auto; display:block;\">\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://streamlit.io/gallery\">\n        <img src=\"https://user-images.githubusercontent.com/7164864/215109229-6ae9111f-e5c1-4f0b-b3a2-87a79268ccc9.gif\" style=\"max-height:150px; width:auto; display:block;\">\n      </a>\n    </td>\n  </tr>\n  <tr>\n    <td>Input widgets</td>\n    <td>Dataframes</td>\n    <td>Charts</td>\n    <td>Layout</td>\n    <td>Multi-page apps</td>\n    <td>Fun</td>\n  </tr>\n</table>\n\n\nOur vibrant creators community also extends Streamlit capabilities using  🧩 [Streamlit Components](https://streamlit.io/components).\n\n## Get inspired\n\nThere's so much you can build with Streamlit:\n- 🤖  [LLMs & chatbot apps](https://streamlit.io/gallery?category=llms)\n- 🧬  [Science & technology apps](https://streamlit.io/gallery?category=science-technology)\n- 💬  [NLP & language apps](https://streamlit.io/gallery?category=nlp-language)\n- 🏦  [Finance & business apps](https://streamlit.io/gallery?category=finance-business)\n- 🗺  [Geography & society apps](https://streamlit.io/gallery?category=geography-society)\n- and more!\n\n**Check out [our gallery!](https://streamlit.io/gallery)** 🎈\n\n## Community Cloud\n\nDeploy, manage and share your apps for free using our [Community Cloud](https://streamlit.io/cloud)! Sign-up [here](https://share.streamlit.io/signup). <br><br>\n<img src=\"https://user-i...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "Made-With-ML",
    "full_name": "GokuMohandas/Made-With-ML",
    "description": "Learn how to design, develop, deploy and iterate on production-grade ML applications.",
    "url": "https://github.com/GokuMohandas/Made-With-ML",
    "stars": 38451,
    "forks": 6092,
    "created_at": "2018-11-05",
    "updated_at": "2025-04-26",
    "topics": [
      "machine-learning",
      "deep-learning",
      "pytorch",
      "natural-language-processing",
      "data-science",
      "python",
      "mlops",
      "data-engineering",
      "data-quality",
      "distributed-ml",
      "llms",
      "ray",
      "distributed-training"
    ],
    "primary_language": "Jupyter Notebook",
    "languages": {
      "Jupyter Notebook": 2725871,
      "Python": 58624,
      "Shell": 2047,
      "Makefile": 415
    },
    "readme_content": "<div align=\"center\">\n<h1><img width=\"30\" src=\"https://madewithml.com/static/images/rounded_logo.png\">&nbsp;<a href=\"https://madewithml.com/\">Made With ML</a></h1>\nDesign · Develop · Deploy · Iterate\n<br>\nJoin 40K+ developers in learning how to responsibly deliver value with ML.\n    <br>\n</div>\n\n<br>\n\n<div align=\"center\">\n    <a target=\"_blank\" href=\"https://madewithml.com/\"><img src=\"https://img.shields.io/badge/Subscribe-40K-brightgreen\"></a>&nbsp;\n    <a target=\"_blank\" href=\"https://github.com/GokuMohandas/Made-With-ML\"><img src=\"https://img.shields.io/github/stars/GokuMohandas/Made-With-ML.svg?style=social&label=Star\"></a>&nbsp;\n    <a target=\"_blank\" href=\"https://www.linkedin.com/in/goku\"><img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\"></a>&nbsp;\n    <a target=\"_blank\" href=\"https://twitter.com/GokuMohandas\"><img src=\"https://img.shields.io/twitter/follow/GokuMohandas.svg?label=Follow&style=social\"></a>\n    <br>\n    🔥&nbsp; Among the <a href=\"https://github.com/GokuMohandas/Made-With-ML\" target=\"_blank\">top ML repositories</a> on GitHub\n</div>\n\n<br>\n<hr>\n\n## Lessons\n\nLearn how to combine machine learning with software engineering to design, develop, deploy and iterate on production-grade ML applications.\n\n- Lessons: https://madewithml.com/\n- Code: [GokuMohandas/Made-With-ML](https://github.com/GokuMohandas/Made-With-ML)\n\n<a href=\"https://madewithml.com/#course\">\n  <img src=\"https://madewithml.com/static/images/lessons.png\" alt=\"lessons\">\n</a>\n\n## Overview\n\nIn this course, we'll go from experimentation (design + development) to production (deployment + iteration). We'll do this iteratively by motivating the components that will enable us to build a *reliable* production system.\n\n<blockquote>\n  <img width=20 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/YouTube_full-color_icon_%282017%29.svg/640px-YouTube_full-color_icon_%282017%29.svg.png\">&nbsp; Be sure to watch the video below for a quick overview of what we'll be building.\n</blockquote>\n\n<div align=\"center\">\n  <a href=\"https://youtu.be/AWgkt8H8yVo\"><img src=\"https://img.youtube.com/vi/AWgkt8H8yVo/0.jpg\" alt=\"Course overview video\"></a>\n</div>\n\n<br>\n\n- **💡 First principles**: before we jump straight into the code, we develop a first principles understanding for every machine learning concept.\n- **💻 Best practices**: implement software engineering best practices as we develop and deploy our machine learning models.\n- **📈 Scale**: easily scale ML workloads (data, train, tune, serve) in Python without having to learn completely new languages.\n- **⚙️ MLOps**: connect MLOps components (tracking, testing, serving, orchestration, etc.) as we build an end-to-end machine learning system.\n- **🚀 Dev to Prod**: learn how to quickly and reliably go from development to production without any changes to our code or infra management.\n- **🐙 CI/CD**: learn how to create mature CI/CD workflows to continuously train and deploy better models in a modular way that integrates with any stack.\n\n## Audience\n\nMachine learning is not a separate industry, instead, it's a powerful way of thinking about data that's not reserved for any one type of person.\n\n- **👩‍💻 All developers**: whether software/infra engineer or data scientist, ML is increasingly becoming a key part of the products that you'll be developing.\n- **👩‍🎓 College graduates**: learn the practical skills required for industry and bridge gap between the university curriculum and what industry expects.\n- **👩‍💼 Product/Leadership**: who want to develop a technical foundation so that they can build amazing (and reliable) products powered by machine learning.\n\n## Set up\n\nBe sure to go through the [course](https://madewithml/#course) for a much more detailed walkthrough of the content on this repository. We will have instructions for both local laptop and Anyscale clusters for the sections below, so be sure to toggle the ► dropdown based on what you're using (Anyscale instructions will be toggled on by default). If you do want to run this course with Anyscale, where we'll provide the **structure**, **compute (GPUs)** and **community** to learn everything in one day, join our next upcoming live cohort → [sign up here](https://4190urw86oh.typeform.com/madewithml)!\n\n### Cluster\n\nWe'll start by setting up our cluster with the environment and compute configurations.\n\n<details>\n  <summary>Local</summary><br>\n  Your personal laptop (single machine) will act as the cluster, where one CPU will be the head node and some of the remaining CPU will be the worker nodes. All of the code in this course will work in any personal laptop though it will be slower than executing the same workloads on a larger cluster.\n</details>\n\n<details open>\n  <summary>Anyscale</summary><br>\n\n  We can create an [Anyscale Workspace](https://docs.anyscale.com/develop/workspaces/get-started) using the [webpage UI](https://console.anyscale.com/o/madewithml/workspaces/add/blank).\n\n  ```md\n  - Wo...",
    "owner_type": "User",
    "is_archived": false
  },
  {
    "name": "gradio",
    "full_name": "gradio-app/gradio",
    "description": "Build and share delightful machine learning apps, all in Python. 🌟 Star to support our work!",
    "url": "https://github.com/gradio-app/gradio",
    "stars": 37689,
    "forks": 2867,
    "created_at": "2018-12-19",
    "updated_at": "2025-04-26",
    "topics": [
      "machine-learning",
      "models",
      "ui",
      "ui-components",
      "interface",
      "python",
      "data-science",
      "data-visualization",
      "deep-learning",
      "data-analysis",
      "gradio",
      "gradio-interface",
      "python-notebook",
      "deploy",
      "hacktoberfest"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 3271269,
      "Svelte": 1313700,
      "TypeScript": 1250478,
      "mdsvex": 221972,
      "CSS": 72732,
      "JavaScript": 62692,
      "Jupyter Notebook": 32113,
      "HTML": 24506,
      "Batchfile": 6427,
      "Shell": 6015,
      "MDX": 1670,
      "Dockerfile": 512
    },
    "readme_content": "<!-- DO NOT EDIT THIS FILE DIRECTLY. INSTEAD EDIT THE `readme_template.md` OR `guides/01_getting-started/01_quickstart.md` TEMPLATES AND THEN RUN `render_readme.py` SCRIPT. -->\n\n<div align=\"center\">\n<a href=\"https://gradio.app\">\n<img src=\"readme_files/gradio.svg\" alt=\"gradio\" width=350>\n</a>\n</div>\n\n<div align=\"center\">\n<span>\n<a href=\"https://www.producthunt.com/posts/gradio-5-0?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-gradio&#0045;5&#0045;0\" target=\"_blank\"><img src=\"https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=501906&theme=light\" alt=\"Gradio&#0032;5&#0046;0 - the&#0032;easiest&#0032;way&#0032;to&#0032;build&#0032;AI&#0032;web&#0032;apps | Product Hunt\" style=\"width: 150px; height: 54px;\" width=\"150\" height=\"54\" /></a>\n<a href=\"https://trendshift.io/repositories/2145\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/2145\" alt=\"gradio-app%2Fgradio | Trendshift\" style=\"width: 150px; height: 55px;\" width=\"150\" height=\"55\"/></a>\n</span>\n\n[![gradio-backend](https://github.com/gradio-app/gradio/actions/workflows/test-python.yml/badge.svg)](https://github.com/gradio-app/gradio/actions/workflows/test-python.yml)\n[![gradio-ui](https://github.com/gradio-app/gradio/actions/workflows/tests-js.yml/badge.svg)](https://github.com/gradio-app/gradio/actions/workflows/tests-js.yml) \n[![PyPI](https://img.shields.io/pypi/v/gradio)](https://pypi.org/project/gradio/)\n[![PyPI downloads](https://img.shields.io/pypi/dm/gradio)](https://pypi.org/project/gradio/)\n![Python version](https://img.shields.io/badge/python-3.10+-important)\n[![Twitter follow](https://img.shields.io/twitter/follow/gradio?style=social&label=follow)](https://twitter.com/gradio)\n\n[Website](https://gradio.app)\n| [Documentation](https://gradio.app/docs/)\n| [Guides](https://gradio.app/guides/)\n| [Getting Started](https://gradio.app/getting_started/)\n| [Examples](demo/)\n\n</div>\n\n<div align=\"center\">\n\nEnglish | [中文](readme_files/zh-cn#readme)\n\n</div>\n\n# Gradio: Build Machine Learning Web Apps — in Python\n\n\n\nGradio is an open-source Python package that allows you to quickly **build** a demo or web application for your machine learning model, API, or any arbitrary Python function. You can then **share** a link to your demo or web application in just a few seconds using Gradio's built-in sharing features. *No JavaScript, CSS, or web hosting experience needed!*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/gif-version.gif\" style=\"padding-bottom: 10px\">\n\nIt just takes a few lines of Python to create your own demo, so let's get started 💫\n\n\n### Installation\n\n**Prerequisite**: Gradio requires [Python 3.10 or higher](https://www.python.org/downloads/).\n\n\nWe recommend installing Gradio using `pip`, which is included by default in Python. Run this in your terminal or command prompt:\n\n```bash\npip install --upgrade gradio\n```\n\n\n> [!TIP]\n > It is best to install Gradio in a virtual environment. Detailed installation instructions for all common operating systems <a href=\"https://www.gradio.app/main/guides/installing-gradio-in-a-virtual-environment\">are provided here</a>. \n\n### Building Your First Demo\n\nYou can run Gradio in your favorite code editor, Jupyter notebook, Google Colab, or anywhere else you write Python. Let's write your first Gradio app:\n\n\n```python\nimport gradio as gr\n\ndef greet(name, intensity):\n    return \"Hello, \" + name + \"!\" * int(intensity)\n\ndemo = gr.Interface(\n    fn=greet,\n    inputs=[\"text\", \"slider\"],\n    outputs=[\"text\"],\n)\n\ndemo.launch()\n```\n\n\n\n> [!TIP]\n > We shorten the imported name from <code>gradio</code> to <code>gr</code>. This is a widely adopted convention for better readability of code. \n\nNow, run your code. If you've written the Python code in a file named `app.py`, then you would run `python app.py` from the terminal.\n\nThe demo below will open in a browser on [http://localhost:7860](http://localhost:7860) if running from a file. If you are running within a notebook, the demo will appear embedded within the notebook.\n\n![`hello_world_4` demo](demo/hello_world_4/screenshot.gif)\n\nType your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.\n\n> [!TIP]\n > When developing locally, you can run your Gradio app in <strong>hot reload mode</strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio</code> before the name of the file instead of <code>python</code>. In the example above, you would type: `gradio app.py` in your terminal. Learn more in the <a href=\"https://www.gradio.app/guides/developing-faster-with-reload-mode\">Hot Reloading Guide</a>.\n\n\n**Understanding the `Interface` Class**\n\nYou'll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning model...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "ray",
    "full_name": "ray-project/ray",
    "description": "Ray is an AI compute engine. Ray consists of a core distributed runtime and a set of AI Libraries for accelerating ML workloads.",
    "url": "https://github.com/ray-project/ray",
    "stars": 36761,
    "forks": 6247,
    "created_at": "2016-10-25",
    "updated_at": "2025-04-26",
    "topics": [
      "ray",
      "distributed",
      "parallel",
      "machine-learning",
      "reinforcement-learning",
      "deep-learning",
      "python",
      "rllib",
      "hyperparameter-search",
      "optimization",
      "data-science",
      "hyperparameter-optimization",
      "serving",
      "deployment",
      "pytorch",
      "tensorflow",
      "llm-serving",
      "large-language-models",
      "llm",
      "llm-inference"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 29789311,
      "C++": 7691302,
      "Java": 1124082,
      "TypeScript": 632135,
      "Starlark": 525928,
      "Cython": 389237,
      "Shell": 170518,
      "C": 39275,
      "Dockerfile": 24514,
      "Jinja": 9660,
      "CSS": 2887,
      "Linker Script": 2821,
      "PowerShell": 2649,
      "HTML": 1831,
      "Roff": 1213,
      "JavaScript": 757
    },
    "readme_content": ".. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/ray_header_logo.png\n\n.. image:: https://readthedocs.org/projects/ray/badge/?version=master\n    :target: http://docs.ray.io/en/master/?badge=master\n\n.. image:: https://img.shields.io/badge/Ray-Join%20Slack-blue\n    :target: https://www.ray.io/join-slack\n\n.. image:: https://img.shields.io/badge/Discuss-Ask%20Questions-blue\n    :target: https://discuss.ray.io/\n\n.. image:: https://img.shields.io/twitter/follow/raydistributed.svg?style=social&logo=twitter\n    :target: https://twitter.com/raydistributed\n\n.. image:: https://img.shields.io/badge/Get_started_for_free-3C8AE9?logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8%2F9hAAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAEKADAAQAAAABAAAAEAAAAAA0VXHyAAABKElEQVQ4Ea2TvWoCQRRGnWCVWChIIlikC9hpJdikSbGgaONbpAoY8gKBdAGfwkfwKQypLQ1sEGyMYhN1Pd%2B6A8PqwBZeOHt%2FvsvMnd3ZXBRFPQjBZ9K6OY8ZxF%2B0IYw9PW3qz8aY6lk92bZ%2BVqSI3oC9T7%2FyCVnrF1ngj93us%2B540sf5BrCDfw9b6jJ5lx%2FyjtGKBBXc3cnqx0INN4ImbI%2Bl%2BPnI8zWfFEr4chLLrWHCp9OO9j19Kbc91HX0zzzBO8EbLK2Iv4ZvNO3is3h6jb%2BCwO0iL8AaWqB7ILPTxq3kDypqvBuYuwswqo6wgYJbT8XxBPZ8KS1TepkFdC79TAHHce%2F7LbVioi3wEfTpmeKtPRGEeoldSP%2FOeoEftpP4BRbgXrYZefsAI%2BP9JU7ImyEAAAAASUVORK5CYII%3D\n   :target: https://www.anyscale.com/ray-on-anyscale?utm_source=github&utm_medium=ray_readme&utm_campaign=get_started_badge\n\nRay is a unified framework for scaling AI and Python applications. Ray consists of a core distributed runtime and a set of AI libraries for simplifying ML compute:\n\n.. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/what-is-ray-padded.svg\n\n..\n  https://docs.google.com/drawings/d/1Pl8aCYOsZCo61cmp57c7Sja6HhIygGCvSZLi_AuBuqo/edit\n\nLearn more about `Ray AI Libraries`_:\n\n- `Data`_: Scalable Datasets for ML\n- `Train`_: Distributed Training\n- `Tune`_: Scalable Hyperparameter Tuning\n- `RLlib`_: Scalable Reinforcement Learning\n- `Serve`_: Scalable and Programmable Serving\n\nOr more about `Ray Core`_ and its key abstractions:\n\n- `Tasks`_: Stateless functions executed in the cluster.\n- `Actors`_: Stateful worker processes created in the cluster.\n- `Objects`_: Immutable values accessible across the cluster.\n\nLearn more about Monitoring and Debugging:\n\n- Monitor Ray apps and clusters with the `Ray Dashboard <https://docs.ray.io/en/latest/ray-core/ray-dashboard.html>`__.\n- Debug Ray apps with the `Ray Distributed Debugger <https://docs.ray.io/en/latest/ray-observability/ray-distributed-debugger.html>`__.\n\nRay runs on any machine, cluster, cloud provider, and Kubernetes, and features a growing\n`ecosystem of community integrations`_.\n\nInstall Ray with: ``pip install ray``. For nightly wheels, see the\n`Installation page <https://docs.ray.io/en/latest/ray-overview/installation.html>`__.\n\n.. _`Serve`: https://docs.ray.io/en/latest/serve/index.html\n.. _`Data`: https://docs.ray.io/en/latest/data/dataset.html\n.. _`Workflow`: https://docs.ray.io/en/latest/workflows/concepts.html\n.. _`Train`: https://docs.ray.io/en/latest/train/train.html\n.. _`Tune`: https://docs.ray.io/en/latest/tune/index.html\n.. _`RLlib`: https://docs.ray.io/en/latest/rllib/index.html\n.. _`ecosystem of community integrations`: https://docs.ray.io/en/latest/ray-overview/ray-libraries.html\n\n\nWhy Ray?\n--------\n\nToday's ML workloads are increasingly compute-intensive. As convenient as they are, single-node development environments such as your laptop cannot scale to meet these demands.\n\nRay is a unified way to scale Python and AI applications from a laptop to a cluster.\n\nWith Ray, you can seamlessly scale the same code from a laptop to a cluster. Ray is designed to be general-purpose, meaning that it can performantly run any kind of workload. If your application is written in Python, you can scale it with Ray, no other infrastructure required.\n\nMore Information\n----------------\n\n- `Documentation`_\n- `Ray Architecture whitepaper`_\n- `Exoshuffle: large-scale data shuffle in Ray`_\n- `Ownership: a distributed futures system for fine-grained tasks`_\n- `RLlib paper`_\n- `Tune paper`_\n\n*Older documents:*\n\n- `Ray paper`_\n- `Ray HotOS paper`_\n- `Ray Architecture v1 whitepaper`_\n\n.. _`Ray AI Libraries`: https://docs.ray.io/en/latest/ray-air/getting-started.html\n.. _`Ray Core`: https://docs.ray.io/en/latest/ray-core/walkthrough.html\n.. _`Tasks`: https://docs.ray.io/en/latest/ray-core/tasks.html\n.. _`Actors`: https://docs.ray.io/en/latest/ray-core/actors.html\n.. _`Objects`: https://docs.ray.io/en/latest/ray-core/objects.html\n.. _`Documentation`: http://docs.ray.io/en/latest/index.html\n.. _`Ray Architecture v1 whitepaper`: https://docs.google.com/document/d/1lAy0Owi-vPz2jEqBSaHNQcy2IBSDEHyXNOQZlGuj93c/preview\n.. _`Ray Architecture whitepaper`: https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview\n.. _`Exoshuffle: large-scale data shuffle in Ray`: https://arxiv.org/abs/2203.05072\n.. _`Ownership: a distributed futures...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "tensorflow",
    "full_name": "tensorflow/tensorflow",
    "description": "An Open Source Machine Learning Framework for Everyone",
    "url": "https://github.com/tensorflow/tensorflow",
    "stars": 189651,
    "forks": 74649,
    "created_at": "2015-11-07",
    "updated_at": "2025-04-26",
    "topics": [
      "tensorflow",
      "machine-learning",
      "python",
      "deep-learning",
      "deep-neural-networks",
      "neural-network",
      "ml",
      "distributed"
    ],
    "primary_language": "C++",
    "languages": {
      "C++": 100440745,
      "Python": 45812977,
      "MLIR": 11536604,
      "HTML": 7662661,
      "Starlark": 7302086,
      "Go": 2173562,
      "C": 1247325,
      "Java": 1178817,
      "Jupyter Notebook": 805762,
      "Shell": 705077,
      "Objective-C++": 279654,
      "Objective-C": 169277,
      "CMake": 148613,
      "Smarty": 139504,
      "Swift": 81677,
      "Dockerfile": 38776,
      "C#": 13585,
      "Batchfile": 12126,
      "Ruby": 8898,
      "Perl": 7536,
      "Roff": 5034,
      "Linker Script": 4497,
      "Cython": 3899,
      "Makefile": 2845,
      "CSS": 2761,
      "Vim Snippet": 58
    },
    "readme_content": "<div align=\"center\">\n  <img src=\"https://www.tensorflow.org/images/tf_logo_horizontal.png\">\n</div>\n\n[![Python](https://img.shields.io/pypi/pyversions/tensorflow.svg)](https://badge.fury.io/py/tensorflow)\n[![PyPI](https://badge.fury.io/py/tensorflow.svg)](https://badge.fury.io/py/tensorflow)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4724125.svg)](https://doi.org/10.5281/zenodo.4724125)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1486/badge)](https://bestpractices.coreinfrastructure.org/projects/1486)\n[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/tensorflow/tensorflow/badge)](https://securityscorecards.dev/viewer/?uri=github.com/tensorflow/tensorflow)\n[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/tensorflow.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:tensorflow)\n[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/tensorflow-py.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:tensorflow-py)\n[![OSSRank](https://shields.io/endpoint?url=https://ossrank.com/shield/44)](https://ossrank.com/p/44)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg)](CODE_OF_CONDUCT.md)\n[![TF Official Continuous](https://tensorflow.github.io/build/TF%20Official%20Continuous.svg)](https://tensorflow.github.io/build#TF%20Official%20Continuous)\n[![TF Official Nightly](https://tensorflow.github.io/build/TF%20Official%20Nightly.svg)](https://tensorflow.github.io/build#TF%20Official%20Nightly)\n\n**`Documentation`** |\n------------------- |\n[![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/api_docs/) |\n\n[TensorFlow](https://www.tensorflow.org/) is an end-to-end open source platform\nfor machine learning. It has a comprehensive, flexible ecosystem of\n[tools](https://www.tensorflow.org/resources/tools),\n[libraries](https://www.tensorflow.org/resources/libraries-extensions), and\n[community](https://www.tensorflow.org/community) resources that lets\nresearchers push the state-of-the-art in ML and developers easily build and\ndeploy ML-powered applications.\n\nTensorFlow was originally developed by researchers and engineers working within\nthe Machine Intelligence team at Google Brain to conduct research in machine\nlearning and neural networks. However, the framework is versatile enough to be\nused in other areas as well.\n\nTensorFlow provides stable [Python](https://www.tensorflow.org/api_docs/python)\nand [C++](https://www.tensorflow.org/api_docs/cc) APIs, as well as a\nnon-guaranteed backward compatible API for\n[other languages](https://www.tensorflow.org/api_docs).\n\nKeep up-to-date with release announcements and security updates by subscribing\nto\n[announce@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/announce).\nSee all the [mailing lists](https://www.tensorflow.org/community/forums).\n\n## Install\n\nSee the [TensorFlow install guide](https://www.tensorflow.org/install) for the\n[pip package](https://www.tensorflow.org/install/pip), to\n[enable GPU support](https://www.tensorflow.org/install/gpu), use a\n[Docker container](https://www.tensorflow.org/install/docker), and\n[build from source](https://www.tensorflow.org/install/source).\n\nTo install the current release, which includes support for\n[CUDA-enabled GPU cards](https://www.tensorflow.org/install/gpu) *(Ubuntu and\nWindows)*:\n\n```\n$ pip install tensorflow\n```\n\nOther devices (DirectX and MacOS-metal) are supported using\n[Device plugins](https://www.tensorflow.org/install/gpu_plugins#available_devices).\n\nA smaller CPU-only package is also available:\n\n```\n$ pip install tensorflow-cpu\n```\n\nTo update TensorFlow to the latest version, add `--upgrade` flag to the above\ncommands.\n\n*Nightly binaries are available for testing using the\n[tf-nightly](https://pypi.python.org/pypi/tf-nightly) and\n[tf-nightly-cpu](https://pypi.python.org/pypi/tf-nightly-cpu) packages on PyPI.*\n\n#### *Try your first TensorFlow program*\n\n```shell\n$ python\n```\n\n```python\n>>> import tensorflow as tf\n>>> tf.add(1, 2).numpy()\n3\n>>> hello = tf.constant('Hello, TensorFlow!')\n>>> hello.numpy()\nb'Hello, TensorFlow!'\n```\n\nFor more examples, see the\n[TensorFlow tutorials](https://www.tensorflow.org/tutorials/).\n\n## Contribution guidelines\n\n**If you want to contribute to TensorFlow, be sure to review the\n[contribution guidelines](CONTRIBUTING.md). This project adheres to TensorFlow's\n[code of conduct](CODE_OF_CONDUCT.md). By participating, you are expected to\nuphold this code.**\n\n**We use [GitHub issues](https://github.com/tensorflow/tensorflow/issues) for\ntracking requests and bugs, please see\n[TensorFlow Forum](https://discuss.tensorflow.org/) for general questions and\ndiscussion, and please direct specific questions to\n[Stack Overflow](https://stackoverflow.com/questions/tagged/tensorflow).**\n\nThe TensorFlow project strives to ...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "transformers",
    "full_name": "huggingface/transformers",
    "description": "🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.",
    "url": "https://github.com/huggingface/transformers",
    "stars": 143514,
    "forks": 28775,
    "created_at": "2018-10-29",
    "updated_at": "2025-04-26",
    "topics": [
      "nlp",
      "natural-language-processing",
      "pytorch",
      "language-model",
      "tensorflow",
      "bert",
      "language-models",
      "pytorch-transformers",
      "nlp-library",
      "transformer",
      "model-hub",
      "pretrained-models",
      "jax",
      "flax",
      "seq2seq",
      "speech-recognition",
      "hacktoberfest",
      "python",
      "machine-learning",
      "deep-learning"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 67095130,
      "Cuda": 204021,
      "Dockerfile": 38230,
      "C++": 19093,
      "C": 7703,
      "Makefile": 4087,
      "Cython": 3635,
      "Shell": 1838,
      "Jsonnet": 937
    },
    "readme_content": "<!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\">\n    <img alt=\"Hugging Face Transformers Library\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\" width=\"352\" height=\"59\" style=\"max-width: 100%;\">\n  </picture>\n  <br/>\n  <br/>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://huggingface.com/models\"><img alt=\"Checkpoints on Hub\" src=\"https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen\"></a>\n    <a href=\"https://circleci.com/gh/huggingface/transformers\"><img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/huggingface/transformers/main\"></a>\n    <a href=\"https://github.com/huggingface/transformers/blob/main/LICENSE\"><img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/transformers.svg?color=blue\"></a>\n    <a href=\"https://huggingface.co/docs/transformers/index\"><img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online\"></a>\n    <a href=\"https://github.com/huggingface/transformers/releases\"><img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/transformers.svg\"></a>\n    <a href=\"https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md\"><img alt=\"Contributor Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg\"></a>\n    <a href=\"https://zenodo.org/badge/latestdoi/155220641\"><img src=\"https://zenodo.org/badge/155220641.svg\" alt=\"DOI\"></a>\n</p>\n\n<h4 align=\"center\">\n    <p>\n        <b>English</b> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md\">简体中文</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md\">繁體中文</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md\">한국어</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_es.md\">Español</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md\">日本語</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md\">हिन्दी</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md\">Русский</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md\">Рortuguês</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_te.md\">తెలుగు</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md\">Français</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_de.md\">Deutsch</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md\">Tiếng Việt</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md\">العربية</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md\">اردو</a> |\n    </p>\n</h4>\n\n<h3 align=\"center\">\n    <p>State-of-the-art pretrained models for inference and training</p>\n</h3>\n\n<h3 align=\"center\">\n    <a href=\"https://hf.co/course\"><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png\"></a>\n</h3>\n\nTransformers is a library of pretrained text, computer vision, audio, video, and multimodal models for inference and training. Use Transformers to fine-tune models on your data, build inference applications, and for generative AI use cases across multiple modalities.\n\nThere are over 500K+ Transformers [model checkpoints](https://huggingface.co/models?library=transformers&sort=trending) on the [Hugging Face Hub](https://huggingface.com/models) you can use.\n\nExplore the [Hub](https://huggingface.com/) today to find a model and use Transformers to help you get started right away.\n\n## Installation\n\nTransformers works with Python 3.9+ [PyTorch](https://pytorch.org/get-started/locally/) 2.1+, [TensorFlow](https://www.te...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "pytorch",
    "full_name": "pytorch/pytorch",
    "description": "Tensors and Dynamic neural networks in Python with strong GPU acceleration",
    "url": "https://github.com/pytorch/pytorch",
    "stars": 89409,
    "forks": 23974,
    "created_at": "2016-08-13",
    "updated_at": "2025-04-26",
    "topics": [
      "neural-network",
      "autograd",
      "gpu",
      "numpy",
      "deep-learning",
      "tensor",
      "python",
      "machine-learning"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 68969194,
      "C++": 41228025,
      "Cuda": 3610955,
      "C": 1784698,
      "Objective-C++": 1348899,
      "CMake": 823348,
      "Shell": 511417,
      "Assembly": 336348,
      "Starlark": 326427,
      "GLSL": 204577,
      "Jupyter Notebook": 183407,
      "Metal": 176719,
      "Java": 135037,
      "JavaScript": 91258,
      "Batchfile": 89106,
      "Objective-C": 53593,
      "Dockerfile": 49528,
      "Makefile": 12248,
      "Thrift": 6963,
      "PowerShell": 3657,
      "Ruby": 2774,
      "GDB": 653,
      "Linker Script": 473,
      "HTML": 384,
      "Smarty": 376,
      "Vim Script": 154
    },
    "readme_content": "![PyTorch Logo](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png)\n\n--------------------------------------------------------------------------------\n\nPyTorch is a Python package that provides two high-level features:\n- Tensor computation (like NumPy) with strong GPU acceleration\n- Deep neural networks built on a tape-based autograd system\n\nYou can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.\n\nOur trunk health (Continuous Integration signals) can be found at [hud.pytorch.org](https://hud.pytorch.org/ci/pytorch/pytorch/main).\n\n<!-- toc -->\n\n- [More About PyTorch](#more-about-pytorch)\n  - [A GPU-Ready Tensor Library](#a-gpu-ready-tensor-library)\n  - [Dynamic Neural Networks: Tape-Based Autograd](#dynamic-neural-networks-tape-based-autograd)\n  - [Python First](#python-first)\n  - [Imperative Experiences](#imperative-experiences)\n  - [Fast and Lean](#fast-and-lean)\n  - [Extensions Without Pain](#extensions-without-pain)\n- [Installation](#installation)\n  - [Binaries](#binaries)\n    - [NVIDIA Jetson Platforms](#nvidia-jetson-platforms)\n  - [From Source](#from-source)\n    - [Prerequisites](#prerequisites)\n      - [NVIDIA CUDA Support](#nvidia-cuda-support)\n      - [AMD ROCm Support](#amd-rocm-support)\n      - [Intel GPU Support](#intel-gpu-support)\n    - [Get the PyTorch Source](#get-the-pytorch-source)\n    - [Install Dependencies](#install-dependencies)\n    - [Install PyTorch](#install-pytorch)\n      - [Adjust Build Options (Optional)](#adjust-build-options-optional)\n  - [Docker Image](#docker-image)\n    - [Using pre-built images](#using-pre-built-images)\n    - [Building the image yourself](#building-the-image-yourself)\n  - [Building the Documentation](#building-the-documentation)\n  - [Previous Versions](#previous-versions)\n- [Getting Started](#getting-started)\n- [Resources](#resources)\n- [Communication](#communication)\n- [Releases and Contributing](#releases-and-contributing)\n- [The Team](#the-team)\n- [License](#license)\n\n<!-- tocstop -->\n\n## More About PyTorch\n\n[Learn the basics of PyTorch](https://pytorch.org/tutorials/beginner/basics/intro.html)\n\nAt a granular level, PyTorch is a library that consists of the following components:\n\n| Component | Description |\n| ---- | --- |\n| [**torch**](https://pytorch.org/docs/stable/torch.html) | A Tensor library like NumPy, with strong GPU support |\n| [**torch.autograd**](https://pytorch.org/docs/stable/autograd.html) | A tape-based automatic differentiation library that supports all differentiable Tensor operations in torch |\n| [**torch.jit**](https://pytorch.org/docs/stable/jit.html) | A compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code  |\n| [**torch.nn**](https://pytorch.org/docs/stable/nn.html) | A neural networks library deeply integrated with autograd designed for maximum flexibility |\n| [**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html) | Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training |\n| [**torch.utils**](https://pytorch.org/docs/stable/data.html) | DataLoader and other utility functions for convenience |\n\nUsually, PyTorch is used either as:\n\n- A replacement for NumPy to use the power of GPUs.\n- A deep learning research platform that provides maximum flexibility and speed.\n\nElaborating Further:\n\n### A GPU-Ready Tensor Library\n\nIf you use NumPy, then you have used Tensors (a.k.a. ndarray).\n\n![Tensor illustration](./docs/source/_static/img/tensor_illustration.png)\n\nPyTorch provides Tensors that can live either on the CPU or the GPU and accelerates the\ncomputation by a huge amount.\n\nWe provide a wide variety of tensor routines to accelerate and fit your scientific computation needs\nsuch as slicing, indexing, mathematical operations, linear algebra, reductions.\nAnd they are fast!\n\n### Dynamic Neural Networks: Tape-Based Autograd\n\nPyTorch has a unique way of building neural networks: using and replaying a tape recorder.\n\nMost frameworks such as TensorFlow, Theano, Caffe, and CNTK have a static view of the world.\nOne has to build a neural network and reuse the same structure again and again.\nChanging the way the network behaves means that one has to start from scratch.\n\nWith PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to\nchange the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes\nfrom several research papers on this topic, as well as current and past work such as\n[torch-autograd](https://github.com/twitter/torch-autograd),\n[autograd](https://github.com/HIPS/autograd),\n[Chainer](https://chainer.org), etc.\n\nWhile this technique is not unique to PyTorch, it's one of the fastest implementations of it to date.\nYou get the best of speed and flexibility for your crazy research.\n\n![Dynamic graph](https://github.com/pytorch/pytorch/raw/main/docs/s...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "netdata",
    "full_name": "netdata/netdata",
    "description": "X-Ray Vision for your infrastructure!",
    "url": "https://github.com/netdata/netdata",
    "stars": 74379,
    "forks": 6046,
    "created_at": "2013-06-17",
    "updated_at": "2025-04-26",
    "topics": [
      "monitoring",
      "docker",
      "statsd",
      "kubernetes",
      "cncf",
      "prometheus",
      "netdata",
      "devops",
      "observability",
      "alerting",
      "influxdb",
      "grafana",
      "data-visualization",
      "database",
      "linux",
      "machine-learning",
      "mysql",
      "postgresql",
      "mongodb",
      "raspberry-pi"
    ],
    "primary_language": "C",
    "languages": {
      "C": 23412195,
      "Go": 5796002,
      "Shell": 869464,
      "Jupyter Notebook": 826673,
      "Python": 785742,
      "C++": 540964,
      "CMake": 233352,
      "HTML": 31489,
      "Batchfile": 10873,
      "Yacc": 7186,
      "Rich Text Format": 6985,
      "POV-Ray SDL": 6860,
      "PowerShell": 6115,
      "Dockerfile": 5247,
      "Makefile": 3014,
      "JavaScript": 2983
    },
    "readme_content": "<p align=\"center\">\n<a href=\"https://www.netdata.cloud#gh-light-mode-only\">\n  <img src=\"https://www.netdata.cloud/img/readme-images/netdata_readme_logo_light.png\" alt=\"Netdata\" width=\"300\"/>\n</a>\n<a href=\"https://www.netdata.cloud#gh-dark-mode-only\">\n  <img src=\"https://www.netdata.cloud/img/readme-images/netdata_readme_logo_dark.png\" alt=\"Netdata\" width=\"300\"/>\n</a>\n</p>\n<h3 align=\"center\">X-Ray Vision for your infrastructure!</h3>\n<h4 align=\"center\">Every Metric, Every Second. No BS.</h4>\n\n<br />\n<p align=\"center\">\n  <a href=\"https://github.com/netdata/netdata/\"><img src=\"https://img.shields.io/github/stars/netdata/netdata?style=social\" alt=\"GitHub Stars\"></a>\n  <br />\n  <a href=\"https://app.netdata.cloud/spaces/netdata-demo?utm_campaign=github_readme_demo_badge\"><img src=\"https://img.shields.io/badge/Live%20Demo-green\" alt=\"Live Demo\"></a>\n  <a href=\"https://github.com/netdata/netdata/releases/latest\"><img src=\"https://img.shields.io/github/release/netdata/netdata.svg\" alt=\"Latest release\"></a>\n  <a href=\"https://github.com/netdata/netdata-nightlies/releases/latest\"><img src=\"https://img.shields.io/github/release/netdata/netdata-nightlies.svg\" alt=\"Latest nightly build\"></a>\n  <br/>\n  <a href=\"https://community.netdata.cloud\"><img alt=\"Discourse topics\" src=\"https://img.shields.io/discourse/topics?server=https%3A%2F%2Fcommunity.netdata.cloud%2F&logo=discourse&label=discourse%20forum\"></a>\n  <a href=\"https://github.com/netdata/netdata/discussions\"><img alt=\"GitHub Discussions\" src=\"https://img.shields.io/github/discussions/netdata/netdata?logo=github&label=github%20discussions\"></a>\n  <br/>\n  <a href=\"https://bestpractices.coreinfrastructure.org/projects/2231\"><img src=\"https://bestpractices.coreinfrastructure.org/projects/2231/badge\" alt=\"CII Best Practices\"></a>\n  <a href=\"https://scan.coverity.com/projects/netdata-netdata?tab=overview\"><img alt=\"Coverity Scan\" src=\"https://img.shields.io/coverity/scan/netdata\"></a>\n</p>\n\n<p align=\"center\"><b>Visit the <a href=\"https://www.netdata.cloud\">Project's Home Page</a></b></p>\n\n<hr class=\"solid\">\n\nMENU: **[GETTING STARTED](#getting-started)** | **[HOW IT WORKS](#how-it-works)** | **[FAQ](#faq)** | **[DOCS](#book-documentation)** | **[COMMUNITY](#tada-community)** | **[CONTRIBUTE](#pray-contribute)** | **[LICENSE](#license)**\n\n> **Important** :bulb:<br/>\n> People get addicted to Netdata. Once you use it on your systems, **there's no going back!**<br/>\n\n[![Platforms](https://img.shields.io/badge/Platforms-Linux%20%7C%20macOS%20%7C%20FreeBSD%20%7C%20Windows-blue)]()\n\n**TL;DR**\n\nNetdata is an open-source, real-time infrastructure monitoring platform designed for instant visibility and proactive troubleshooting across your entire IT environment. It captures every metric, every second, providing detailed insights into systems, containers, applications, and logs without compromising performance or requiring complex setup.\n\nKey Advantages:\n\n- **Instant Insights**<br/>Real-time, per-second metrics and visualizations for rapid problem detection.\n- **Automated and Zero-Configuration**<br/>Easy deployment with immediate monitoring—no complex setup needed.\n- **ML-Driven Intelligence**<br/>Built-in machine learning automatically detects anomalies, predicts issues, and assists in root-cause analysis.\n- **Highly Efficient**<br/>Proven minimal resource usage, exceptional scalability, and best-in-class energy efficiency validated by independent research.\n- **Distributed & Secure**<br/>Data stays securely within your infrastructure; no centralization required.\n\nNetdata complements or replaces traditional monitoring tools, offering significant performance and usability advantages over Prometheus, Datadog, Dynatrace, and similar products, while remaining fully compatible and integration-friendly.\n\nDesigned for organizations seeking simplified operations, reduced overhead, and cost-effective monitoring solutions, Netdata provides a comprehensive, scalable, and user-friendly approach to observability.\n\n**:sparkles: Key Features**:\n\n- **Real-Time**\n\n  Per-second data collection and real-time processing provides immediate visibility into your infrastructure's behavior.\n\n  _**Unique**: Netdata works in a beat, and everything happens at this pace. You hit Enter in the terminal, and just a second later, the result appears on the dashboard._\n\n- **Zero-Configuration**\n\n  Start monitoring in minutes with automatic detection and discovery, fully automated dashboards, and hundreds of pre-configured alerts.\n\n  _**Unique**: Netdata auto-discovers everything on the nodes it runs. All kernel technologies, all processes, all applications, all containers, all hardware components. And with its dynamic configuration, any changes can be done via the dashboard._\n\n- **ML-Powered**\n\n  Unsupervised anomaly detection and pattern recognition for all metrics, providing advanced correlations and instant root cause analysis.\n\n  _**Unique**: Netdata trains multiple true ML models per metric, at the edge, for all ...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "ML-For-Beginners",
    "full_name": "microsoft/ML-For-Beginners",
    "description": "12 weeks, 26 lessons, 52 quizzes, classic Machine Learning for all",
    "url": "https://github.com/microsoft/ML-For-Beginners",
    "stars": 72006,
    "forks": 15544,
    "created_at": "2021-03-03",
    "updated_at": "2025-04-26",
    "topics": [
      "ml",
      "data-science",
      "machine-learning",
      "machine-learning-algorithms",
      "machinelearning",
      "python",
      "machinelearning-python",
      "scikit-learn",
      "scikit-learn-python",
      "r",
      "education",
      "microsoft-for-beginners"
    ],
    "primary_language": "HTML",
    "languages": {
      "HTML": 69312552,
      "Jupyter Notebook": 5468130,
      "Python": 54534,
      "Vue": 3931,
      "JavaScript": 1960,
      "Dockerfile": 1371,
      "CSS": 394
    },
    "readme_content": "[![GitHub license](https://img.shields.io/github/license/microsoft/ML-For-Beginners.svg)](https://github.com/microsoft/ML-For-Beginners/blob/master/LICENSE)\n[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/graphs/contributors/)\n[![GitHub issues](https://img.shields.io/github/issues/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/issues/)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/pulls/)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)\n\n[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/ML-For-Beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/ML-For-Beginners/watchers/)\n[![GitHub forks](https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ML-For-Beginners/network/)\n[![GitHub stars](https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ML-For-Beginners/stargazers/)\n\n[![](https://dcbadge.vercel.app/api/server/ByRwuEEgH4)](https://discord.gg/zxKYvhSnVp?WT.mc_id=academic-000002-leestott)\n\n# Machine Learning for Beginners - A Curriculum\n\n> 🌍 Travel around the world as we explore Machine Learning by means of world cultures 🌍\n\nCloud Advocates at Microsoft are pleased to offer a 12-week, 26-lesson curriculum all about **Machine Learning**. In this curriculum, you will learn about what is sometimes called **classic machine learning**, using primarily Scikit-learn as a library and avoiding deep learning, which is covered in our [AI for Beginners' curriculum](https://aka.ms/ai4beginners). Pair these lessons with our ['Data Science for Beginners' curriculum](https://aka.ms/ds4beginners), as well!\n\nTravel with us around the world as we apply these classic techniques to data from many areas of the world. Each lesson includes pre- and post-lesson quizzes, written instructions to complete the lesson, a solution, an assignment, and more. Our project-based pedagogy allows you to learn while building, a proven way for new skills to 'stick'.\n\n**✍️ Hearty thanks to our authors** Jen Looper, Stephen Howell, Francesca Lazzeri, Tomomi Imura, Cassie Breviu, Dmitry Soshnikov, Chris Noring, Anirban Mukherjee, Ornella Altunyan, Ruth Yakubu and Amy Boyd\n\n**🎨 Thanks as well to our illustrators** Tomomi Imura, Dasani Madipalli, and Jen Looper\n\n**🙏 Special thanks 🙏 to our Microsoft Student Ambassador authors, reviewers, and content contributors**, notably Rishit Dagli, Muhammad Sakib Khan Inan, Rohan Raj, Alexandru Petrescu, Abhishek Jaiswal, Nawrin Tabassum, Ioan Samuila, and Snigdha Agarwal\n\n**🤩 Extra gratitude to Microsoft Student Ambassadors Eric Wanjau, Jasleen Sondhi, and Vidushi Gupta for our R lessons!**\n\n# Getting Started\n\nFollow these steps:\n1. **Fork the Repository**: Click on the \"Fork\" button at the top-right corner of this page.\n2. **Clone the Repository**:   `git clone https://github.com/microsoft/ML-For-Beginners.git`\n\n> [find all additional resources for this course in our Microsoft Learn collection](https://learn.microsoft.com/en-us/collections/qrqzamz1nn2wx3?WT.mc_id=academic-77952-bethanycheum)\n\n\n**[Students](https://aka.ms/student-page)**, to use this curriculum, fork the entire repo to your own GitHub account and complete the exercises on your own or with a group:\n\n- Start with a pre-lecture quiz.\n- Read the lecture and complete the activities, pausing and reflecting at each knowledge check.\n- Try to create the projects by comprehending the lessons rather than running the solution code; however that code is available in the `/solution` folders in each project-oriented lesson.\n- Take the post-lecture quiz.\n- Complete the challenge.\n- Complete the assignment.\n- After completing a lesson group, visit the [Discussion Board](https://github.com/microsoft/ML-For-Beginners/discussions) and \"learn out loud\" by filling out the appropriate PAT rubric. A 'PAT' is a Progress Assessment Tool that is a rubric you fill out to further your learning. You can also react to other PATs so we can learn together.\n\n> For further study, we recommend following these [Microsoft Learn](https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-77952-leestott) modules and learning paths.\n\n**Teachers**, we have [included some suggestions](for-teachers.md) on how to use this curriculum.\n\n---\n\n## Video walkthroughs\n\nSome of the lessons are available as short form video. You can find all these in-line in the lessons, or on the [ML for Beginners playlist on the Microsoft Developer YouTube channel](https://aka.ms/ml-beginners-videos) by clicking the image below.\n\n[![ML for beginners banner](./ml-for-beginners-video-banner.png)](https://aka.ms/m...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "d2l-zh",
    "full_name": "d2l-ai/d2l-zh",
    "description": "《动手学深度学习》：面向中文读者、能运行、可讨论。中英文版被70多个国家的500多所大学用于教学。",
    "url": "https://github.com/d2l-ai/d2l-zh",
    "stars": 68791,
    "forks": 11582,
    "created_at": "2017-08-23",
    "updated_at": "2025-04-26",
    "topics": [
      "deep-learning",
      "book",
      "notebook",
      "natural-language-processing",
      "computer-vision",
      "machine-learning",
      "python",
      "chinese"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 427646,
      "HTML": 68515,
      "TeX": 61055,
      "Shell": 10935
    },
    "readme_content": "# 动手学深度学习（Dive into Deep Learning，D2L.ai）\n\n[第二版：zh.D2L.ai](https://zh.d2l.ai)  | [第一版：zh-v1.D2L.ai](https://zh-v1.d2l.ai/) |  安装和使用书中源代码： [第二版](https://zh.d2l.ai/chapter_installation/index.html) [第一版](https://zh-v1.d2l.ai/chapter_prerequisite/install.html)\n\n<h5 align=\"center\"><i>理解深度学习的最佳方法是学以致用。</i></h5>\n\n<p align=\"center\">\n  <img width=\"200\"  src=\"static/frontpage/_images/eq.jpg\">\n  <img width=\"200\"  src=\"static/frontpage/_images/figure.jpg\">\n  <img width=\"200\"  src=\"static/frontpage/_images/code.jpg\">\n  <img width=\"200\"  src=\"static/frontpage/_images/notebook.gif\">\n</p>\n\n本开源项目代表了我们的一种尝试：我们将教给读者概念、背景知识和代码；我们将在同一个地方阐述剖析问题所需的批判性思维、解决问题所需的数学知识，以及实现解决方案所需的工程技能。\n\n我们的目标是创建一个为实现以下目标的统一资源：\n1. 所有人均可在网上免费获取；\n1. 提供足够的技术深度，从而帮助读者实际成为深度学习应用科学家：既理解数学原理，又能够实现并不断改进方法；\n1. 包含可运行的代码，为读者展示如何在实际中解决问题。这样不仅直接将数学公式对应成实际代码，而且可以修改代码、观察结果并及时获取经验；\n1. 允许我们和整个社区不断快速迭代内容，从而紧跟仍在高速发展的深度学习领域；\n1. 由包含有关技术细节问答的论坛作为补充，使大家可以相互答疑并交换经验。\n\n<h5 align=\"center\">将本书（中英文版）用作教材或参考书的大学</h5>\n<p align=\"center\">\n  <img width=\"400\"  src=\"https://d2l.ai/_images/map.png\">\n</p>\n\n如果本书对你有帮助，请Star (★) 本仓库或引用本书的英文版：\n\n```\n@book{zhang2023dive,\n    title={Dive into Deep Learning},\n    author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},\n    publisher={Cambridge University Press},\n    note={\\url{https://D2L.ai}},\n    year={2023}\n}\n```\n\n## 本书的英文版\n\n虽然纸质书已出版，但深度学习领域依然在迅速发展。为了得到来自更广泛的英文开源社区的帮助，从而提升本书质量，本书的新版将继续用英文编写，并搬回中文版。\n\n欢迎关注本书的[英文开源项目](https://github.com/d2l-ai/d2l-en)。\n\n## 中英文教学资源\n\n加州大学伯克利分校 2019 年春学期 [*Introduction to Deep Learning* 课程](http://courses.d2l.ai/berkeley-stat-157/index.html)教材（同时提供含教学视频地址的[中文版课件](https://github.com/d2l-ai/berkeley-stat-157/tree/master/slides-zh)）。\n\n## 学术界推荐\n\n> <p>\"Dive into this book if you want to dive into deep learning!\"</p>\n> <b>&mdash; 韩家炜，ACM 院士、IEEE 院士，美国伊利诺伊大学香槟分校计算机系 Michael Aiken Chair 教授</b>\n\n> <p>\"This is a highly welcome addition to the machine learning literature.\"</p>\n> <b>&mdash; Bernhard Schölkopf，ACM 院士、德国国家科学院院士，德国马克斯•普朗克研究所智能系统院院长</b>\n\n> <p>\"书中代码可谓‘所学即所用’。\"</p>\n> <b>&mdash; 周志华，ACM 院士、IEEE 院士、AAAS 院士，南京大学计算机科学与技术系主任</b>\n\n> <p>\"这本书可以帮助深度学习实践者快速提升自己的能力。\"</p>\n> <b>&mdash; 张潼，ASA 院士、IMS 院士，香港科技大学计算机系和数学系教授</b>\n\n## 工业界推荐\n\n> <p>\"一本优秀的深度学习教材，值得任何想了解深度学习何以引爆人工智能革命的人关注。\"</p>\n> <b>&mdash; 黄仁勋，NVIDIA创始人 & CEO</b>\n\n> <p>\"《动手学深度学习》是最适合工业界研发工程师学习的。我毫无保留地向广大的读者们强烈推荐。\"</p>\n> <b>&mdash; 余凯，地平线公司创始人 & CEO</b>\n\n> <p>\"强烈推荐这本书！我特别赞赏这种手脑一体的学习方式。\"</p>\n> <b>&mdash; 漆远，复旦大学“浩清”教授、人工智能创新与产业研究院院长</b>\n\n> <p>\"《动手学深度学习》是一本很容易让学习者上瘾的书。\"</p>\n> <b>&mdash; 沈强，将门创投创始合伙人</b>\n\n## 贡献\n\n感谢[社区贡献者们](https://github.com/d2l-ai/d2l-zh/graphs/contributors)为每一位读者改进这本开源书。\n\n[如何贡献](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/contributing.html) | [致谢](https://zh.d2l.ai/chapter_preface/index.html) | [讨论或报告问题](https://discuss.d2l.ai/c/chinese-version/16) | [其他](INFO.md)\n",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "cs-video-courses",
    "full_name": "Developer-Y/cs-video-courses",
    "description": "List of Computer Science courses with video lectures.",
    "url": "https://github.com/Developer-Y/cs-video-courses",
    "stars": 68567,
    "forks": 9278,
    "created_at": "2016-10-21",
    "updated_at": "2025-04-26",
    "topics": [
      "computer-science",
      "algorithms",
      "systems",
      "databases",
      "machine-learning",
      "web-development",
      "security",
      "computer-architecture",
      "bioinformatics",
      "robotics",
      "embedded-systems",
      "database-systems",
      "computer-vision",
      "quantum-computing",
      "computational-biology",
      "computational-physics",
      "deep-learning",
      "reinforcement-learning"
    ],
    "primary_language": null,
    "languages": {},
    "readme_content": "<!-- omit in toc -->\n# Computer Science courses with video lectures\n\n<!-- omit in toc -->\n## Introduction\n\n- Please check [NOTES](https://github.com/Developer-Y/cs-video-courses/blob/master/NOTES.md) for general information about this list.\n- Please refer [CONTRIBUTING.md](https://github.com/Developer-Y/cs-video-courses/blob/master/CONTRIBUTING.md) for contribution guidelines.\n- Please feel free to raise any genuine issue you may have, however, it has been noticed that few people open empty issues to raise their GitHub contribution on their account. Such spammers will be blocked. \n- You are welcome to contribute, please create PR for actual college/University level courses. Please do not add links for small MOOCs, basic tutorials, or advertisements for some sites/channels.\n\n------------------------------\n\nTable of Contents\n\n------------------------------\n\n- [Introduction to Computer Science](#introduction-to-computer-science)\n- [Data Structures and Algorithms](#data-structures-and-algorithms)\n- [Systems Programming](#systems-programming)\n  * [Operating Systems](#operating-systems)\n  * [Distributed Systems](#distributed-systems)\n  * [Real-Time Systems](#real-time-systems) \n- [Database Systems](#database-systems)\n- [Software Engineering](#software-engineering)\n  * [Object Oriented Design](#object-oriented-design)\n  * [Software Engineering](#software-engineering)\n  * [Software Architecture](#software-architecture)\n  * [Concurrency](#concurrency)\n  * [Mobile Application Development](#mobile-application-development)\n- [Artificial Intelligence](#artificial-intelligence)\n- [Machine Learning](#machine-learning)\n  * [Introduction to Machine Learning](#introduction-to-machine-learning)\n  * [Data Mining](#data-mining)\n  * [Probabilistic Graphical Modeling](#probabilistic-graphical-modeling)\n  * [Deep Learning](#deep-learning)\n  * [Reinforcement Learning](#reinforcement-learning)\n  * [Advanced Machine Learning](#advanced-machine-learning)\n  * [Natural Language Processing](#natural-language-processing)\n  * [Generative AI](#generative-ai)\n  * [Computer Vision](#computer-vision)\n  * [Time Series Analysis](#time-series-analysis)\n  * [Optimization](#optimization)\n  * [Unsupervised Learning](#unsupervised-learning)\n  * [Misc Machine Learning Topics](#misc-machine-learning-topics)\n- [Computer Networks](#computer-networks)\n- [Math for Computer Scientist](#math-for-computer-scientist)\n- [Web Programming and Internet Technologies](#web-programming-and-internet-technologies)\n- [Theoretical CS and Programming Languages](#theoretical-cs-and-programming-languages)\n- [Embedded Systems](#embedded-systems)\n- [Real time system evaluation](#real-time-system-evaluation)\n- [Computer Organization and Architecture](#computer-organization-and-architecture)\n- [Security](#security)\n- [Computer Graphics](#computer-graphics)\n- [Image Processing and Computer Vision](#image-processing-and-computer-vision)\n- [Computational Physics](#computational-physics)\n- [Computational Biology](#computational-biology)\n- [Quantum Computing](#quantum-computing)\n- [Robotics and Control](#robotics-and-control)\n- [Computational Finance](#computational-finance)\n- [Blockchain Development](#blockchain-development)\n- [Misc](#misc)\n\n<!-- omit in toc -->\n## Courses\n\n------------------------------\n\n### Introduction to Computer Science\n\n- [CS 10 - The Beauty and Joy of Computing - Spring 2015 - Dan Garcia - UC Berkeley InfoCoBuild](http://www.infocobuild.com/education/audio-video-courses/computer-science/cs10-spring2015-berkeley.html)\n- [6.0001 - Introduction to Computer Science and Programming in Python - MIT OCW](https://ocw.mit.edu/courses/6-0001-introduction-to-computer-science-and-programming-in-python-fall-2016/video_galleries/lecture-videos/)\n- [6.001 - Structure and Interpretation of Computer Programs, MIT](https://ocw.mit.edu/courses/6-001-structure-and-interpretation-of-computer-programs-spring-2005/video_galleries/video-lectures/)\n- [Introduction to Computational Thinking - MIT](https://computationalthinking.mit.edu/Fall22/)\n- [CS 50 - Introduction to Computer Science, Harvard University](https://online-learning.harvard.edu/course/cs50-introduction-computer-science) ([cs50.tv](http://cs50.tv/2017/fall/))\n- [CS50R - Introduction to Programming with R](https://cs50.harvard.edu/r/2024/) ([Lecture Videos](https://www.youtube.com/playlist?list=PLhQjrBD2T382yfNp_-xzX244d-O9W6YmD))\n- [CS 61A - Structure and Interpretation of Computer Programs [Python], UC Berkeley](https://cs61a.org/)\n- [CPSC 110 - Systematic Program Design [Racket], University of British Columbia](https://www.youtube.com/channel/UC7dEjIUwSxSNcW4PqNRQW8w/playlists?view=1&flow=grid&sort=da)\n- [CS50's Understanding Technology](https://www.youtube.com/playlist?list=PLhQjrBD2T382p8amnvUp1rws1p7n7gJ2p)\n- [CSE 142 Computer Programming I (Java Programming), Spring 2016 - University of Washington](https://courses.cs.washington.edu/courses/cse142/16sp/calendar.shtml)\n- [CS 1301 Intro to computing - Gatech](https:/...",
    "owner_type": "User",
    "is_archived": false
  },
  {
    "name": "tesseract",
    "full_name": "tesseract-ocr/tesseract",
    "description": "Tesseract Open Source OCR Engine (main repository)",
    "url": "https://github.com/tesseract-ocr/tesseract",
    "stars": 66407,
    "forks": 9842,
    "created_at": "2014-08-12",
    "updated_at": "2025-04-26",
    "topics": [
      "tesseract",
      "tesseract-ocr",
      "ocr",
      "lstm",
      "machine-learning",
      "ocr-engine",
      "hacktoberfest"
    ],
    "primary_language": "C++",
    "languages": {
      "C++": 7501719,
      "Java": 74136,
      "Makefile": 67075,
      "CMake": 64731,
      "NSIS": 51446,
      "C": 41152,
      "Shell": 29511,
      "M4": 3378,
      "Python": 2520
    },
    "readme_content": "# Tesseract OCR\n\n[![Coverity Scan Build Status](https://scan.coverity.com/projects/tesseract-ocr/badge.svg)](https://scan.coverity.com/projects/tesseract-ocr)\n[![CodeQL](https://github.com/tesseract-ocr/tesseract/workflows/CodeQL/badge.svg)](https://github.com/tesseract-ocr/tesseract/security/code-scanning)\n[![OSS-Fuzz](https://img.shields.io/badge/oss--fuzz-fuzzing-brightgreen)](https://issues.oss-fuzz.com/issues?q=is:open%20title:tesseract-ocr)\n\\\n[![GitHub license](https://img.shields.io/badge/license-Apache--2.0-blue.svg)](https://raw.githubusercontent.com/tesseract-ocr/tesseract/main/LICENSE)\n[![Downloads](https://img.shields.io/badge/download-all%20releases-brightgreen.svg)](https://github.com/tesseract-ocr/tesseract/releases/)\n\n## Table of Contents\n\n* [Tesseract OCR](#tesseract-ocr)\n  * [About](#about)\n  * [Brief history](#brief-history)\n  * [Installing Tesseract](#installing-tesseract)\n  * [Running Tesseract](#running-tesseract)\n  * [For developers](#for-developers)\n  * [Support](#support)\n  * [License](#license)\n  * [Dependencies](#dependencies)\n  * [Latest Version of README](#latest-version-of-readme)\n\n## About\n\nThis package contains an **OCR engine** - `libtesseract` and a **command line program** - `tesseract`.\n\nTesseract 4 adds a new neural net (LSTM) based [OCR engine](https://en.wikipedia.org/wiki/Optical_character_recognition) which is focused on line recognition, but also still supports the legacy Tesseract OCR engine of Tesseract 3 which works by recognizing character patterns. Compatibility with Tesseract 3 is enabled by using the Legacy OCR Engine mode (--oem 0).\nIt also needs [traineddata](https://tesseract-ocr.github.io/tessdoc/Data-Files.html) files which support the legacy engine, for example those from the [tessdata](https://github.com/tesseract-ocr/tessdata) repository.\n\nStefan Weil is the current lead developer. Ray Smith was the lead developer until 2018. The maintainer is Zdenko Podobny. For a list of contributors see [AUTHORS](https://github.com/tesseract-ocr/tesseract/blob/main/AUTHORS)\nand GitHub's log of [contributors](https://github.com/tesseract-ocr/tesseract/graphs/contributors).\n\nTesseract has **unicode (UTF-8) support**, and can **recognize [more than 100 languages](https://tesseract-ocr.github.io/tessdoc/Data-Files-in-different-versions.html)** \"out of the box\".\n\nTesseract supports **[various image formats](https://tesseract-ocr.github.io/tessdoc/InputFormats)** including PNG, JPEG and TIFF.\n\nTesseract supports **various output formats**: plain text, hOCR (HTML), PDF, invisible-text-only PDF, TSV, ALTO and PAGE.\n\nYou should note that in many cases, in order to get better OCR results, you'll need to **[improve the quality](https://tesseract-ocr.github.io/tessdoc/ImproveQuality.html) of the image** you are giving Tesseract.\n\nThis project **does not include a GUI application**. If you need one, please see the [3rdParty](https://tesseract-ocr.github.io/tessdoc/User-Projects-%E2%80%93-3rdParty.html) documentation.\n\nTesseract **can be trained to recognize other languages**.\nSee [Tesseract Training](https://tesseract-ocr.github.io/tessdoc/Training-Tesseract.html) for more information.\n\n## Brief history\n\nTesseract was originally developed at Hewlett-Packard Laboratories Bristol UK and at Hewlett-Packard Co, Greeley Colorado USA between 1985 and 1994, with some more changes made in 1996 to port to Windows, and some C++izing in 1998. In 2005 Tesseract was open sourced by HP. From 2006 until November 2018 it was developed by Google.\n\nMajor version 5 is the current stable version and started with release\n[5.0.0](https://github.com/tesseract-ocr/tesseract/releases/tag/5.0.0) on November 30, 2021. Newer minor versions and bugfix versions are available from\n[GitHub](https://github.com/tesseract-ocr/tesseract/releases/).\n\nLatest source code is available from [main branch on GitHub](https://github.com/tesseract-ocr/tesseract/tree/main).\nOpen issues can be found in [issue tracker](https://github.com/tesseract-ocr/tesseract/issues),\nand [planning documentation](https://tesseract-ocr.github.io/tessdoc/Planning.html).\n\nSee **[Release Notes](https://tesseract-ocr.github.io/tessdoc/ReleaseNotes.html)**\nand **[Change Log](https://github.com/tesseract-ocr/tesseract/blob/main/ChangeLog)** for more details of the releases.\n\n## Installing Tesseract\n\nYou can either [Install Tesseract via pre-built binary package](https://tesseract-ocr.github.io/tessdoc/Installation.html)\nor [build it from source](https://tesseract-ocr.github.io/tessdoc/Compiling.html).\n\nBefore building Tesseract from source, please check that your system has a compiler which is one of the [supported compilers](https://tesseract-ocr.github.io/tessdoc/supported-compilers.html).\n\n## Running Tesseract\n\nBasic **[command line usage](https://tesseract-ocr.github.io/tessdoc/Command-Line-Usage.html)**:\n\n    tesseract imagename outputbase [-l lang] [--oem ocrenginemode] [--psm pagesegmode] [configfiles...]\n\nFor more information about the var...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "keras",
    "full_name": "keras-team/keras",
    "description": "Deep Learning for humans",
    "url": "https://github.com/keras-team/keras",
    "stars": 62921,
    "forks": 19575,
    "created_at": "2015-03-28",
    "updated_at": "2025-04-26",
    "topics": [
      "deep-learning",
      "tensorflow",
      "neural-networks",
      "machine-learning",
      "data-science",
      "python",
      "jax",
      "pytorch"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 8800074,
      "Shell": 3904
    },
    "readme_content": "# Keras 3: Deep Learning for Humans\n\nKeras 3 is a multi-backend deep learning framework, with support for JAX, TensorFlow, PyTorch, and OpenVINO (for inference-only).\nEffortlessly build and train models for computer vision, natural language processing, audio processing,\ntimeseries forecasting, recommender systems, etc.\n\n- **Accelerated model development**: Ship deep learning solutions faster thanks to the high-level UX of Keras\nand the availability of easy-to-debug runtimes like PyTorch or JAX eager execution.\n- **State-of-the-art performance**: By picking the backend that is the fastest for your model architecture (often JAX!),\nleverage speedups ranging from 20% to 350% compared to other frameworks. [Benchmark here](https://keras.io/getting_started/benchmarks/).\n- **Datacenter-scale training**: Scale confidently from your laptop to large clusters of GPUs or TPUs.\n\nJoin nearly three million developers, from burgeoning startups to global enterprises, in harnessing the power of Keras 3.\n\n\n## Installation\n\n### Install with pip\n\nKeras 3 is available on PyPI as `keras`. Note that Keras 2 remains available as the `tf-keras` package.\n\n1. Install `keras`:\n\n```\npip install keras --upgrade\n```\n\n2. Install backend package(s).\n\nTo use `keras`, you should also install the backend of choice: `tensorflow`, `jax`, or `torch`.\nNote that `tensorflow` is required for using certain Keras 3 features: certain preprocessing layers\nas well as `tf.data` pipelines.\n\n### Local installation\n\n#### Minimal installation\n\nKeras 3 is compatible with Linux and MacOS systems. For Windows users, we recommend using WSL2 to run Keras.\nTo install a local development version:\n\n1. Install dependencies:\n\n```\npip install -r requirements.txt\n```\n\n2. Run installation command from the root directory.\n\n```\npython pip_build.py --install\n```\n\n3. Run API generation script when creating PRs that update `keras_export` public APIs:\n\n```\n./shell/api_gen.sh\n```\n\n#### Adding GPU support\n\nThe `requirements.txt` file will install a CPU-only version of TensorFlow, JAX, and PyTorch. For GPU support, we also\nprovide a separate `requirements-{backend}-cuda.txt` for TensorFlow, JAX, and PyTorch. These install all CUDA\ndependencies via `pip` and expect a NVIDIA driver to be pre-installed. We recommend a clean python environment for each\nbackend to avoid CUDA version mismatches. As an example, here is how to create a Jax GPU environment with `conda`:\n\n```shell\nconda create -y -n keras-jax python=3.10\nconda activate keras-jax\npip install -r requirements-jax-cuda.txt\npython pip_build.py --install\n```\n\n## Configuring your backend\n\nYou can export the environment variable `KERAS_BACKEND` or you can edit your local config file at `~/.keras/keras.json`\nto configure your backend. Available backend options are: `\"tensorflow\"`, `\"jax\"`, `\"torch\"`, `\"openvino\"`. Example:\n\n```\nexport KERAS_BACKEND=\"jax\"\n```\n\nIn Colab, you can do:\n\n```python\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n\nimport keras\n```\n\n**Note:** The backend must be configured before importing `keras`, and the backend cannot be changed after \nthe package has been imported.\n\n**Note:** The OpenVINO backend is an inference-only backend, meaning it is designed only for running model\npredictions using `model.predict()` method.\n\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/).\n",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "scikit-learn",
    "full_name": "scikit-learn/scikit-learn",
    "description": "scikit-learn: machine learning in Python",
    "url": "https://github.com/scikit-learn/scikit-learn",
    "stars": 61858,
    "forks": 25776,
    "created_at": "2010-08-17",
    "updated_at": "2025-04-26",
    "topics": [
      "machine-learning",
      "python",
      "statistics",
      "data-science",
      "data-analysis"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 12378245,
      "Cython": 728558,
      "C++": 147428,
      "Shell": 47591,
      "C": 41895,
      "Meson": 32407,
      "CSS": 11277,
      "Makefile": 1034
    },
    "readme_content": ".. -*- mode: rst -*-\n\n|Azure| |Codecov| |CircleCI| |Nightly wheels| |Ruff| |PythonVersion| |PyPi| |DOI| |Benchmark|\n\n.. |Azure| image:: https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=main\n   :target: https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&branchName=main\n\n.. |CircleCI| image:: https://circleci.com/gh/scikit-learn/scikit-learn/tree/main.svg?style=shield\n   :target: https://circleci.com/gh/scikit-learn/scikit-learn\n\n.. |Codecov| image:: https://codecov.io/gh/scikit-learn/scikit-learn/branch/main/graph/badge.svg?token=Pk8G9gg3y9\n   :target: https://codecov.io/gh/scikit-learn/scikit-learn\n\n.. |Nightly wheels| image:: https://github.com/scikit-learn/scikit-learn/workflows/Wheel%20builder/badge.svg?event=schedule\n   :target: https://github.com/scikit-learn/scikit-learn/actions?query=workflow%3A%22Wheel+builder%22+event%3Aschedule\n\n.. |Ruff| image:: https://img.shields.io/badge/code%20style-ruff-000000.svg\n   :target: https://github.com/astral-sh/ruff\n\n.. |PythonVersion| image:: https://img.shields.io/pypi/pyversions/scikit-learn.svg\n   :target: https://pypi.org/project/scikit-learn/\n\n.. |PyPi| image:: https://img.shields.io/pypi/v/scikit-learn\n   :target: https://pypi.org/project/scikit-learn\n\n.. |DOI| image:: https://zenodo.org/badge/21369/scikit-learn/scikit-learn.svg\n   :target: https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn\n\n.. |Benchmark| image:: https://img.shields.io/badge/Benchmarked%20by-asv-blue\n   :target: https://scikit-learn.org/scikit-learn-benchmarks\n\n.. |PythonMinVersion| replace:: 3.10\n.. |NumPyMinVersion| replace:: 1.22.0\n.. |SciPyMinVersion| replace:: 1.8.0\n.. |JoblibMinVersion| replace:: 1.2.0\n.. |ThreadpoolctlMinVersion| replace:: 3.1.0\n.. |MatplotlibMinVersion| replace:: 3.5.0\n.. |Scikit-ImageMinVersion| replace:: 0.19.0\n.. |PandasMinVersion| replace:: 1.4.0\n.. |SeabornMinVersion| replace:: 0.9.0\n.. |PytestMinVersion| replace:: 7.1.2\n.. |PlotlyMinVersion| replace:: 5.14.0\n\n.. image:: https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/doc/logos/scikit-learn-logo.png\n  :target: https://scikit-learn.org/\n\n**scikit-learn** is a Python module for machine learning built on top of\nSciPy and is distributed under the 3-Clause BSD license.\n\nThe project was started in 2007 by David Cournapeau as a Google Summer\nof Code project, and since then many volunteers have contributed. See\nthe `About us <https://scikit-learn.org/dev/about.html#authors>`__ page\nfor a list of core contributors.\n\nIt is currently maintained by a team of volunteers.\n\nWebsite: https://scikit-learn.org\n\nInstallation\n------------\n\nDependencies\n~~~~~~~~~~~~\n\nscikit-learn requires:\n\n- Python (>= |PythonMinVersion|)\n- NumPy (>= |NumPyMinVersion|)\n- SciPy (>= |SciPyMinVersion|)\n- joblib (>= |JoblibMinVersion|)\n- threadpoolctl (>= |ThreadpoolctlMinVersion|)\n\n=======\n\nScikit-learn plotting capabilities (i.e., functions start with ``plot_`` and\nclasses end with ``Display``) require Matplotlib (>= |MatplotlibMinVersion|).\nFor running the examples Matplotlib >= |MatplotlibMinVersion| is required.\nA few examples require scikit-image >= |Scikit-ImageMinVersion|, a few examples\nrequire pandas >= |PandasMinVersion|, some examples require seaborn >=\n|SeabornMinVersion| and plotly >= |PlotlyMinVersion|.\n\nUser installation\n~~~~~~~~~~~~~~~~~\n\nIf you already have a working installation of NumPy and SciPy,\nthe easiest way to install scikit-learn is using ``pip``::\n\n    pip install -U scikit-learn\n\nor ``conda``::\n\n    conda install -c conda-forge scikit-learn\n\nThe documentation includes more detailed `installation instructions <https://scikit-learn.org/stable/install.html>`_.\n\n\nChangelog\n---------\n\nSee the `changelog <https://scikit-learn.org/dev/whats_new.html>`__\nfor a history of notable changes to scikit-learn.\n\nDevelopment\n-----------\n\nWe welcome new contributors of all experience levels. The scikit-learn\ncommunity goals are to be helpful, welcoming, and effective. The\n`Development Guide <https://scikit-learn.org/stable/developers/index.html>`_\nhas detailed information about contributing code, documentation, tests, and\nmore. We've included some basic information in this README.\n\nImportant links\n~~~~~~~~~~~~~~~\n\n- Official source code repo: https://github.com/scikit-learn/scikit-learn\n- Download releases: https://pypi.org/project/scikit-learn/\n- Issue tracker: https://github.com/scikit-learn/scikit-learn/issues\n\nSource code\n~~~~~~~~~~~\n\nYou can check the latest sources with the command::\n\n    git clone https://github.com/scikit-learn/scikit-learn.git\n\nContributing\n~~~~~~~~~~~~\n\nTo learn more about making a contribution to scikit-learn, please see our\n`Contributing guide\n<https://scikit-learn.org/dev/developers/contributing.html>`_.\n\nTesting\n~~~~~~~\n\nAfter installation, you can launch the test suite from outside the source\ndirectory (you will need to have ``pytest`` >= |PyTestMinVersion| installed)::\n\n    pytest sklearn\n\nSee th...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "superset",
    "full_name": "apache/superset",
    "description": "Apache Superset is a Data Visualization and Data Exploration Platform",
    "url": "https://github.com/apache/superset",
    "stars": 65914,
    "forks": 14911,
    "created_at": "2015-07-21",
    "updated_at": "2025-04-26",
    "topics": [
      "superset",
      "apache",
      "apache-superset",
      "data-visualization",
      "data-viz",
      "analytics",
      "business-intelligence",
      "data-science",
      "data-engineering",
      "asf",
      "bi",
      "business-analytics",
      "data-analytics",
      "data-analysis",
      "python",
      "react",
      "sql-editor",
      "flask"
    ],
    "primary_language": "Jupyter Notebook",
    "languages": {
      "Jupyter Notebook": 10916999,
      "TypeScript": 10525974,
      "Python": 8815029,
      "JavaScript": 1881652,
      "HTML": 1342551,
      "Shell": 65640,
      "Less": 56942,
      "Dockerfile": 12025,
      "Jinja": 5847,
      "Smarty": 5044,
      "CSS": 4808,
      "Makefile": 4133,
      "Pug": 2969,
      "Mako": 1197
    },
    "readme_content": "<!--\nLicensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n\n  http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n-->\n\n# Superset\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/license/apache-2-0)\n[![Latest Release on Github](https://img.shields.io/github/v/release/apache/superset?sort=semver)](https://github.com/apache/superset/releases/latest)\n[![Build Status](https://github.com/apache/superset/actions/workflows/superset-python-unittest.yml/badge.svg)](https://github.com/apache/superset/actions)\n[![PyPI version](https://badge.fury.io/py/apache_superset.svg)](https://badge.fury.io/py/apache_superset)\n[![Coverage Status](https://codecov.io/github/apache/superset/coverage.svg?branch=master)](https://codecov.io/github/apache/superset)\n[![PyPI](https://img.shields.io/pypi/pyversions/apache_superset.svg?maxAge=2592000)](https://pypi.python.org/pypi/apache_superset)\n[![Get on Slack](https://img.shields.io/badge/slack-join-orange.svg)](http://bit.ly/join-superset-slack)\n[![Documentation](https://img.shields.io/badge/docs-apache.org-blue.svg)](https://superset.apache.org)\n\n<picture width=\"500\">\n  <source\n    width=\"600\"\n    media=\"(prefers-color-scheme: dark)\"\n    src=\"https://superset.apache.org/img/superset-logo-horiz-dark.svg\"\n    alt=\"Superset logo (dark)\"\n  />\n  <img\n    width=\"600\"\n    src=\"https://superset.apache.org/img/superset-logo-horiz-apache.svg\"\n    alt=\"Superset logo (light)\"\n  />\n</picture>\n\nA modern, enterprise-ready business intelligence web application.\n\n[**Why Superset?**](#why-superset) |\n[**Supported Databases**](#supported-databases) |\n[**Installation and Configuration**](#installation-and-configuration) |\n[**Release Notes**](https://github.com/apache/superset/blob/master/RELEASING/README.md#release-notes-for-recent-releases) |\n[**Get Involved**](#get-involved) |\n[**Contributor Guide**](#contributor-guide) |\n[**Resources**](#resources) |\n[**Organizations Using Superset**](https://github.com/apache/superset/blob/master/RESOURCES/INTHEWILD.md)\n\n## Why Superset?\n\nSuperset is a modern data exploration and data visualization platform. Superset can replace or augment proprietary business intelligence tools for many teams. Superset integrates well with a variety of data sources.\n\nSuperset provides:\n\n- A **no-code interface** for building charts quickly\n- A powerful, web-based **SQL Editor** for advanced querying\n- A **lightweight semantic layer** for quickly defining custom dimensions and metrics\n- Out of the box support for **nearly any SQL** database or data engine\n- A wide array of **beautiful visualizations** to showcase your data, ranging from simple bar charts to geospatial visualizations\n- Lightweight, configurable **caching layer** to help ease database load\n- Highly extensible **security roles and authentication** options\n- An **API** for programmatic customization\n- A **cloud-native architecture** designed from the ground up for scale\n\n## Screenshots & Gifs\n\n**Video Overview**\n\n<!-- File hosted here https://github.com/apache/superset-site/raw/lfs/superset-video-4k.mp4 -->\n\n[superset-video-1080p.webm](https://github.com/user-attachments/assets/b37388f7-a971-409c-96a7-90c4e31322e6)\n\n<br/>\n\n**Large Gallery of Visualizations**\n\n<kbd><img title=\"Gallery\" src=\"https://superset.apache.org/img/screenshots/gallery.jpg\"/></kbd><br/>\n\n**Craft Beautiful, Dynamic Dashboards**\n\n<kbd><img title=\"View Dashboards\" src=\"https://superset.apache.org/img/screenshots/slack_dash.jpg\"/></kbd><br/>\n\n**No-Code Chart Builder**\n\n<kbd><img title=\"Slice & dice your data\" src=\"https://superset.apache.org/img/screenshots/explore.jpg\"/></kbd><br/>\n\n**Powerful SQL Editor**\n\n<kbd><img title=\"SQL Lab\" src=\"https://superset.apache.org/img/screenshots/sql_lab.jpg\"/></kbd><br/>\n\n## Supported Databases\n\nSuperset can query data from any SQL-speaking datastore or data engine (Presto, Trino, Athena, [and more](https://superset.apache.org/docs/configuration/databases)) that has a Python DB-API driver and a SQLAlchemy dialect.\n\nHere are some of the major database solutions that are supported:\n\n<p align=\"center\">\n  <img src=\"https://superset.apache.org/img/databases/redshift.png\" alt=\"redshift\" border=\"0\" width=\"200\"/>\n  <img src=\"https://superset.apache.org/img/databases/google-biquery.png\" alt=\"google-biquery\" border=\"0\" width=\"200\"/>\n  <img src=\"ht...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "scikit-learn",
    "full_name": "scikit-learn/scikit-learn",
    "description": "scikit-learn: machine learning in Python",
    "url": "https://github.com/scikit-learn/scikit-learn",
    "stars": 61858,
    "forks": 25776,
    "created_at": "2010-08-17",
    "updated_at": "2025-04-26",
    "topics": [
      "machine-learning",
      "python",
      "statistics",
      "data-science",
      "data-analysis"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 12378245,
      "Cython": 728558,
      "C++": 147428,
      "Shell": 47591,
      "C": 41895,
      "Meson": 32407,
      "CSS": 11277,
      "Makefile": 1034
    },
    "readme_content": ".. -*- mode: rst -*-\n\n|Azure| |Codecov| |CircleCI| |Nightly wheels| |Ruff| |PythonVersion| |PyPi| |DOI| |Benchmark|\n\n.. |Azure| image:: https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=main\n   :target: https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&branchName=main\n\n.. |CircleCI| image:: https://circleci.com/gh/scikit-learn/scikit-learn/tree/main.svg?style=shield\n   :target: https://circleci.com/gh/scikit-learn/scikit-learn\n\n.. |Codecov| image:: https://codecov.io/gh/scikit-learn/scikit-learn/branch/main/graph/badge.svg?token=Pk8G9gg3y9\n   :target: https://codecov.io/gh/scikit-learn/scikit-learn\n\n.. |Nightly wheels| image:: https://github.com/scikit-learn/scikit-learn/workflows/Wheel%20builder/badge.svg?event=schedule\n   :target: https://github.com/scikit-learn/scikit-learn/actions?query=workflow%3A%22Wheel+builder%22+event%3Aschedule\n\n.. |Ruff| image:: https://img.shields.io/badge/code%20style-ruff-000000.svg\n   :target: https://github.com/astral-sh/ruff\n\n.. |PythonVersion| image:: https://img.shields.io/pypi/pyversions/scikit-learn.svg\n   :target: https://pypi.org/project/scikit-learn/\n\n.. |PyPi| image:: https://img.shields.io/pypi/v/scikit-learn\n   :target: https://pypi.org/project/scikit-learn\n\n.. |DOI| image:: https://zenodo.org/badge/21369/scikit-learn/scikit-learn.svg\n   :target: https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn\n\n.. |Benchmark| image:: https://img.shields.io/badge/Benchmarked%20by-asv-blue\n   :target: https://scikit-learn.org/scikit-learn-benchmarks\n\n.. |PythonMinVersion| replace:: 3.10\n.. |NumPyMinVersion| replace:: 1.22.0\n.. |SciPyMinVersion| replace:: 1.8.0\n.. |JoblibMinVersion| replace:: 1.2.0\n.. |ThreadpoolctlMinVersion| replace:: 3.1.0\n.. |MatplotlibMinVersion| replace:: 3.5.0\n.. |Scikit-ImageMinVersion| replace:: 0.19.0\n.. |PandasMinVersion| replace:: 1.4.0\n.. |SeabornMinVersion| replace:: 0.9.0\n.. |PytestMinVersion| replace:: 7.1.2\n.. |PlotlyMinVersion| replace:: 5.14.0\n\n.. image:: https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/doc/logos/scikit-learn-logo.png\n  :target: https://scikit-learn.org/\n\n**scikit-learn** is a Python module for machine learning built on top of\nSciPy and is distributed under the 3-Clause BSD license.\n\nThe project was started in 2007 by David Cournapeau as a Google Summer\nof Code project, and since then many volunteers have contributed. See\nthe `About us <https://scikit-learn.org/dev/about.html#authors>`__ page\nfor a list of core contributors.\n\nIt is currently maintained by a team of volunteers.\n\nWebsite: https://scikit-learn.org\n\nInstallation\n------------\n\nDependencies\n~~~~~~~~~~~~\n\nscikit-learn requires:\n\n- Python (>= |PythonMinVersion|)\n- NumPy (>= |NumPyMinVersion|)\n- SciPy (>= |SciPyMinVersion|)\n- joblib (>= |JoblibMinVersion|)\n- threadpoolctl (>= |ThreadpoolctlMinVersion|)\n\n=======\n\nScikit-learn plotting capabilities (i.e., functions start with ``plot_`` and\nclasses end with ``Display``) require Matplotlib (>= |MatplotlibMinVersion|).\nFor running the examples Matplotlib >= |MatplotlibMinVersion| is required.\nA few examples require scikit-image >= |Scikit-ImageMinVersion|, a few examples\nrequire pandas >= |PandasMinVersion|, some examples require seaborn >=\n|SeabornMinVersion| and plotly >= |PlotlyMinVersion|.\n\nUser installation\n~~~~~~~~~~~~~~~~~\n\nIf you already have a working installation of NumPy and SciPy,\nthe easiest way to install scikit-learn is using ``pip``::\n\n    pip install -U scikit-learn\n\nor ``conda``::\n\n    conda install -c conda-forge scikit-learn\n\nThe documentation includes more detailed `installation instructions <https://scikit-learn.org/stable/install.html>`_.\n\n\nChangelog\n---------\n\nSee the `changelog <https://scikit-learn.org/dev/whats_new.html>`__\nfor a history of notable changes to scikit-learn.\n\nDevelopment\n-----------\n\nWe welcome new contributors of all experience levels. The scikit-learn\ncommunity goals are to be helpful, welcoming, and effective. The\n`Development Guide <https://scikit-learn.org/stable/developers/index.html>`_\nhas detailed information about contributing code, documentation, tests, and\nmore. We've included some basic information in this README.\n\nImportant links\n~~~~~~~~~~~~~~~\n\n- Official source code repo: https://github.com/scikit-learn/scikit-learn\n- Download releases: https://pypi.org/project/scikit-learn/\n- Issue tracker: https://github.com/scikit-learn/scikit-learn/issues\n\nSource code\n~~~~~~~~~~~\n\nYou can check the latest sources with the command::\n\n    git clone https://github.com/scikit-learn/scikit-learn.git\n\nContributing\n~~~~~~~~~~~~\n\nTo learn more about making a contribution to scikit-learn, please see our\n`Contributing guide\n<https://scikit-learn.org/dev/developers/contributing.html>`_.\n\nTesting\n~~~~~~~\n\nAfter installation, you can launch the test suite from outside the source\ndirectory (you will need to have ``pytest`` >= |PyTestMinVersion| installed)::\n\n    pytest sklearn\n\nSee th...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "pandas",
    "full_name": "pandas-dev/pandas",
    "description": "Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures similar to R data.frame objects, statistical functions, and much more",
    "url": "https://github.com/pandas-dev/pandas",
    "stars": 45241,
    "forks": 18446,
    "created_at": "2010-08-24",
    "updated_at": "2025-04-26",
    "topics": [
      "data-analysis",
      "pandas",
      "flexible",
      "alignment",
      "python",
      "data-science"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 20925817,
      "Cython": 1388723,
      "HTML": 457849,
      "C": 354589,
      "Shell": 22853,
      "Meson": 11215,
      "Smarty": 8852,
      "CSS": 6804,
      "Dockerfile": 5876,
      "XSLT": 1196
    },
    "readme_content": "<picture align=\"center\">\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://pandas.pydata.org/static/img/pandas_white.svg\">\n  <img alt=\"Pandas Logo\" src=\"https://pandas.pydata.org/static/img/pandas.svg\">\n</picture>\n\n-----------------\n\n# pandas: powerful Python data analysis toolkit\n\n| | |\n| --- | --- |\n| Testing | [![CI - Test](https://github.com/pandas-dev/pandas/actions/workflows/unit-tests.yml/badge.svg)](https://github.com/pandas-dev/pandas/actions/workflows/unit-tests.yml) [![Coverage](https://codecov.io/github/pandas-dev/pandas/coverage.svg?branch=main)](https://codecov.io/gh/pandas-dev/pandas) |\n| Package | [![PyPI Latest Release](https://img.shields.io/pypi/v/pandas.svg)](https://pypi.org/project/pandas/) [![PyPI Downloads](https://img.shields.io/pypi/dm/pandas.svg?label=PyPI%20downloads)](https://pypi.org/project/pandas/) [![Conda Latest Release](https://anaconda.org/conda-forge/pandas/badges/version.svg)](https://anaconda.org/conda-forge/pandas) [![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/pandas.svg?label=Conda%20downloads)](https://anaconda.org/conda-forge/pandas) |\n| Meta | [![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3509134.svg)](https://doi.org/10.5281/zenodo.3509134) [![License - BSD 3-Clause](https://img.shields.io/pypi/l/pandas.svg)](https://github.com/pandas-dev/pandas/blob/main/LICENSE) [![Slack](https://img.shields.io/badge/join_Slack-information-brightgreen.svg?logo=slack)](https://pandas.pydata.org/docs/dev/development/community.html?highlight=slack#community-slack) |\n\n\n## What is it?\n\n**pandas** is a Python package that provides fast, flexible, and expressive data\nstructures designed to make working with \"relational\" or \"labeled\" data both\neasy and intuitive. It aims to be the fundamental high-level building block for\ndoing practical, **real world** data analysis in Python. Additionally, it has\nthe broader goal of becoming **the most powerful and flexible open source data\nanalysis / manipulation tool available in any language**. It is already well on\nits way towards this goal.\n\n## Table of Contents\n\n- [Main Features](#main-features)\n- [Where to get it](#where-to-get-it)\n- [Dependencies](#dependencies)\n- [Installation from sources](#installation-from-sources)\n- [License](#license)\n- [Documentation](#documentation)\n- [Background](#background)\n- [Getting Help](#getting-help)\n- [Discussion and Development](#discussion-and-development)\n- [Contributing to pandas](#contributing-to-pandas)\n\n## Main Features\nHere are just a few of the things that pandas does well:\n\n  - Easy handling of [**missing data**][missing-data] (represented as\n    `NaN`, `NA`, or `NaT`) in floating point as well as non-floating point data\n  - Size mutability: columns can be [**inserted and\n    deleted**][insertion-deletion] from DataFrame and higher dimensional\n    objects\n  - Automatic and explicit [**data alignment**][alignment]: objects can\n    be explicitly aligned to a set of labels, or the user can simply\n    ignore the labels and let `Series`, `DataFrame`, etc. automatically\n    align the data for you in computations\n  - Powerful, flexible [**group by**][groupby] functionality to perform\n    split-apply-combine operations on data sets, for both aggregating\n    and transforming data\n  - Make it [**easy to convert**][conversion] ragged,\n    differently-indexed data in other Python and NumPy data structures\n    into DataFrame objects\n  - Intelligent label-based [**slicing**][slicing], [**fancy\n    indexing**][fancy-indexing], and [**subsetting**][subsetting] of\n    large data sets\n  - Intuitive [**merging**][merging] and [**joining**][joining] data\n    sets\n  - Flexible [**reshaping**][reshape] and [**pivoting**][pivot-table] of\n    data sets\n  - [**Hierarchical**][mi] labeling of axes (possible to have multiple\n    labels per tick)\n  - Robust IO tools for loading data from [**flat files**][flat-files]\n    (CSV and delimited), [**Excel files**][excel], [**databases**][db],\n    and saving/loading data from the ultrafast [**HDF5 format**][hdfstore]\n  - [**Time series**][timeseries]-specific functionality: date range\n    generation and frequency conversion, moving window statistics,\n    date shifting and lagging\n\n\n   [missing-data]: https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html\n   [insertion-deletion]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#column-selection-addition-deletion\n   [alignment]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html?highlight=alignment#intro-to-data-structures\n   [groupby]: https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#group-by-split-apply-combine\n   [conversion]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#dataframe\n   [slicing]: https://pandas.pydata.org/pandas-docs/stable/user_guide/i...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "metabase",
    "full_name": "metabase/metabase",
    "description": "The easy-to-use open source Business Intelligence and Embedded Analytics tool that lets everyone work with data :bar_chart:",
    "url": "https://github.com/metabase/metabase",
    "stars": 41743,
    "forks": 5517,
    "created_at": "2015-02-02",
    "updated_at": "2025-04-26",
    "topics": [
      "analytics",
      "businessintelligence",
      "dashboard",
      "reporting",
      "slack",
      "clojure",
      "database",
      "metabase",
      "postgres",
      "postgresql",
      "mysql",
      "bi",
      "visualization",
      "data",
      "data-analysis",
      "sql-editor",
      "data-visualization",
      "business-intelligence"
    ],
    "primary_language": "Clojure",
    "languages": {
      "Clojure": 22305883,
      "TypeScript": 15312394,
      "JavaScript": 6358144,
      "CSS": 232015,
      "MDX": 52846,
      "Shell": 39274,
      "Handlebars": 29885,
      "HTML": 5476,
      "Emacs Lisp": 5176,
      "Dockerfile": 5007,
      "Mustache": 2982
    },
    "readme_content": "# Metabase\n\n[Metabase](https://www.metabase.com) is the easy, open-source way for everyone in your company to ask questions and learn from data.\n\n![Metabase Product Screenshot](docs/images/metabase-product-screenshot.png)\n\n[![Latest Release](https://img.shields.io/github/release/metabase/metabase.svg?label=latest%20release)](https://github.com/metabase/metabase/releases)\n[![codecov](https://codecov.io/gh/metabase/metabase/branch/master/graph/badge.svg)](https://codecov.io/gh/metabase/metabase)\n![Docker Pulls](https://img.shields.io/docker/pulls/metabase/metabase)\n\n## Get started\n\nThe easiest way to get started with Metabase is to sign up for a free trial of [Metabase Cloud](https://store.metabase.com/checkout). You get support, backups, upgrades, an SMTP server, SSL certificate, SoC2 Type 2 security auditing, and more (plus your money goes toward improving Metabase). Check out our quick overview of [cloud vs self-hosting](https://www.metabase.com/docs/latest/cloud/cloud-vs-self-hosting). If you need to, you can always switch to [self-hosting](https://www.metabase.com/docs/latest/installation-and-operation/installing-metabase) Metabase at any time (or vice versa).\n\n## Features\n\n- [Set up in five minutes](https://www.metabase.com/docs/latest/setting-up-metabase.html) (we're not kidding).\n- Let anyone on your team [ask questions](https://www.metabase.com/docs/latest/questions/introduction) without knowing SQL.\n- Use the [SQL editor](https://www.metabase.com/docs/latest/questions/native-editor/writing-sql) for more complex queries.\n- Build handsome, interactive [dashboards](https://www.metabase.com/docs/latest/users-guide/07-dashboards.html) with filters, auto-refresh, fullscreen, and custom click behavior.\n- Create [models](https://www.metabase.com/learn/metabase-basics/getting-started/models) that clean up, annotate, and/or combine raw tables.\n- Define canonical [segments and metrics](https://www.metabase.com/docs/latest/administration-guide/07-segments-and-metrics.html) for your team to use.\n- Send data to Slack or email on a schedule with [dashboard subscriptions](https://www.metabase.com/docs/latest/users-guide/dashboard-subscriptions).\n- Set up [alerts](https://www.metabase.com/docs/latest/users-guide/15-alerts.html) to have Metabase notify you when your data changes.\n- [Embed charts and dashboards](https://www.metabase.com/docs/latest/administration-guide/13-embedding.html) in your app, or even [your entire Metabase](https://www.metabase.com/docs/latest/enterprise-guide/full-app-embedding.html).\n\nTake a [tour of Metabase](https://www.metabase.com/learn/getting-started/tour-of-metabase).\n\n## Supported databases\n\n- [Officially supported databases](./docs/databases/connecting.md#connecting-to-supported-databases)\n- [Community drivers](./docs/developers-guide/partner-and-community-drivers.md)\n\n## Installation\n\nMetabase can be run just about anywhere. Check out our [Installation Guides](https://www.metabase.com/docs/latest/operations-guide/installing-metabase).\n\n## Contributing\n\n## Quick Setup: Dev environment\n\nIn order to spin up a development environment, you need to start the front end and the backend as follows:\n\n### Frontend quick setup\n\nThe following command will install the Javascript dependencies:\n\n```\n$ yarn install\n```\n\nTo build and run without watching changes:\n\n```\n$ yarn build\n```\n\nTo build and run with hot-reload:\n\n```\n$ yarn build-hot\n```\n\n### Backend  quick setup\n\nIn order to run the backend, you'll need to build the drivers first, and then start the backend:\n\n```\n$ ./bin/build-drivers.sh\n$ clojure -M:run\n```\n\nFor a more detailed setup of a dev environment for Metabase, check out our [Developers Guide](./docs/developers-guide/start.md).\n\n## Internationalization\n\nWe want Metabase to be available in as many languages as possible. See which translations are available and help contribute to internationalization using our project over at [Crowdin](https://crowdin.com/project/metabase-i18n). You can also check out our [policies on translations](https://www.metabase.com/docs/latest/administration-guide/localization.html).\n\n## Extending Metabase\n\nHit our Query API from Javascript to integrate analytics. Metabase enables your application to:\n\n- Build moderation interfaces.\n- Export subsets of your users to third party marketing automation software.\n- Provide a custom customer lookup application for the people in your company.\n\nCheck out our guide, [Working with the Metabase API](https://www.metabase.com/learn/administration/metabase-api).\n\n## Security Disclosure\n\nSee [SECURITY.md](./SECURITY.md) for details.\n\n## License\n\nThis repository contains the source code for both the Open Source edition of Metabase, released under the AGPL, as well as the [commercial editions of Metabase](https://www.metabase.com/pricing/), which are released under the Metabase Commercial Software License.\n\nSee [LICENSE.txt](./LICENSE.txt) for details.\n\nUnless otherwise noted, all files © 2025 Metabase, Inc.\n\n## [Metabase Expert...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "streamlit",
    "full_name": "streamlit/streamlit",
    "description": "Streamlit — A faster way to build and share data apps.",
    "url": "https://github.com/streamlit/streamlit",
    "stars": 39034,
    "forks": 3412,
    "created_at": "2019-08-24",
    "updated_at": "2025-04-26",
    "topics": [
      "python",
      "machine-learning",
      "data-science",
      "deep-learning",
      "data-visualization",
      "streamlit",
      "data-analysis",
      "developer-tools"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 5825838,
      "TypeScript": 4016135,
      "JavaScript": 66360,
      "HTML": 17078,
      "Makefile": 16067,
      "SCSS": 6463,
      "Shell": 3688,
      "Dockerfile": 1121,
      "Batchfile": 676,
      "CSS": 146
    },
    "readme_content": "<br>\n\n<img src=\"https://user-images.githubusercontent.com/7164864/217935870-c0bc60a3-6fc0-4047-b011-7b4c59488c91.png\" alt=\"Streamlit logo\" style=\"margin-top:50px\"></img>\n\n# Welcome to Streamlit 👋\n\n**A faster way to build and share data apps.**\n\n## What is Streamlit?\n\nStreamlit lets you transform Python scripts into interactive web apps in minutes, instead of weeks. Build dashboards, generate reports, or create chat apps. Once you’ve created an app, you can use our [Community Cloud platform](https://streamlit.io/cloud) to deploy, manage, and share your app.\n\n### Why choose Streamlit?\n\n- **Simple and Pythonic:** Write beautiful, easy-to-read code.\n- **Fast, interactive prototyping:** Let others interact with your data and provide feedback quickly.\n- **Live editing:** See your app update instantly as you edit your script.\n- **Open-source and free:** Join a vibrant community and contribute to Streamlit's future.\n\n## Installation\n\nOpen a terminal and run:\n\n```bash\n$ pip install streamlit\n$ streamlit hello\n```\n\nIf this opens our sweet _Streamlit Hello_ app in your browser, you're all set! If not, head over to [our docs](https://docs.streamlit.io/get-started) for specific installs.\n\nThe app features a bunch of examples of what you can do with Streamlit. Jump to the [quickstart](#quickstart) section to understand how that all works.\n\n<img src=\"https://user-images.githubusercontent.com/7164864/217936487-1017784e-68ec-4e0d-a7f6-6b97525ddf88.gif\" alt=\"Streamlit Hello\" width=500 href=\"none\"></img>\n\n## Quickstart\n\n### A little example\n\nCreate a new file named `streamlit_app.py` in your project directory with the following code:\n```python\nimport streamlit as st\nx = st.slider(\"Select a value\")\nst.write(x, \"squared is\", x * x)\n```\n\nNow run it to open the app!\n```\n$ streamlit run streamlit_app.py\n```\n\n<img src=\"https://user-images.githubusercontent.com/7164864/215172915-cf087c56-e7ae-449a-83a4-b5fa0328d954.gif\" width=300 alt=\"Little example\"></img>\n\n### Give me more!\n\nStreamlit comes in with [a ton of additional powerful elements](https://docs.streamlit.io/develop/api-reference) to spice up your data apps and delight your viewers. Some examples:\n\n<table border=\"0\">\n  <tr>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.streamlit.io/develop/api-reference/widgets\">\n        <img src=\"https://user-images.githubusercontent.com/7164864/217936099-12c16f8c-7fe4-44b1-889a-1ac9ee6a1b44.png\" style=\"max-height:150px; width:auto; display:block;\">\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.streamlit.io/develop/api-reference/data/st.dataframe\">\n        <img src=\"https://user-images.githubusercontent.com/7164864/215110064-5eb4e294-8f30-4933-9563-0275230e52b5.gif\" style=\"max-height:150px; width:auto; display:block;\">\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.streamlit.io/develop/api-reference/charts\">\n        <img src=\"https://user-images.githubusercontent.com/7164864/215174472-bca8a0d7-cf4b-4268-9c3b-8c03dad50bcd.gif\" style=\"max-height:150px; width:auto; display:block;\">\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.streamlit.io/develop/api-reference/layout\">\n        <img src=\"https://user-images.githubusercontent.com/7164864/217936149-a35c35be-0d96-4c63-8c6a-1c4b52aa8f60.png\" style=\"max-height:150px; width:auto; display:block;\">\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.streamlit.io/develop/concepts/multipage-apps\">\n        <img src=\"https://user-images.githubusercontent.com/7164864/215173883-eae0de69-7c1d-4d78-97d0-3bc1ab865e5b.gif\" style=\"max-height:150px; width:auto; display:block;\">\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://streamlit.io/gallery\">\n        <img src=\"https://user-images.githubusercontent.com/7164864/215109229-6ae9111f-e5c1-4f0b-b3a2-87a79268ccc9.gif\" style=\"max-height:150px; width:auto; display:block;\">\n      </a>\n    </td>\n  </tr>\n  <tr>\n    <td>Input widgets</td>\n    <td>Dataframes</td>\n    <td>Charts</td>\n    <td>Layout</td>\n    <td>Multi-page apps</td>\n    <td>Fun</td>\n  </tr>\n</table>\n\n\nOur vibrant creators community also extends Streamlit capabilities using  🧩 [Streamlit Components](https://streamlit.io/components).\n\n## Get inspired\n\nThere's so much you can build with Streamlit:\n- 🤖  [LLMs & chatbot apps](https://streamlit.io/gallery?category=llms)\n- 🧬  [Science & technology apps](https://streamlit.io/gallery?category=science-technology)\n- 💬  [NLP & language apps](https://streamlit.io/gallery?category=nlp-language)\n- 🏦  [Finance & business apps](https://streamlit.io/gallery?category=finance-business)\n- 🗺  [Geography & society apps](https://streamlit.io/gallery?category=geography-society)\n- and more!\n\n**Check out [our gallery!](https://streamlit.io/gallery)** 🎈\n\n## Community Cloud\n\nDeploy, manage and share your apps for free using our [Community Cloud](https://streamlit.io/cloud)! Sign-up [here](https://share.streamlit.io/signup). <br><br>\n<img src=\"https://user-i...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "gradio",
    "full_name": "gradio-app/gradio",
    "description": "Build and share delightful machine learning apps, all in Python. 🌟 Star to support our work!",
    "url": "https://github.com/gradio-app/gradio",
    "stars": 37689,
    "forks": 2867,
    "created_at": "2018-12-19",
    "updated_at": "2025-04-26",
    "topics": [
      "machine-learning",
      "models",
      "ui",
      "ui-components",
      "interface",
      "python",
      "data-science",
      "data-visualization",
      "deep-learning",
      "data-analysis",
      "gradio",
      "gradio-interface",
      "python-notebook",
      "deploy",
      "hacktoberfest"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 3271269,
      "Svelte": 1313700,
      "TypeScript": 1250478,
      "mdsvex": 221972,
      "CSS": 72732,
      "JavaScript": 62692,
      "Jupyter Notebook": 32113,
      "HTML": 24506,
      "Batchfile": 6427,
      "Shell": 6015,
      "MDX": 1670,
      "Dockerfile": 512
    },
    "readme_content": "<!-- DO NOT EDIT THIS FILE DIRECTLY. INSTEAD EDIT THE `readme_template.md` OR `guides/01_getting-started/01_quickstart.md` TEMPLATES AND THEN RUN `render_readme.py` SCRIPT. -->\n\n<div align=\"center\">\n<a href=\"https://gradio.app\">\n<img src=\"readme_files/gradio.svg\" alt=\"gradio\" width=350>\n</a>\n</div>\n\n<div align=\"center\">\n<span>\n<a href=\"https://www.producthunt.com/posts/gradio-5-0?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-gradio&#0045;5&#0045;0\" target=\"_blank\"><img src=\"https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=501906&theme=light\" alt=\"Gradio&#0032;5&#0046;0 - the&#0032;easiest&#0032;way&#0032;to&#0032;build&#0032;AI&#0032;web&#0032;apps | Product Hunt\" style=\"width: 150px; height: 54px;\" width=\"150\" height=\"54\" /></a>\n<a href=\"https://trendshift.io/repositories/2145\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/2145\" alt=\"gradio-app%2Fgradio | Trendshift\" style=\"width: 150px; height: 55px;\" width=\"150\" height=\"55\"/></a>\n</span>\n\n[![gradio-backend](https://github.com/gradio-app/gradio/actions/workflows/test-python.yml/badge.svg)](https://github.com/gradio-app/gradio/actions/workflows/test-python.yml)\n[![gradio-ui](https://github.com/gradio-app/gradio/actions/workflows/tests-js.yml/badge.svg)](https://github.com/gradio-app/gradio/actions/workflows/tests-js.yml) \n[![PyPI](https://img.shields.io/pypi/v/gradio)](https://pypi.org/project/gradio/)\n[![PyPI downloads](https://img.shields.io/pypi/dm/gradio)](https://pypi.org/project/gradio/)\n![Python version](https://img.shields.io/badge/python-3.10+-important)\n[![Twitter follow](https://img.shields.io/twitter/follow/gradio?style=social&label=follow)](https://twitter.com/gradio)\n\n[Website](https://gradio.app)\n| [Documentation](https://gradio.app/docs/)\n| [Guides](https://gradio.app/guides/)\n| [Getting Started](https://gradio.app/getting_started/)\n| [Examples](demo/)\n\n</div>\n\n<div align=\"center\">\n\nEnglish | [中文](readme_files/zh-cn#readme)\n\n</div>\n\n# Gradio: Build Machine Learning Web Apps — in Python\n\n\n\nGradio is an open-source Python package that allows you to quickly **build** a demo or web application for your machine learning model, API, or any arbitrary Python function. You can then **share** a link to your demo or web application in just a few seconds using Gradio's built-in sharing features. *No JavaScript, CSS, or web hosting experience needed!*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/gif-version.gif\" style=\"padding-bottom: 10px\">\n\nIt just takes a few lines of Python to create your own demo, so let's get started 💫\n\n\n### Installation\n\n**Prerequisite**: Gradio requires [Python 3.10 or higher](https://www.python.org/downloads/).\n\n\nWe recommend installing Gradio using `pip`, which is included by default in Python. Run this in your terminal or command prompt:\n\n```bash\npip install --upgrade gradio\n```\n\n\n> [!TIP]\n > It is best to install Gradio in a virtual environment. Detailed installation instructions for all common operating systems <a href=\"https://www.gradio.app/main/guides/installing-gradio-in-a-virtual-environment\">are provided here</a>. \n\n### Building Your First Demo\n\nYou can run Gradio in your favorite code editor, Jupyter notebook, Google Colab, or anywhere else you write Python. Let's write your first Gradio app:\n\n\n```python\nimport gradio as gr\n\ndef greet(name, intensity):\n    return \"Hello, \" + name + \"!\" * int(intensity)\n\ndemo = gr.Interface(\n    fn=greet,\n    inputs=[\"text\", \"slider\"],\n    outputs=[\"text\"],\n)\n\ndemo.launch()\n```\n\n\n\n> [!TIP]\n > We shorten the imported name from <code>gradio</code> to <code>gr</code>. This is a widely adopted convention for better readability of code. \n\nNow, run your code. If you've written the Python code in a file named `app.py`, then you would run `python app.py` from the terminal.\n\nThe demo below will open in a browser on [http://localhost:7860](http://localhost:7860) if running from a file. If you are running within a notebook, the demo will appear embedded within the notebook.\n\n![`hello_world_4` demo](demo/hello_world_4/screenshot.gif)\n\nType your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.\n\n> [!TIP]\n > When developing locally, you can run your Gradio app in <strong>hot reload mode</strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio</code> before the name of the file instead of <code>python</code>. In the example above, you would type: `gradio app.py` in your terminal. Learn more in the <a href=\"https://www.gradio.app/guides/developing-faster-with-reload-mode\">Hot Reloading Guide</a>.\n\n\n**Understanding the `Interface` Class**\n\nYou'll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning model...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "CyberChef",
    "full_name": "gchq/CyberChef",
    "description": "The Cyber Swiss Army Knife - a web app for encryption, encoding, compression and data analysis",
    "url": "https://github.com/gchq/CyberChef",
    "stars": 31022,
    "forks": 3480,
    "created_at": "2016-11-28",
    "updated_at": "2025-04-26",
    "topics": [
      "data-analysis",
      "data-manipulation",
      "encryption",
      "encoding",
      "compression",
      "parsing",
      "hashing"
    ],
    "primary_language": "JavaScript",
    "languages": {
      "JavaScript": 4819192,
      "HTML": 73208,
      "CSS": 65991,
      "Dockerfile": 1389
    },
    "readme_content": "# CyberChef\n\n[![](https://github.com/gchq/CyberChef/workflows/Master%20Build,%20Test%20&%20Deploy/badge.svg)](https://github.com/gchq/CyberChef/actions?query=workflow%3A%22Master+Build%2C+Test+%26+Deploy%22)\n[![npm](https://img.shields.io/npm/v/cyberchef.svg)](https://www.npmjs.com/package/cyberchef)\n[![](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/gchq/CyberChef/blob/master/LICENSE)\n[![Gitter](https://badges.gitter.im/gchq/CyberChef.svg)](https://gitter.im/gchq/CyberChef?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n\n\n#### *The Cyber Swiss Army Knife*\n\nCyberChef is a simple, intuitive web app for carrying out all manner of \"cyber\" operations within a web browser. These operations include simple encoding like XOR and Base64, more complex encryption like AES, DES and Blowfish, creating binary and hexdumps, compression and decompression of data, calculating hashes and checksums, IPv6 and X.509 parsing, changing character encodings, and much more.\n\nThe tool is designed to enable both technical and non-technical analysts to manipulate data in complex ways without having to deal with complex tools or algorithms. It was conceived, designed, built and incrementally improved by an analyst in their 10% innovation time over several years.\n\n## Live demo\n\nCyberChef is still under active development. As a result, it shouldn't be considered a finished product. There is still testing and bug fixing to do, new features to be added and additional documentation to write. Please contribute!\n\nCryptographic operations in CyberChef should not be relied upon to provide security in any situation. No guarantee is offered for their correctness.\n\n[A live demo can be found here][1] - have fun!\n\n## Containers\n\nIf you would like to try out CyberChef locally you can either build it yourself:\n\n```bash\ndocker build --tag cyberchef --ulimit nofile=10000 .\ndocker run -it -p 8080:80 cyberchef\n```\n\nOr you can use our image directly:\n\n```bash\ndocker run -it -p 8080:80 ghcr.io/gchq/cyberchef:latest\n```\n\nThis image is built and published through our [GitHub Workflows](.github/workflows/releases.yml)\n\n## How it works\n\nThere are four main areas in CyberChef:\n\n 1. The **input** box in the top right, where you can paste, type or drag the text or file you want to operate on.\n 2. The **output** box in the bottom right, where the outcome of your processing will be displayed.\n 3. The **operations** list on the far left, where you can find all the operations that CyberChef is capable of in categorised lists, or by searching.\n 4. The **recipe** area in the middle, where you can drag the operations that you want to use and specify arguments and options.\n\nYou can use as many operations as you like in simple or complex ways. Some examples are as follows:\n\n - [Decode a Base64-encoded string][2]\n - [Convert a date and time to a different time zone][3]\n - [Parse a Teredo IPv6 address][4]\n - [Convert data from a hexdump, then decompress][5]\n - [Decrypt and disassemble shellcode][6]\n - [Display multiple timestamps as full dates][7]\n - [Carry out different operations on data of different types][8]\n - [Use parts of the input as arguments to operations][9]\n - [Perform AES decryption, extracting the IV from the beginning of the cipher stream][10]\n - [Automagically detect several layers of nested encoding][12]\n\n\n## Features\n\n - Drag and drop\n     - Operations can be dragged in and out of the recipe list, or reorganised.\n     - Files up to 2GB can be dragged over the input box to load them directly into the browser.\n - Auto Bake\n     - Whenever you modify the input or the recipe, CyberChef will automatically \"bake\" for you and produce the output immediately.\n     - This can be turned off and operated manually if it is affecting performance (if the input is very large, for instance).\n - Automated encoding detection\n     - CyberChef uses [a number of techniques](https://github.com/gchq/CyberChef/wiki/Automatic-detection-of-encoded-data-using-CyberChef-Magic) to attempt to automatically detect which encodings your data is under. If it finds a suitable operation that make sense of your data, it displays the 'magic' icon in the Output field which you can click to decode your data.\n - Breakpoints\n     - You can set breakpoints on any operation in your recipe to pause execution before running it.\n     - You can also step through the recipe one operation at a time to see what the data looks like at each stage.\n - Save and load recipes\n     - If you come up with an awesome recipe that you know you’ll want to use again, just click \"Save recipe\" and add it to your local storage. It'll be waiting for you next time you visit CyberChef.\n     - You can also copy the URL, which includes your recipe and input, to easily share it with others.\n - Search\n     - If you know the name of the operation you want or a word associated with it, start typing it into the search field and any matching operations will immediately be shown.\n - Highlighting...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "AI-Expert-Roadmap",
    "full_name": "AMAI-GmbH/AI-Expert-Roadmap",
    "description": "Roadmap to becoming an Artificial Intelligence Expert in 2022",
    "url": "https://github.com/AMAI-GmbH/AI-Expert-Roadmap",
    "stars": 29787,
    "forks": 2520,
    "created_at": "2020-10-24",
    "updated_at": "2025-04-26",
    "topics": [
      "deep-learning",
      "artificial-intelligence",
      "roadmap",
      "ai-roadmap",
      "machine-learning",
      "study-plan",
      "data-science",
      "data-analysis",
      "neural-network",
      "ai"
    ],
    "primary_language": "JavaScript",
    "languages": {
      "JavaScript": 5569,
      "Vue": 1641,
      "Stylus": 1556
    },
    "readme_content": "<p align=\"center\">\n  <a href=\"https://github.com/AMAI-GmbH/AI-Expert-Roadmap\">\n    <img src=\"https://uploads-ssl.webflow.com/58e6a2b25c28230d367487ad/5c32232ecb585fcc5c4645e1_icon_machine-learning.svg\" alt=\"Developer Roadmap\" width=\"96\" height=\"96\">\n  </a>\n  <h2 align=\"center\">i.am.ai<br>AI Expert Roadmap</h2>\n  <p align=\"center\">Roadmap to becoming an Artificial Intelligence Expert in 2022</p>\n  <p align=\"center\">\n      <a href=\"https://twitter.com/home?status=https://i.am.ai/roadmap Roadmap to becoming an Artificial Intelligence Expert in 2022\" target=\"_blank\"><img src=\"https://img.shields.io/badge/tweet-blue.svg?logo=twitter&logoColor=white\" style=\"display: inherit;\"/></a>\n      <a href=\"https://www.linkedin.com/shareArticle?mini=true&url=https://i.am.ai/roadmap&title=&summary=Roadmap to becoming an Artificial Intelligence Expert in 2022&source=\" target=\"_blank\"><img src=\"https://img.shields.io/badge/post-blue.svg?logo=linkedin&logoColor=white\" style=\"display: inherit;\"/></a>\n      <a href=\"https://github.com/AMAI-GmbH/AI-Expert-Roadmap\"><img src=\"https://img.shields.io/badge/Roadmap-2022-yellowgreen.svg\" style=\"display: inherit;\"/></a>\n      <a href=\"https://am.ai?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Badge\" target=\"_blank\"><img alt=\"AMAI GmbH\" src=\"https://img.shields.io/badge/Author-AMAI GmbH-blue.svg\" style=\"display: inherit;\"/></a>\n<a href=\"https://opensource.org/licenses/MIT/\" target=\"_blank\"><img alt=\"MIT License\" src=\"https://img.shields.io/badge/License-MIT-blue.svg\" style=\"display: inherit;\"/></a>\n  </p>\n  <br>\n</p>\n\nBelow you find a set of charts demonstrating the paths that you can take and the technologies that you would want to adopt in order to become a data scientist, machine learning or an AI expert. We made these charts for our new employees to make them AI Experts but we wanted to share them here to help the community.\n\nIf you are interested to become an AI EXPERT at [AMAI](https://www.linkedin.com/company/amai-gmbh/?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Become+Expert) in Germany, or you want to [hire an AI Expert](https://am.ai?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Hire+Expert), please say [hi@am.ai](mailto:hi@am.ai).\n\n## Note\n\n👉 An **interactive version with links to follow** about each bullet of the list can be found at [i.am.ai/roadmap](https://i.am.ai/roadmap?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Interactive) 👈\n\nTo receive updates [star :star:](https://github.com/AMAI-GmbH/AI-Expert-Roadmap/stargazers) and watch :eyes: the [GitHub Repo](https://github.com/AMAI-GmbH/AI-Expert-Roadmap/) to get notified, when we add new content to stay on the top of the most recent research.\n\nFollow our [AI Newsletter](https://i.am.ai/newsletter?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Newsletter) to stay up to date with the latest developments in AI. We cover new use cases and research topics.\n\n## Disclaimer\n\nThe purpose of these roadmaps is to give you an idea about the landscape and to guide you if you are confused about what to learn next and not to encourage you to pick what is hip and trendy. You should grow some understanding of why one tool would be better suited for some cases than the other and remember hip and trendy never means best suited for the job.\n\n## Introduction\n\n<p align=\"center\">\n  <a href=\"https://i.am.ai/roadmap#introduction?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Introduction\" target=\"_blank\">\n      <img src=\"./images/intro.svg\"/>\n  </a>\n</p>\n\n## Fundamentals\n\n<p align=\"center\">\n  <a href=\"https://i.am.ai/roadmap#fundamentals?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Fundamentals\" target=\"_blank\">\n      <img src=\"./images/fundamentals.svg\"/>\n  </a>\n</p>\n\n## Data Science Roadmap\n\n<p align=\"center\">\n  <a href=\"https://i.am.ai/roadmap#data-science-roadmap?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+DataScience\" target=\"_blank\">\n      <img src=\"./images/datascience.svg\"/>\n  </a>\n</p>\n\n## Machine Learning Roadmap\n\n<p align=\"center\">\n  <a href=\"https://i.am.ai/roadmap#machine-learning-roadmap?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+MachineLearning\" target=\"_blank\">\n      <img src=\"./images/machine_learning.svg\"/>\n  </a>\n</p>\n\n## Deep Learning Roadmap\n\n<p align=\"center\">\n  <a href=\"https://i.am.ai/roadmap#deep-learning-roadmap?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+DeepLearning\" target=\"_blank\">\n      <img src=\"./images/deep_learning.svg\"/>\n  </a>\n</p>\n\n## Data Engineer Roadmap\n\n<p align=\"center\">\n  <a href=\"https://i.am.ai/roadmap#data-engineer-roadmap?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+DataEngineer\" target=\"_blank\">\n      <img src=\"./images/data_engineer.svg\"/>\n  </a>\n</p>\n\n## Big Data Engineer Roadmap\n\n<p align=\"center\">\n  <a href=\"https://i.am.ai/roadmap#big-d...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "Data-Science-For-Beginners",
    "full_name": "microsoft/Data-Science-For-Beginners",
    "description": "10 Weeks, 20 Lessons, Data Science for All!",
    "url": "https://github.com/microsoft/Data-Science-For-Beginners",
    "stars": 29324,
    "forks": 6121,
    "created_at": "2021-03-03",
    "updated_at": "2025-04-26",
    "topics": [
      "data-science",
      "python",
      "data-visualization",
      "data-analysis",
      "pandas",
      "microsoft-for-beginners"
    ],
    "primary_language": "Jupyter Notebook",
    "languages": {
      "Jupyter Notebook": 10093190,
      "Vue": 11821,
      "HTML": 3334,
      "JavaScript": 3009
    },
    "readme_content": "# Data Science for Beginners - A Curriculum\n\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://github.com/codespaces/new?hide_repo_select=true&ref=main&repo=344191198)\n\n[![GitHub license](https://img.shields.io/github/license/microsoft/Data-Science-For-Beginners.svg)](https://github.com/microsoft/Data-Science-For-Beginners/blob/master/LICENSE)\n[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/Data-Science-For-Beginners.svg)](https://GitHub.com/microsoft/Data-Science-For-Beginners/graphs/contributors/)\n[![GitHub issues](https://img.shields.io/github/issues/microsoft/Data-Science-For-Beginners.svg)](https://GitHub.com/microsoft/Data-Science-For-Beginners/issues/)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/Data-Science-For-Beginners.svg)](https://GitHub.com/microsoft/Data-Science-For-Beginners/pulls/)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)\n\n[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/Data-Science-For-Beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/Data-Science-For-Beginners/watchers/)\n[![GitHub forks](https://img.shields.io/github/forks/microsoft/Data-Science-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/Data-Science-For-Beginners/network/)\n[![GitHub stars](https://img.shields.io/github/stars/microsoft/Data-Science-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/Data-Science-For-Beginners/stargazers/)\n\n[![](https://dcbadge.vercel.app/api/server/ByRwuEEgH4)](https://discord.gg/zxKYvhSnVp?WT.mc_id=academic-000002-leestott)\n\nAzure Cloud Advocates at Microsoft are pleased to offer a 10-week, 20-lesson curriculum all about Data Science. Each lesson includes pre-lesson and post-lesson quizzes, written instructions to complete the lesson, a solution, and an assignment. Our project-based pedagogy allows you to learn while building, a proven way for new skills to 'stick'.\n\n**Hearty thanks to our authors:** [Jasmine Greenaway](https://www.twitter.com/paladique), [Dmitry Soshnikov](http://soshnikov.com), [Nitya Narasimhan](https://twitter.com/nitya), [Jalen McGee](https://twitter.com/JalenMcG), [Jen Looper](https://twitter.com/jenlooper), [Maud Levy](https://twitter.com/maudstweets), [Tiffany Souterre](https://twitter.com/TiffanySouterre), [Christopher Harrison](https://www.twitter.com/geektrainer).\n\n**🙏 Special thanks 🙏 to our [Microsoft Student Ambassador](https://studentambassadors.microsoft.com/) authors, reviewers and content contributors,** notably Aaryan Arora, [Aditya Garg](https://github.com/AdityaGarg00), [Alondra Sanchez](https://www.linkedin.com/in/alondra-sanchez-molina/), [Ankita Singh](https://www.linkedin.com/in/ankitasingh007), [Anupam Mishra](https://www.linkedin.com/in/anupam--mishra/), [Arpita Das](https://www.linkedin.com/in/arpitadas01/), ChhailBihari Dubey, [Dibri Nsofor](https://www.linkedin.com/in/dibrinsofor), [Dishita Bhasin](https://www.linkedin.com/in/dishita-bhasin-7065281bb), [Majd Safi](https://www.linkedin.com/in/majd-s/), [Max Blum](https://www.linkedin.com/in/max-blum-6036a1186/), [Miguel Correa](https://www.linkedin.com/in/miguelmque/), [Mohamma Iftekher (Iftu) Ebne Jalal](https://twitter.com/iftu119), [Nawrin Tabassum](https://www.linkedin.com/in/nawrin-tabassum), [Raymond Wangsa Putra](https://www.linkedin.com/in/raymond-wp/), [Rohit Yadav](https://www.linkedin.com/in/rty2423), Samridhi Sharma, [Sanya Sinha](https://www.linkedin.com/mwlite/in/sanya-sinha-13aab1200),\n[Sheena Narula](https://www.linkedin.com/in/sheena-narua-n/), [Tauqeer Ahmad](https://www.linkedin.com/in/tauqeerahmad5201/), Yogendrasingh Pawar , [Vidushi Gupta](https://www.linkedin.com/in/vidushi-gupta07/), [Jasleen Sondhi](https://www.linkedin.com/in/jasleen-sondhi/)\n\n|![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev) ](./sketchnotes/00-Title.png)|\n|:---:|\n| Data Science For Beginners - _Sketchnote by [@nitya](https://twitter.com/nitya)_ |\n\n## Announcement - New Curriculum on Generative AI was just released!\n\nWe just released a 12 lesson curriculum on generative AI. Come learn things like:\n\n- prompting and prompt engineering\n- text and image app generation\n- search apps\n\nAs usual, there's a lesson, assignments to complete, knowledge checks and challenges.\n\nCheck it out:\n\n> https://aka.ms/genai-beginners\n\n# Are you a student?\n\nGet started with the following resources:\n\n- [Student Hub page](https://docs.microsoft.com/en-gb/learn/student-hub?WT.mc_id=academic-77958-bethanycheum) In this page, you will find beginner resources, Student packs and even ways to get a free cert voucher. This is one page you want to bookmark and check from time to time as we switch out content at least monthly.\n- [Microsoft Learn Student Ambassadors](https://studentambassadors.microsoft.com?WT.mc_id=academic-77958-bethanycheum) Join a global community of student am...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "dataease",
    "full_name": "dataease/dataease",
    "description": "🔥 人人可用的开源 BI 工具，数据可视化神器。An open-source BI tool alternative to Tableau.",
    "url": "https://github.com/dataease/dataease",
    "stars": 19978,
    "forks": 3560,
    "created_at": "2021-02-02",
    "updated_at": "2025-04-26",
    "topics": [
      "tableau",
      "data-visualization",
      "superset",
      "apache-doris",
      "data-analysis",
      "business-intelligence",
      "echarts",
      "kettle"
    ],
    "primary_language": "Java",
    "languages": {
      "Java": 15562214,
      "Vue": 4275258,
      "Shell": 27269,
      "Lua": 22086,
      "Less": 12533,
      "Dockerfile": 952
    },
    "readme_content": "<p align=\"center\"><a href=\"https://dataease.io\"><img src=\"https://dataease.oss-cn-hangzhou.aliyuncs.com/img/dataease-logo.png\" alt=\"DataEase\" width=\"300\" /></a></p>\n<h3 align=\"center\">人人可用的开源 BI 工具</h3>\n<p align=\"center\">\n  <a href=\"https://www.gnu.org/licenses/gpl-3.0.html\"><img src=\"https://img.shields.io/github/license/dataease/dataease?color=%231890FF\" alt=\"License: GPL v3\"></a>\n  <a href=\"https://app.codacy.com/gh/dataease/dataease?utm_source=github.com&utm_medium=referral&utm_content=dataease/dataease&utm_campaign=Badge_Grade_Dashboard\"><img src=\"https://app.codacy.com/project/badge/Grade/da67574fd82b473992781d1386b937ef\" alt=\"Codacy\"></a>\n  <a href=\"https://github.com/dataease/dataease\"><img src=\"https://img.shields.io/github/stars/dataease/dataease?color=%231890FF&style=flat-square\" alt=\"GitHub Stars\"></a>\n  <a href=\"https://github.com/dataease/dataease/releases\"><img src=\"https://img.shields.io/github/v/release/dataease/dataease\" alt=\"GitHub release\"></a>\n  <a href=\"https://gitee.com/fit2cloud-feizhiyun/DataEase\"><img src=\"https://gitee.com/fit2cloud-feizhiyun/DataEase/badge/star.svg?theme=gvp\" alt=\"Gitee Stars\"></a>\n</p>\n<p align=\"center\">\n  <a href=\"/README.md\"><img alt=\"中文(简体)\" src=\"https://img.shields.io/badge/中文(简体)-d9d9d9\"></a>\n  <a href=\"/docs/README.en.md\"><img alt=\"English\" src=\"https://img.shields.io/badge/English-d9d9d9\"></a>\n  <a href=\"/docs/README.zh-Hant.md\"><img alt=\"中文(繁體)\" src=\"https://img.shields.io/badge/中文(繁體)-d9d9d9\"></a>\n  <a href=\"/docs/README.ja.md\"><img alt=\"日本語\" src=\"https://img.shields.io/badge/日本語-d9d9d9\"></a>\n  <a href=\"/docs/README.pt-br.md\"><img alt=\"Português (Brasil)\" src=\"https://img.shields.io/badge/Português (Brasil)-d9d9d9\"></a>\n  <a href=\"/docs/README.ar.md\"><img alt=\"العربية\" src=\"https://img.shields.io/badge/العربية-d9d9d9\"></a>\n  <a href=\"/docs/README.de.md\"><img alt=\"Deutsch\" src=\"https://img.shields.io/badge/Deutsch-d9d9d9\"></a>\n  <a href=\"/docs/README.es.md\"><img alt=\"Español\" src=\"https://img.shields.io/badge/Español-d9d9d9\"></a>\n  <a href=\"/docs/README.fr.md\"><img alt=\"français\" src=\"https://img.shields.io/badge/français-d9d9d9\"></a>\n  <a href=\"/docs/README.ko.md\"><img alt=\"한국어\" src=\"https://img.shields.io/badge/한국어-d9d9d9\"></a>\n  <a href=\"/docs/README.id.md\"><img alt=\"Bahasa Indonesia\" src=\"https://img.shields.io/badge/Bahasa Indonesia-d9d9d9\"></a>\n  <a href=\"/docs/README.tr.md\"><img alt=\"Türkçe\" src=\"https://img.shields.io/badge/Türkçe-d9d9d9\"></a>\n</p>\n\n------------------------------\n\n## 什么是 DataEase？\n\nDataEase 是开源的 BI 工具，帮助用户快速分析数据并洞察业务趋势，从而实现业务的改进与优化。DataEase 支持丰富的数据源连接，能够通过拖拉拽方式快速制作图表，并可以方便的与他人分享。\n\n**DataEase 的优势：**\n\n-   开源开放：零门槛，线上快速获取和安装，按月迭代；\n-   简单易用：极易上手，通过鼠标点击和拖拽即可完成分析；\n-   全场景支持：多平台安装和多样化嵌入支持；\n-   安全分享：支持多种数据分享方式，确保数据安全。\n\n**DataEase 支持的数据源：**\n\n-   OLTP 数据库： MySQL、Oracle、SQL Server、PostgreSQL、MariaDB、Db2、TiDB、MongoDB-BI 等；\n-   OLAP 数据库： ClickHouse、Apache Doris、Apache Impala、StarRocks 等；\n-   数据仓库/数据湖： Amazon RedShift 等；\n-   数据文件： Excel、CSV 等；\n-   API 数据源。\n\n如果您需要向团队介绍 DataEase，可以使用这个 [官方 PPT 材料](https://fit2cloud.com/dataease/download/introduce-dataease_202501.pdf)。\n\n## 快速开始\n\n**桌面版：**\n\n你可以在 PC 上安装 DataEasae 桌面版，下载地址为：https://dataease.cn/\n\n**服务器版：**\n\n```\n# 准备一台 2 核 4G 以上的 Linux 服务器，并以 root 用户运行以下一键安装脚本：\n\ncurl -sSL https://dataease.oss-cn-hangzhou.aliyuncs.com/quick_start_v2.sh | bash\n\n# 用户名: admin\n# 密码: DataEase@123456\n```\n\n你也可以通过 [1Panel 应用商店](https://dataease.io/docs/v2/installation/1panel_installation/) 快速部署 DataEase。如果是用于生产环境，推荐使用 [离线安装包方式](https://dataease.io/docs/v2/installation/offline_INSTL_and_UPG/) 进行安装部署。\n\n如你有更多问题，可以查看在线文档，或者通过论坛与我们交流。\n\n-   [视频介绍](https://www.bilibili.com/video/BV1Y8dAYLErb/)\n-   [在线文档](https://dataease.io/docs/)\n-   [社区论坛](https://bbs.fit2cloud.com/c/de/6)\n-   [案例研究](/docs/use-cases.md)\n\n## UI 展示\n\n<table style=\"border-collapse: collapse; border: 1px solid black;\">\n  <tr>\n    <td style=\"padding: 5px;background-color:#fff;\"><img src= \"https://github.com/dataease/dataease/assets/41712985/8dbed4e1-39f0-4392-aa8c-d1fd83ba42eb\" alt=\"DataEase 工作台\"   /></td>\n    <td style=\"padding: 5px;background-color:#fff;\"><img src= \"https://github.com/dataease/dataease/assets/41712985/7c54cb07-51ef-4bb6-a931-8a95c64c7e11\" alt=\"DataEase 仪表板\"   /></td>\n  </tr>\n\n  <tr>\n    <td style=\"padding: 5px;background-color:#fff;\"><img src= \"https://github.com/dataease/dataease/assets/41712985/ffa79361-a7b3-4486-b14a-f3fd3a28f01a\" alt=\"DataEase 数据源\"   /></td>\n    <td style=\"padding: 5px;background-color:#fff;\"><img src= \"https://github.com/dataease/dataease/assets/41712985/bb28f4e4-636e-4ab0-85c5-1dfbd7a5397e\" alt=\"DataEase 模板中心\"   /></td>\n  </tr>\n</table>\n\n## 技术栈\n\n-   前端：[Vue.js](https://vuejs.org/)、[Element](https://element.eleme.cn/)\n-   图库：[AntV](https://antv.vision/zh)\n-   后端：[Spring Boot](https://spring.io/projects/spring-boot)\n-   数据库：[MySQL](https://www.mysql.com/)\n-   数据处理：[Apache Calcite](https://github.com/apache/calcite/)、[Apache SeaTunnel](https://github.com/apache/seatunnel)\n-   基础设施：[Docker](https://www.docker.com/)\n\n## 飞致云的其他明星项目\n\n- [...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "tensorflow",
    "full_name": "tensorflow/tensorflow",
    "description": "An Open Source Machine Learning Framework for Everyone",
    "url": "https://github.com/tensorflow/tensorflow",
    "stars": 189651,
    "forks": 74649,
    "created_at": "2015-11-07",
    "updated_at": "2025-04-26",
    "topics": [
      "tensorflow",
      "machine-learning",
      "python",
      "deep-learning",
      "deep-neural-networks",
      "neural-network",
      "ml",
      "distributed"
    ],
    "primary_language": "C++",
    "languages": {
      "C++": 100440745,
      "Python": 45812977,
      "MLIR": 11536604,
      "HTML": 7662661,
      "Starlark": 7302086,
      "Go": 2173562,
      "C": 1247325,
      "Java": 1178817,
      "Jupyter Notebook": 805762,
      "Shell": 705077,
      "Objective-C++": 279654,
      "Objective-C": 169277,
      "CMake": 148613,
      "Smarty": 139504,
      "Swift": 81677,
      "Dockerfile": 38776,
      "C#": 13585,
      "Batchfile": 12126,
      "Ruby": 8898,
      "Perl": 7536,
      "Roff": 5034,
      "Linker Script": 4497,
      "Cython": 3899,
      "Makefile": 2845,
      "CSS": 2761,
      "Vim Snippet": 58
    },
    "readme_content": "<div align=\"center\">\n  <img src=\"https://www.tensorflow.org/images/tf_logo_horizontal.png\">\n</div>\n\n[![Python](https://img.shields.io/pypi/pyversions/tensorflow.svg)](https://badge.fury.io/py/tensorflow)\n[![PyPI](https://badge.fury.io/py/tensorflow.svg)](https://badge.fury.io/py/tensorflow)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4724125.svg)](https://doi.org/10.5281/zenodo.4724125)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1486/badge)](https://bestpractices.coreinfrastructure.org/projects/1486)\n[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/tensorflow/tensorflow/badge)](https://securityscorecards.dev/viewer/?uri=github.com/tensorflow/tensorflow)\n[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/tensorflow.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:tensorflow)\n[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/tensorflow-py.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:tensorflow-py)\n[![OSSRank](https://shields.io/endpoint?url=https://ossrank.com/shield/44)](https://ossrank.com/p/44)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg)](CODE_OF_CONDUCT.md)\n[![TF Official Continuous](https://tensorflow.github.io/build/TF%20Official%20Continuous.svg)](https://tensorflow.github.io/build#TF%20Official%20Continuous)\n[![TF Official Nightly](https://tensorflow.github.io/build/TF%20Official%20Nightly.svg)](https://tensorflow.github.io/build#TF%20Official%20Nightly)\n\n**`Documentation`** |\n------------------- |\n[![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/api_docs/) |\n\n[TensorFlow](https://www.tensorflow.org/) is an end-to-end open source platform\nfor machine learning. It has a comprehensive, flexible ecosystem of\n[tools](https://www.tensorflow.org/resources/tools),\n[libraries](https://www.tensorflow.org/resources/libraries-extensions), and\n[community](https://www.tensorflow.org/community) resources that lets\nresearchers push the state-of-the-art in ML and developers easily build and\ndeploy ML-powered applications.\n\nTensorFlow was originally developed by researchers and engineers working within\nthe Machine Intelligence team at Google Brain to conduct research in machine\nlearning and neural networks. However, the framework is versatile enough to be\nused in other areas as well.\n\nTensorFlow provides stable [Python](https://www.tensorflow.org/api_docs/python)\nand [C++](https://www.tensorflow.org/api_docs/cc) APIs, as well as a\nnon-guaranteed backward compatible API for\n[other languages](https://www.tensorflow.org/api_docs).\n\nKeep up-to-date with release announcements and security updates by subscribing\nto\n[announce@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/announce).\nSee all the [mailing lists](https://www.tensorflow.org/community/forums).\n\n## Install\n\nSee the [TensorFlow install guide](https://www.tensorflow.org/install) for the\n[pip package](https://www.tensorflow.org/install/pip), to\n[enable GPU support](https://www.tensorflow.org/install/gpu), use a\n[Docker container](https://www.tensorflow.org/install/docker), and\n[build from source](https://www.tensorflow.org/install/source).\n\nTo install the current release, which includes support for\n[CUDA-enabled GPU cards](https://www.tensorflow.org/install/gpu) *(Ubuntu and\nWindows)*:\n\n```\n$ pip install tensorflow\n```\n\nOther devices (DirectX and MacOS-metal) are supported using\n[Device plugins](https://www.tensorflow.org/install/gpu_plugins#available_devices).\n\nA smaller CPU-only package is also available:\n\n```\n$ pip install tensorflow-cpu\n```\n\nTo update TensorFlow to the latest version, add `--upgrade` flag to the above\ncommands.\n\n*Nightly binaries are available for testing using the\n[tf-nightly](https://pypi.python.org/pypi/tf-nightly) and\n[tf-nightly-cpu](https://pypi.python.org/pypi/tf-nightly-cpu) packages on PyPI.*\n\n#### *Try your first TensorFlow program*\n\n```shell\n$ python\n```\n\n```python\n>>> import tensorflow as tf\n>>> tf.add(1, 2).numpy()\n3\n>>> hello = tf.constant('Hello, TensorFlow!')\n>>> hello.numpy()\nb'Hello, TensorFlow!'\n```\n\nFor more examples, see the\n[TensorFlow tutorials](https://www.tensorflow.org/tutorials/).\n\n## Contribution guidelines\n\n**If you want to contribute to TensorFlow, be sure to review the\n[contribution guidelines](CONTRIBUTING.md). This project adheres to TensorFlow's\n[code of conduct](CODE_OF_CONDUCT.md). By participating, you are expected to\nuphold this code.**\n\n**We use [GitHub issues](https://github.com/tensorflow/tensorflow/issues) for\ntracking requests and bugs, please see\n[TensorFlow Forum](https://discuss.tensorflow.org/) for general questions and\ndiscussion, and please direct specific questions to\n[Stack Overflow](https://stackoverflow.com/questions/tagged/tensorflow).**\n\nThe TensorFlow project strives to ...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "stable-diffusion-webui",
    "full_name": "AUTOMATIC1111/stable-diffusion-webui",
    "description": "Stable Diffusion web UI",
    "url": "https://github.com/AUTOMATIC1111/stable-diffusion-webui",
    "stars": 151749,
    "forks": 28217,
    "created_at": "2022-08-22",
    "updated_at": "2025-04-26",
    "topics": [
      "deep-learning",
      "diffusion",
      "image-generation",
      "image2image",
      "img2img",
      "text2image",
      "txt2img",
      "ai",
      "ai-art",
      "gradio",
      "pytorch",
      "stable-diffusion",
      "torch",
      "upscaling",
      "web",
      "unstable"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 1840622,
      "JavaScript": 175861,
      "CSS": 44551,
      "HTML": 27503,
      "Shell": 13304,
      "Batchfile": 2631
    },
    "readme_content": "# Stable Diffusion web UI\r\nA web interface for Stable Diffusion, implemented using Gradio library.\r\n\r\n![](screenshot.png)\r\n\r\n## Features\r\n[Detailed feature showcase with images](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features):\r\n- Original txt2img and img2img modes\r\n- One click install and run script (but you still must install python and git)\r\n- Outpainting\r\n- Inpainting\r\n- Color Sketch\r\n- Prompt Matrix\r\n- Stable Diffusion Upscale\r\n- Attention, specify parts of text that the model should pay more attention to\r\n    - a man in a `((tuxedo))` - will pay more attention to tuxedo\r\n    - a man in a `(tuxedo:1.21)` - alternative syntax\r\n    - select text and press `Ctrl+Up` or `Ctrl+Down` (or `Command+Up` or `Command+Down` if you're on a MacOS) to automatically adjust attention to selected text (code contributed by anonymous user)\r\n- Loopback, run img2img processing multiple times\r\n- X/Y/Z plot, a way to draw a 3 dimensional plot of images with different parameters\r\n- Textual Inversion\r\n    - have as many embeddings as you want and use any names you like for them\r\n    - use multiple embeddings with different numbers of vectors per token\r\n    - works with half precision floating point numbers\r\n    - train embeddings on 8GB (also reports of 6GB working)\r\n- Extras tab with:\r\n    - GFPGAN, neural network that fixes faces\r\n    - CodeFormer, face restoration tool as an alternative to GFPGAN\r\n    - RealESRGAN, neural network upscaler\r\n    - ESRGAN, neural network upscaler with a lot of third party models\r\n    - SwinIR and Swin2SR ([see here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/2092)), neural network upscalers\r\n    - LDSR, Latent diffusion super resolution upscaling\r\n- Resizing aspect ratio options\r\n- Sampling method selection\r\n    - Adjust sampler eta values (noise multiplier)\r\n    - More advanced noise setting options\r\n- Interrupt processing at any time\r\n- 4GB video card support (also reports of 2GB working)\r\n- Correct seeds for batches\r\n- Live prompt token length validation\r\n- Generation parameters\r\n     - parameters you used to generate images are saved with that image\r\n     - in PNG chunks for PNG, in EXIF for JPEG\r\n     - can drag the image to PNG info tab to restore generation parameters and automatically copy them into UI\r\n     - can be disabled in settings\r\n     - drag and drop an image/text-parameters to promptbox\r\n- Read Generation Parameters Button, loads parameters in promptbox to UI\r\n- Settings page\r\n- Running arbitrary python code from UI (must run with `--allow-code` to enable)\r\n- Mouseover hints for most UI elements\r\n- Possible to change defaults/mix/max/step values for UI elements via text config\r\n- Tiling support, a checkbox to create images that can be tiled like textures\r\n- Progress bar and live image generation preview\r\n    - Can use a separate neural network to produce previews with almost none VRAM or compute requirement\r\n- Negative prompt, an extra text field that allows you to list what you don't want to see in generated image\r\n- Styles, a way to save part of prompt and easily apply them via dropdown later\r\n- Variations, a way to generate same image but with tiny differences\r\n- Seed resizing, a way to generate same image but at slightly different resolution\r\n- CLIP interrogator, a button that tries to guess prompt from an image\r\n- Prompt Editing, a way to change prompt mid-generation, say to start making a watermelon and switch to anime girl midway\r\n- Batch Processing, process a group of files using img2img\r\n- Img2img Alternative, reverse Euler method of cross attention control\r\n- Highres Fix, a convenience option to produce high resolution pictures in one click without usual distortions\r\n- Reloading checkpoints on the fly\r\n- Checkpoint Merger, a tab that allows you to merge up to 3 checkpoints into one\r\n- [Custom scripts](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Custom-Scripts) with many extensions from community\r\n- [Composable-Diffusion](https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/), a way to use multiple prompts at once\r\n     - separate prompts using uppercase `AND`\r\n     - also supports weights for prompts: `a cat :1.2 AND a dog AND a penguin :2.2`\r\n- No token limit for prompts (original stable diffusion lets you use up to 75 tokens)\r\n- DeepDanbooru integration, creates danbooru style tags for anime prompts\r\n- [xformers](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers), major speed increase for select cards: (add `--xformers` to commandline args)\r\n- via extension: [History tab](https://github.com/yfszzx/stable-diffusion-webui-images-browser): view, direct and delete images conveniently within the UI\r\n- Generate forever option\r\n- Training tab\r\n     - hypernetworks and embeddings options\r\n     - Preprocessing images: cropping, mirroring, autotagging using BLIP or deepdanbooru (for anime)\r\n- Clip skip\r\n- Hypernetworks\r\n- Loras (same as Hypernetworks ...",
    "owner_type": "User",
    "is_archived": false
  },
  {
    "name": "transformers",
    "full_name": "huggingface/transformers",
    "description": "🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.",
    "url": "https://github.com/huggingface/transformers",
    "stars": 143514,
    "forks": 28775,
    "created_at": "2018-10-29",
    "updated_at": "2025-04-26",
    "topics": [
      "nlp",
      "natural-language-processing",
      "pytorch",
      "language-model",
      "tensorflow",
      "bert",
      "language-models",
      "pytorch-transformers",
      "nlp-library",
      "transformer",
      "model-hub",
      "pretrained-models",
      "jax",
      "flax",
      "seq2seq",
      "speech-recognition",
      "hacktoberfest",
      "python",
      "machine-learning",
      "deep-learning"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 67095130,
      "Cuda": 204021,
      "Dockerfile": 38230,
      "C++": 19093,
      "C": 7703,
      "Makefile": 4087,
      "Cython": 3635,
      "Shell": 1838,
      "Jsonnet": 937
    },
    "readme_content": "<!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\">\n    <img alt=\"Hugging Face Transformers Library\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\" width=\"352\" height=\"59\" style=\"max-width: 100%;\">\n  </picture>\n  <br/>\n  <br/>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://huggingface.com/models\"><img alt=\"Checkpoints on Hub\" src=\"https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen\"></a>\n    <a href=\"https://circleci.com/gh/huggingface/transformers\"><img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/huggingface/transformers/main\"></a>\n    <a href=\"https://github.com/huggingface/transformers/blob/main/LICENSE\"><img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/transformers.svg?color=blue\"></a>\n    <a href=\"https://huggingface.co/docs/transformers/index\"><img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online\"></a>\n    <a href=\"https://github.com/huggingface/transformers/releases\"><img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/transformers.svg\"></a>\n    <a href=\"https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md\"><img alt=\"Contributor Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg\"></a>\n    <a href=\"https://zenodo.org/badge/latestdoi/155220641\"><img src=\"https://zenodo.org/badge/155220641.svg\" alt=\"DOI\"></a>\n</p>\n\n<h4 align=\"center\">\n    <p>\n        <b>English</b> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md\">简体中文</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md\">繁體中文</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md\">한국어</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_es.md\">Español</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md\">日本語</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md\">हिन्दी</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md\">Русский</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md\">Рortuguês</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_te.md\">తెలుగు</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md\">Français</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_de.md\">Deutsch</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md\">Tiếng Việt</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md\">العربية</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md\">اردو</a> |\n    </p>\n</h4>\n\n<h3 align=\"center\">\n    <p>State-of-the-art pretrained models for inference and training</p>\n</h3>\n\n<h3 align=\"center\">\n    <a href=\"https://hf.co/course\"><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png\"></a>\n</h3>\n\nTransformers is a library of pretrained text, computer vision, audio, video, and multimodal models for inference and training. Use Transformers to fine-tune models on your data, build inference applications, and for generative AI use cases across multiple modalities.\n\nThere are over 500K+ Transformers [model checkpoints](https://huggingface.co/models?library=transformers&sort=trending) on the [Hugging Face Hub](https://huggingface.com/models) you can use.\n\nExplore the [Hub](https://huggingface.com/) today to find a model and use Transformers to help you get started right away.\n\n## Installation\n\nTransformers works with Python 3.9+ [PyTorch](https://pytorch.org/get-started/locally/) 2.1+, [TensorFlow](https://www.te...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "pytorch",
    "full_name": "pytorch/pytorch",
    "description": "Tensors and Dynamic neural networks in Python with strong GPU acceleration",
    "url": "https://github.com/pytorch/pytorch",
    "stars": 89410,
    "forks": 23974,
    "created_at": "2016-08-13",
    "updated_at": "2025-04-26",
    "topics": [
      "neural-network",
      "autograd",
      "gpu",
      "numpy",
      "deep-learning",
      "tensor",
      "python",
      "machine-learning"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 68969194,
      "C++": 41228025,
      "Cuda": 3610955,
      "C": 1784698,
      "Objective-C++": 1348899,
      "CMake": 823348,
      "Shell": 511417,
      "Assembly": 336348,
      "Starlark": 326427,
      "GLSL": 204577,
      "Jupyter Notebook": 183407,
      "Metal": 176719,
      "Java": 135037,
      "JavaScript": 91258,
      "Batchfile": 89106,
      "Objective-C": 53593,
      "Dockerfile": 49528,
      "Makefile": 12248,
      "Thrift": 6963,
      "PowerShell": 3657,
      "Ruby": 2774,
      "GDB": 653,
      "Linker Script": 473,
      "HTML": 384,
      "Smarty": 376,
      "Vim Script": 154
    },
    "readme_content": "![PyTorch Logo](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png)\n\n--------------------------------------------------------------------------------\n\nPyTorch is a Python package that provides two high-level features:\n- Tensor computation (like NumPy) with strong GPU acceleration\n- Deep neural networks built on a tape-based autograd system\n\nYou can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.\n\nOur trunk health (Continuous Integration signals) can be found at [hud.pytorch.org](https://hud.pytorch.org/ci/pytorch/pytorch/main).\n\n<!-- toc -->\n\n- [More About PyTorch](#more-about-pytorch)\n  - [A GPU-Ready Tensor Library](#a-gpu-ready-tensor-library)\n  - [Dynamic Neural Networks: Tape-Based Autograd](#dynamic-neural-networks-tape-based-autograd)\n  - [Python First](#python-first)\n  - [Imperative Experiences](#imperative-experiences)\n  - [Fast and Lean](#fast-and-lean)\n  - [Extensions Without Pain](#extensions-without-pain)\n- [Installation](#installation)\n  - [Binaries](#binaries)\n    - [NVIDIA Jetson Platforms](#nvidia-jetson-platforms)\n  - [From Source](#from-source)\n    - [Prerequisites](#prerequisites)\n      - [NVIDIA CUDA Support](#nvidia-cuda-support)\n      - [AMD ROCm Support](#amd-rocm-support)\n      - [Intel GPU Support](#intel-gpu-support)\n    - [Get the PyTorch Source](#get-the-pytorch-source)\n    - [Install Dependencies](#install-dependencies)\n    - [Install PyTorch](#install-pytorch)\n      - [Adjust Build Options (Optional)](#adjust-build-options-optional)\n  - [Docker Image](#docker-image)\n    - [Using pre-built images](#using-pre-built-images)\n    - [Building the image yourself](#building-the-image-yourself)\n  - [Building the Documentation](#building-the-documentation)\n  - [Previous Versions](#previous-versions)\n- [Getting Started](#getting-started)\n- [Resources](#resources)\n- [Communication](#communication)\n- [Releases and Contributing](#releases-and-contributing)\n- [The Team](#the-team)\n- [License](#license)\n\n<!-- tocstop -->\n\n## More About PyTorch\n\n[Learn the basics of PyTorch](https://pytorch.org/tutorials/beginner/basics/intro.html)\n\nAt a granular level, PyTorch is a library that consists of the following components:\n\n| Component | Description |\n| ---- | --- |\n| [**torch**](https://pytorch.org/docs/stable/torch.html) | A Tensor library like NumPy, with strong GPU support |\n| [**torch.autograd**](https://pytorch.org/docs/stable/autograd.html) | A tape-based automatic differentiation library that supports all differentiable Tensor operations in torch |\n| [**torch.jit**](https://pytorch.org/docs/stable/jit.html) | A compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code  |\n| [**torch.nn**](https://pytorch.org/docs/stable/nn.html) | A neural networks library deeply integrated with autograd designed for maximum flexibility |\n| [**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html) | Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training |\n| [**torch.utils**](https://pytorch.org/docs/stable/data.html) | DataLoader and other utility functions for convenience |\n\nUsually, PyTorch is used either as:\n\n- A replacement for NumPy to use the power of GPUs.\n- A deep learning research platform that provides maximum flexibility and speed.\n\nElaborating Further:\n\n### A GPU-Ready Tensor Library\n\nIf you use NumPy, then you have used Tensors (a.k.a. ndarray).\n\n![Tensor illustration](./docs/source/_static/img/tensor_illustration.png)\n\nPyTorch provides Tensors that can live either on the CPU or the GPU and accelerates the\ncomputation by a huge amount.\n\nWe provide a wide variety of tensor routines to accelerate and fit your scientific computation needs\nsuch as slicing, indexing, mathematical operations, linear algebra, reductions.\nAnd they are fast!\n\n### Dynamic Neural Networks: Tape-Based Autograd\n\nPyTorch has a unique way of building neural networks: using and replaying a tape recorder.\n\nMost frameworks such as TensorFlow, Theano, Caffe, and CNTK have a static view of the world.\nOne has to build a neural network and reuse the same structure again and again.\nChanging the way the network behaves means that one has to start from scratch.\n\nWith PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to\nchange the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes\nfrom several research papers on this topic, as well as current and past work such as\n[torch-autograd](https://github.com/twitter/torch-autograd),\n[autograd](https://github.com/HIPS/autograd),\n[Chainer](https://chainer.org), etc.\n\nWhile this technique is not unique to PyTorch, it's one of the fastest implementations of it to date.\nYou get the best of speed and flexibility for your crazy research.\n\n![Dynamic graph](https://github.com/pytorch/pytorch/raw/main/docs/s...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "opencv",
    "full_name": "opencv/opencv",
    "description": "Open Source Computer Vision Library",
    "url": "https://github.com/opencv/opencv",
    "stars": 81880,
    "forks": 56146,
    "created_at": "2012-07-19",
    "updated_at": "2025-04-26",
    "topics": [
      "opencv",
      "c-plus-plus",
      "computer-vision",
      "deep-learning",
      "image-processing"
    ],
    "primary_language": "C++",
    "languages": {
      "C++": 43085591,
      "C": 1544403,
      "Python": 1325111,
      "CMake": 1009077,
      "Java": 761164,
      "Objective-C++": 391761,
      "Cuda": 343525,
      "Swift": 301765,
      "JavaScript": 243601,
      "Objective-C": 99998,
      "HTML": 40097,
      "Shell": 26085,
      "Perl": 15865,
      "PowerShell": 14591,
      "Kotlin": 5815,
      "TeX": 5144,
      "Batchfile": 1498,
      "Prolog": 843,
      "Dockerfile": 309
    },
    "readme_content": "## OpenCV: Open Source Computer Vision Library\n\n\n### Resources\n\n* Homepage: <https://opencv.org>\n  * Courses: <https://opencv.org/courses>\n* Docs: <https://docs.opencv.org/4.x/>\n* Q&A forum: <https://forum.opencv.org>\n  * previous forum (read only): <http://answers.opencv.org>\n* Issue tracking: <https://github.com/opencv/opencv/issues>\n* Additional OpenCV functionality: <https://github.com/opencv/opencv_contrib>\n* Donate to OpenCV: <https://opencv.org/support/>\n\n\n### Contributing\n\nPlease read the [contribution guidelines](https://github.com/opencv/opencv/wiki/How_to_contribute) before starting work on a pull request.\n\n#### Summary of the guidelines:\n\n* One pull request per issue;\n* Choose the right base branch;\n* Include tests and documentation;\n* Clean up \"oops\" commits before submitting;\n* Follow the [coding style guide](https://github.com/opencv/opencv/wiki/Coding_Style_Guide).\n\n### Additional Resources\n\n* [Submit your OpenCV-based project](https://form.jotform.com/233105358823151) for inclusion in Community Friday on opencv.org\n* [Subscribe to the OpenCV YouTube Channel](http://youtube.com/@opencvofficial) featuring OpenCV Live, an hour-long streaming show\n* [Follow OpenCV on LinkedIn](http://linkedin.com/company/opencv/) for daily posts showing the state-of-the-art in computer vision & AI\n* [Apply to be an OpenCV Volunteer](https://form.jotform.com/232745316792159) to help organize events and online campaigns as well as amplify them\n* [Follow OpenCV on Mastodon](http://mastodon.social/@opencv) in the Fediverse\n* [Follow OpenCV on Twitter](https://twitter.com/opencvlive)\n* [OpenCV.ai](https://opencv.ai): Computer Vision and AI development services from the OpenCV team.\n",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "d2l-zh",
    "full_name": "d2l-ai/d2l-zh",
    "description": "《动手学深度学习》：面向中文读者、能运行、可讨论。中英文版被70多个国家的500多所大学用于教学。",
    "url": "https://github.com/d2l-ai/d2l-zh",
    "stars": 68791,
    "forks": 11582,
    "created_at": "2017-08-23",
    "updated_at": "2025-04-26",
    "topics": [
      "deep-learning",
      "book",
      "notebook",
      "natural-language-processing",
      "computer-vision",
      "machine-learning",
      "python",
      "chinese"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 427646,
      "HTML": 68515,
      "TeX": 61055,
      "Shell": 10935
    },
    "readme_content": "# 动手学深度学习（Dive into Deep Learning，D2L.ai）\n\n[第二版：zh.D2L.ai](https://zh.d2l.ai)  | [第一版：zh-v1.D2L.ai](https://zh-v1.d2l.ai/) |  安装和使用书中源代码： [第二版](https://zh.d2l.ai/chapter_installation/index.html) [第一版](https://zh-v1.d2l.ai/chapter_prerequisite/install.html)\n\n<h5 align=\"center\"><i>理解深度学习的最佳方法是学以致用。</i></h5>\n\n<p align=\"center\">\n  <img width=\"200\"  src=\"static/frontpage/_images/eq.jpg\">\n  <img width=\"200\"  src=\"static/frontpage/_images/figure.jpg\">\n  <img width=\"200\"  src=\"static/frontpage/_images/code.jpg\">\n  <img width=\"200\"  src=\"static/frontpage/_images/notebook.gif\">\n</p>\n\n本开源项目代表了我们的一种尝试：我们将教给读者概念、背景知识和代码；我们将在同一个地方阐述剖析问题所需的批判性思维、解决问题所需的数学知识，以及实现解决方案所需的工程技能。\n\n我们的目标是创建一个为实现以下目标的统一资源：\n1. 所有人均可在网上免费获取；\n1. 提供足够的技术深度，从而帮助读者实际成为深度学习应用科学家：既理解数学原理，又能够实现并不断改进方法；\n1. 包含可运行的代码，为读者展示如何在实际中解决问题。这样不仅直接将数学公式对应成实际代码，而且可以修改代码、观察结果并及时获取经验；\n1. 允许我们和整个社区不断快速迭代内容，从而紧跟仍在高速发展的深度学习领域；\n1. 由包含有关技术细节问答的论坛作为补充，使大家可以相互答疑并交换经验。\n\n<h5 align=\"center\">将本书（中英文版）用作教材或参考书的大学</h5>\n<p align=\"center\">\n  <img width=\"400\"  src=\"https://d2l.ai/_images/map.png\">\n</p>\n\n如果本书对你有帮助，请Star (★) 本仓库或引用本书的英文版：\n\n```\n@book{zhang2023dive,\n    title={Dive into Deep Learning},\n    author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},\n    publisher={Cambridge University Press},\n    note={\\url{https://D2L.ai}},\n    year={2023}\n}\n```\n\n## 本书的英文版\n\n虽然纸质书已出版，但深度学习领域依然在迅速发展。为了得到来自更广泛的英文开源社区的帮助，从而提升本书质量，本书的新版将继续用英文编写，并搬回中文版。\n\n欢迎关注本书的[英文开源项目](https://github.com/d2l-ai/d2l-en)。\n\n## 中英文教学资源\n\n加州大学伯克利分校 2019 年春学期 [*Introduction to Deep Learning* 课程](http://courses.d2l.ai/berkeley-stat-157/index.html)教材（同时提供含教学视频地址的[中文版课件](https://github.com/d2l-ai/berkeley-stat-157/tree/master/slides-zh)）。\n\n## 学术界推荐\n\n> <p>\"Dive into this book if you want to dive into deep learning!\"</p>\n> <b>&mdash; 韩家炜，ACM 院士、IEEE 院士，美国伊利诺伊大学香槟分校计算机系 Michael Aiken Chair 教授</b>\n\n> <p>\"This is a highly welcome addition to the machine learning literature.\"</p>\n> <b>&mdash; Bernhard Schölkopf，ACM 院士、德国国家科学院院士，德国马克斯•普朗克研究所智能系统院院长</b>\n\n> <p>\"书中代码可谓‘所学即所用’。\"</p>\n> <b>&mdash; 周志华，ACM 院士、IEEE 院士、AAAS 院士，南京大学计算机科学与技术系主任</b>\n\n> <p>\"这本书可以帮助深度学习实践者快速提升自己的能力。\"</p>\n> <b>&mdash; 张潼，ASA 院士、IMS 院士，香港科技大学计算机系和数学系教授</b>\n\n## 工业界推荐\n\n> <p>\"一本优秀的深度学习教材，值得任何想了解深度学习何以引爆人工智能革命的人关注。\"</p>\n> <b>&mdash; 黄仁勋，NVIDIA创始人 & CEO</b>\n\n> <p>\"《动手学深度学习》是最适合工业界研发工程师学习的。我毫无保留地向广大的读者们强烈推荐。\"</p>\n> <b>&mdash; 余凯，地平线公司创始人 & CEO</b>\n\n> <p>\"强烈推荐这本书！我特别赞赏这种手脑一体的学习方式。\"</p>\n> <b>&mdash; 漆远，复旦大学“浩清”教授、人工智能创新与产业研究院院长</b>\n\n> <p>\"《动手学深度学习》是一本很容易让学习者上瘾的书。\"</p>\n> <b>&mdash; 沈强，将门创投创始合伙人</b>\n\n## 贡献\n\n感谢[社区贡献者们](https://github.com/d2l-ai/d2l-zh/graphs/contributors)为每一位读者改进这本开源书。\n\n[如何贡献](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/contributing.html) | [致谢](https://zh.d2l.ai/chapter_preface/index.html) | [讨论或报告问题](https://discuss.d2l.ai/c/chinese-version/16) | [其他](INFO.md)\n",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "cs-video-courses",
    "full_name": "Developer-Y/cs-video-courses",
    "description": "List of Computer Science courses with video lectures.",
    "url": "https://github.com/Developer-Y/cs-video-courses",
    "stars": 68567,
    "forks": 9278,
    "created_at": "2016-10-21",
    "updated_at": "2025-04-26",
    "topics": [
      "computer-science",
      "algorithms",
      "systems",
      "databases",
      "machine-learning",
      "web-development",
      "security",
      "computer-architecture",
      "bioinformatics",
      "robotics",
      "embedded-systems",
      "database-systems",
      "computer-vision",
      "quantum-computing",
      "computational-biology",
      "computational-physics",
      "deep-learning",
      "reinforcement-learning"
    ],
    "primary_language": null,
    "languages": {},
    "readme_content": "<!-- omit in toc -->\n# Computer Science courses with video lectures\n\n<!-- omit in toc -->\n## Introduction\n\n- Please check [NOTES](https://github.com/Developer-Y/cs-video-courses/blob/master/NOTES.md) for general information about this list.\n- Please refer [CONTRIBUTING.md](https://github.com/Developer-Y/cs-video-courses/blob/master/CONTRIBUTING.md) for contribution guidelines.\n- Please feel free to raise any genuine issue you may have, however, it has been noticed that few people open empty issues to raise their GitHub contribution on their account. Such spammers will be blocked. \n- You are welcome to contribute, please create PR for actual college/University level courses. Please do not add links for small MOOCs, basic tutorials, or advertisements for some sites/channels.\n\n------------------------------\n\nTable of Contents\n\n------------------------------\n\n- [Introduction to Computer Science](#introduction-to-computer-science)\n- [Data Structures and Algorithms](#data-structures-and-algorithms)\n- [Systems Programming](#systems-programming)\n  * [Operating Systems](#operating-systems)\n  * [Distributed Systems](#distributed-systems)\n  * [Real-Time Systems](#real-time-systems) \n- [Database Systems](#database-systems)\n- [Software Engineering](#software-engineering)\n  * [Object Oriented Design](#object-oriented-design)\n  * [Software Engineering](#software-engineering)\n  * [Software Architecture](#software-architecture)\n  * [Concurrency](#concurrency)\n  * [Mobile Application Development](#mobile-application-development)\n- [Artificial Intelligence](#artificial-intelligence)\n- [Machine Learning](#machine-learning)\n  * [Introduction to Machine Learning](#introduction-to-machine-learning)\n  * [Data Mining](#data-mining)\n  * [Probabilistic Graphical Modeling](#probabilistic-graphical-modeling)\n  * [Deep Learning](#deep-learning)\n  * [Reinforcement Learning](#reinforcement-learning)\n  * [Advanced Machine Learning](#advanced-machine-learning)\n  * [Natural Language Processing](#natural-language-processing)\n  * [Generative AI](#generative-ai)\n  * [Computer Vision](#computer-vision)\n  * [Time Series Analysis](#time-series-analysis)\n  * [Optimization](#optimization)\n  * [Unsupervised Learning](#unsupervised-learning)\n  * [Misc Machine Learning Topics](#misc-machine-learning-topics)\n- [Computer Networks](#computer-networks)\n- [Math for Computer Scientist](#math-for-computer-scientist)\n- [Web Programming and Internet Technologies](#web-programming-and-internet-technologies)\n- [Theoretical CS and Programming Languages](#theoretical-cs-and-programming-languages)\n- [Embedded Systems](#embedded-systems)\n- [Real time system evaluation](#real-time-system-evaluation)\n- [Computer Organization and Architecture](#computer-organization-and-architecture)\n- [Security](#security)\n- [Computer Graphics](#computer-graphics)\n- [Image Processing and Computer Vision](#image-processing-and-computer-vision)\n- [Computational Physics](#computational-physics)\n- [Computational Biology](#computational-biology)\n- [Quantum Computing](#quantum-computing)\n- [Robotics and Control](#robotics-and-control)\n- [Computational Finance](#computational-finance)\n- [Blockchain Development](#blockchain-development)\n- [Misc](#misc)\n\n<!-- omit in toc -->\n## Courses\n\n------------------------------\n\n### Introduction to Computer Science\n\n- [CS 10 - The Beauty and Joy of Computing - Spring 2015 - Dan Garcia - UC Berkeley InfoCoBuild](http://www.infocobuild.com/education/audio-video-courses/computer-science/cs10-spring2015-berkeley.html)\n- [6.0001 - Introduction to Computer Science and Programming in Python - MIT OCW](https://ocw.mit.edu/courses/6-0001-introduction-to-computer-science-and-programming-in-python-fall-2016/video_galleries/lecture-videos/)\n- [6.001 - Structure and Interpretation of Computer Programs, MIT](https://ocw.mit.edu/courses/6-001-structure-and-interpretation-of-computer-programs-spring-2005/video_galleries/video-lectures/)\n- [Introduction to Computational Thinking - MIT](https://computationalthinking.mit.edu/Fall22/)\n- [CS 50 - Introduction to Computer Science, Harvard University](https://online-learning.harvard.edu/course/cs50-introduction-computer-science) ([cs50.tv](http://cs50.tv/2017/fall/))\n- [CS50R - Introduction to Programming with R](https://cs50.harvard.edu/r/2024/) ([Lecture Videos](https://www.youtube.com/playlist?list=PLhQjrBD2T382yfNp_-xzX244d-O9W6YmD))\n- [CS 61A - Structure and Interpretation of Computer Programs [Python], UC Berkeley](https://cs61a.org/)\n- [CPSC 110 - Systematic Program Design [Racket], University of British Columbia](https://www.youtube.com/channel/UC7dEjIUwSxSNcW4PqNRQW8w/playlists?view=1&flow=grid&sort=da)\n- [CS50's Understanding Technology](https://www.youtube.com/playlist?list=PLhQjrBD2T382p8amnvUp1rws1p7n7gJ2p)\n- [CSE 142 Computer Programming I (Java Programming), Spring 2016 - University of Washington](https://courses.cs.washington.edu/courses/cse142/16sp/calendar.shtml)\n- [CS 1301 Intro to computing - Gatech](https:/...",
    "owner_type": "User",
    "is_archived": false
  },
  {
    "name": "keras",
    "full_name": "keras-team/keras",
    "description": "Deep Learning for humans",
    "url": "https://github.com/keras-team/keras",
    "stars": 62921,
    "forks": 19575,
    "created_at": "2015-03-28",
    "updated_at": "2025-04-26",
    "topics": [
      "deep-learning",
      "tensorflow",
      "neural-networks",
      "machine-learning",
      "data-science",
      "python",
      "jax",
      "pytorch"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 8800074,
      "Shell": 3904
    },
    "readme_content": "# Keras 3: Deep Learning for Humans\n\nKeras 3 is a multi-backend deep learning framework, with support for JAX, TensorFlow, PyTorch, and OpenVINO (for inference-only).\nEffortlessly build and train models for computer vision, natural language processing, audio processing,\ntimeseries forecasting, recommender systems, etc.\n\n- **Accelerated model development**: Ship deep learning solutions faster thanks to the high-level UX of Keras\nand the availability of easy-to-debug runtimes like PyTorch or JAX eager execution.\n- **State-of-the-art performance**: By picking the backend that is the fastest for your model architecture (often JAX!),\nleverage speedups ranging from 20% to 350% compared to other frameworks. [Benchmark here](https://keras.io/getting_started/benchmarks/).\n- **Datacenter-scale training**: Scale confidently from your laptop to large clusters of GPUs or TPUs.\n\nJoin nearly three million developers, from burgeoning startups to global enterprises, in harnessing the power of Keras 3.\n\n\n## Installation\n\n### Install with pip\n\nKeras 3 is available on PyPI as `keras`. Note that Keras 2 remains available as the `tf-keras` package.\n\n1. Install `keras`:\n\n```\npip install keras --upgrade\n```\n\n2. Install backend package(s).\n\nTo use `keras`, you should also install the backend of choice: `tensorflow`, `jax`, or `torch`.\nNote that `tensorflow` is required for using certain Keras 3 features: certain preprocessing layers\nas well as `tf.data` pipelines.\n\n### Local installation\n\n#### Minimal installation\n\nKeras 3 is compatible with Linux and MacOS systems. For Windows users, we recommend using WSL2 to run Keras.\nTo install a local development version:\n\n1. Install dependencies:\n\n```\npip install -r requirements.txt\n```\n\n2. Run installation command from the root directory.\n\n```\npython pip_build.py --install\n```\n\n3. Run API generation script when creating PRs that update `keras_export` public APIs:\n\n```\n./shell/api_gen.sh\n```\n\n#### Adding GPU support\n\nThe `requirements.txt` file will install a CPU-only version of TensorFlow, JAX, and PyTorch. For GPU support, we also\nprovide a separate `requirements-{backend}-cuda.txt` for TensorFlow, JAX, and PyTorch. These install all CUDA\ndependencies via `pip` and expect a NVIDIA driver to be pre-installed. We recommend a clean python environment for each\nbackend to avoid CUDA version mismatches. As an example, here is how to create a Jax GPU environment with `conda`:\n\n```shell\nconda create -y -n keras-jax python=3.10\nconda activate keras-jax\npip install -r requirements-jax-cuda.txt\npython pip_build.py --install\n```\n\n## Configuring your backend\n\nYou can export the environment variable `KERAS_BACKEND` or you can edit your local config file at `~/.keras/keras.json`\nto configure your backend. Available backend options are: `\"tensorflow\"`, `\"jax\"`, `\"torch\"`, `\"openvino\"`. Example:\n\n```\nexport KERAS_BACKEND=\"jax\"\n```\n\nIn Colab, you can do:\n\n```python\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n\nimport keras\n```\n\n**Note:** The backend must be configured before importing `keras`, and the backend cannot be changed after \nthe package has been imported.\n\n**Note:** The OpenVINO backend is an inference-only backend, meaning it is designed only for running model\npredictions using `model.predict()` method.\n\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/).\n",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "annotated_deep_learning_paper_implementations",
    "full_name": "labmlai/annotated_deep_learning_paper_implementations",
    "description": "🧑‍🏫 60+ Implementations/tutorials of deep learning papers with side-by-side notes 📝; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, sophia, ...), gans(cyclegan, stylegan2, ...), 🎮 reinforcement learning (ppo, dqn), capsnet, distillation, ... 🧠",
    "url": "https://github.com/labmlai/annotated_deep_learning_paper_implementations",
    "stars": 60297,
    "forks": 6087,
    "created_at": "2020-08-25",
    "updated_at": "2025-04-26",
    "topics": [
      "deep-learning",
      "deep-learning-tutorial",
      "pytorch",
      "gan",
      "transformers",
      "reinforcement-learning",
      "optimizers",
      "neural-networks",
      "transformer",
      "machine-learning",
      "attention",
      "literate-programming",
      "lora"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 1280520,
      "Jupyter Notebook": 159576,
      "Makefile": 1984
    },
    "readme_content": "[![Twitter](https://img.shields.io/twitter/follow/labmlai?style=social)](https://twitter.com/labmlai)\n[![Sponsor](https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&color=%23fe8e86)](https://github.com/sponsors/labmlai)\n\n# [labml.ai Deep Learning Paper Implementations](https://nn.labml.ai/index.html)\n\nThis is a collection of simple PyTorch implementations of\nneural networks and related algorithms.\nThese implementations are documented with explanations,\n\n[The website](https://nn.labml.ai/index.html)\nrenders these as side-by-side formatted notes.\nWe believe these would help you understand these algorithms better.\n\n![Screenshot](https://nn.labml.ai/dqn-light.png)\n\nWe are actively maintaining this repo and adding new \nimplementations almost weekly.\n[![Twitter](https://img.shields.io/twitter/follow/labmlai?style=social)](https://twitter.com/labmlai) for updates.\n\n## Paper Implementations\n\n#### ✨ [Transformers](https://nn.labml.ai/transformers/index.html)\n\n* [Multi-headed attention](https://nn.labml.ai/transformers/mha.html)\n* [Transformer building blocks](https://nn.labml.ai/transformers/models.html) \n* [Transformer XL](https://nn.labml.ai/transformers/xl/index.html)\n    * [Relative multi-headed attention](https://nn.labml.ai/transformers/xl/relative_mha.html)\n* [Rotary Positional Embeddings](https://nn.labml.ai/transformers/rope/index.html)\n* [Attention with Linear Biases (ALiBi)](https://nn.labml.ai/transformers/alibi/index.html)\n* [RETRO](https://nn.labml.ai/transformers/retro/index.html)\n* [Compressive Transformer](https://nn.labml.ai/transformers/compressive/index.html)\n* [GPT Architecture](https://nn.labml.ai/transformers/gpt/index.html)\n* [GLU Variants](https://nn.labml.ai/transformers/glu_variants/simple.html)\n* [kNN-LM: Generalization through Memorization](https://nn.labml.ai/transformers/knn)\n* [Feedback Transformer](https://nn.labml.ai/transformers/feedback/index.html)\n* [Switch Transformer](https://nn.labml.ai/transformers/switch/index.html)\n* [Fast Weights Transformer](https://nn.labml.ai/transformers/fast_weights/index.html)\n* [FNet](https://nn.labml.ai/transformers/fnet/index.html)\n* [Attention Free Transformer](https://nn.labml.ai/transformers/aft/index.html)\n* [Masked Language Model](https://nn.labml.ai/transformers/mlm/index.html)\n* [MLP-Mixer: An all-MLP Architecture for Vision](https://nn.labml.ai/transformers/mlp_mixer/index.html)\n* [Pay Attention to MLPs (gMLP)](https://nn.labml.ai/transformers/gmlp/index.html)\n* [Vision Transformer (ViT)](https://nn.labml.ai/transformers/vit/index.html)\n* [Primer EZ](https://nn.labml.ai/transformers/primer_ez/index.html)\n* [Hourglass](https://nn.labml.ai/transformers/hour_glass/index.html)\n\n#### ✨ [Low-Rank Adaptation (LoRA)](https://nn.labml.ai/lora/index.html)\n\n#### ✨ [Eleuther GPT-NeoX](https://nn.labml.ai/neox/index.html)\n* [Generate on a 48GB GPU](https://nn.labml.ai/neox/samples/generate.html)\n* [Finetune on two 48GB GPUs](https://nn.labml.ai/neox/samples/finetune.html)\n* [LLM.int8()](https://nn.labml.ai/neox/utils/llm_int8.html)\n\n#### ✨ [Diffusion models](https://nn.labml.ai/diffusion/index.html)\n\n* [Denoising Diffusion Probabilistic Models (DDPM)](https://nn.labml.ai/diffusion/ddpm/index.html)\n* [Denoising Diffusion Implicit Models (DDIM)](https://nn.labml.ai/diffusion/stable_diffusion/sampler/ddim.html)\n* [Latent Diffusion Models](https://nn.labml.ai/diffusion/stable_diffusion/latent_diffusion.html)\n* [Stable Diffusion](https://nn.labml.ai/diffusion/stable_diffusion/index.html)\n\n#### ✨ [Generative Adversarial Networks](https://nn.labml.ai/gan/index.html)\n* [Original GAN](https://nn.labml.ai/gan/original/index.html)\n* [GAN with deep convolutional network](https://nn.labml.ai/gan/dcgan/index.html)\n* [Cycle GAN](https://nn.labml.ai/gan/cycle_gan/index.html)\n* [Wasserstein GAN](https://nn.labml.ai/gan/wasserstein/index.html)\n* [Wasserstein GAN with Gradient Penalty](https://nn.labml.ai/gan/wasserstein/gradient_penalty/index.html)\n* [StyleGAN 2](https://nn.labml.ai/gan/stylegan/index.html)\n\n#### ✨ [Recurrent Highway Networks](https://nn.labml.ai/recurrent_highway_networks/index.html)\n\n#### ✨ [LSTM](https://nn.labml.ai/lstm/index.html)\n\n#### ✨ [HyperNetworks - HyperLSTM](https://nn.labml.ai/hypernetworks/hyper_lstm.html)\n\n#### ✨ [ResNet](https://nn.labml.ai/resnet/index.html)\n\n#### ✨ [ConvMixer](https://nn.labml.ai/conv_mixer/index.html)\n\n#### ✨ [Capsule Networks](https://nn.labml.ai/capsule_networks/index.html)\n\n#### ✨ [U-Net](https://nn.labml.ai/unet/index.html)\n\n#### ✨ [Sketch RNN](https://nn.labml.ai/sketch_rnn/index.html)\n\n#### ✨ Graph Neural Networks\n\n* [Graph Attention Networks (GAT)](https://nn.labml.ai/graphs/gat/index.html)\n* [Graph Attention Networks v2 (GATv2)](https://nn.labml.ai/graphs/gatv2/index.html)\n\n#### ✨ [Counterfactual Regret Minimization (CFR)](https://nn.labml.ai/cfr/index.html)\n\nSolving games with incomplete information such as poker with CFR.\n\n* [Kuhn Poker](https://nn.labml.ai/cfr/kuhn/index.html)\n\n##...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "Prompt-Engineering-Guide",
    "full_name": "dair-ai/Prompt-Engineering-Guide",
    "description": "🐙 Guides, papers, lecture, notebooks and resources for prompt engineering",
    "url": "https://github.com/dair-ai/Prompt-Engineering-Guide",
    "stars": 55420,
    "forks": 5451,
    "created_at": "2022-12-16",
    "updated_at": "2025-04-26",
    "topics": [
      "deep-learning",
      "prompt-engineering",
      "openai",
      "chatgpt",
      "language-model",
      "generative-ai"
    ],
    "primary_language": "MDX",
    "languages": {
      "MDX": 4788109,
      "Jupyter Notebook": 207291,
      "TypeScript": 16197,
      "JavaScript": 5236,
      "CSS": 131
    },
    "readme_content": "# Prompt Engineering Guide\n\nPrompt engineering is a relatively new discipline for developing and optimizing prompts to efficiently use language models (LMs) for a wide variety of applications and research topics. Prompt engineering skills help to better understand the capabilities and limitations of large language models (LLMs). Researchers use prompt engineering to improve the capacity of LLMs on a wide range of common and complex tasks such as question answering and arithmetic reasoning. Developers use prompt engineering to design robust and effective prompting techniques that interface with LLMs and other tools.\n\nMotivated by the high interest in developing with LLMs, we have created this new prompt engineering guide that contains all the latest papers, learning guides, lectures, references, and tools related to prompt engineering for LLMs.\n\n🌐 [Prompt Engineering Guide (Web Version)](https://www.promptingguide.ai/)\n\n🎉 We are excited to launch our brand new prompt engineering courses under the DAIR.AI Academy. [Join Now](https://dair-ai.thinkific.com/bundles/pro)!\n\nUse code PROMPTING20 to get an extra 20% off.\n\nHappy Prompting!\n\n---\n## Announcements / Updates\n\n- 🎓 We now offer self-paced prompt engineering courses under our DAIR.AI Academy. [Join Now](https://dair-ai.thinkific.com/bundles/pro)! \n- 🎓 New course on Prompt Engineering for LLMs announced! [Enroll here](https://maven.com/dair-ai/prompt-engineering-llms)!\n- 💼 We now offer several [services](https://www.promptingguide.ai/services) like corporate training, consulting, and talks.\n- 🌐 We now support 13 languages! Welcoming more translations.\n- 👩‍🎓 We crossed 3 million learners in January 2024!\n- 🎉 We have launched a new web version of the guide [here](https://www.promptingguide.ai/)\n- 🔥 We reached #1 on Hacker News on 21 Feb 2023\n- 🎉 The First Prompt Engineering Lecture went live [here](https://youtu.be/dOxUroR57xs)\n\n[Join our Discord](https://discord.com/invite/SKgkVT8BGJ)\n\n[Follow us on Twitter](https://twitter.com/dair_ai)\n\n[Subscribe to our YouTube](https://www.youtube.com/channel/UCyna_OxOWL7IEuOwb7WhmxQ)\n\n[Subscribe to our Newsletter](https://nlpnews.substack.com/)\n\n---\n\n## Guides\nYou can also find the most up-to-date guides on our new website [https://www.promptingguide.ai/](https://www.promptingguide.ai/).\n\n- [Prompt Engineering - Introduction](https://www.promptingguide.ai/introduction)\n  - [Prompt Engineering - LLM Settings](https://www.promptingguide.ai/introduction/settings)\n  - [Prompt Engineering - Basics of Prompting](https://www.promptingguide.ai/introduction/basics)\n  - [Prompt Engineering - Prompt Elements](https://www.promptingguide.ai/introduction/elements)\n  - [Prompt Engineering - General Tips for Designing Prompts](https://www.promptingguide.ai/introduction/tips)\n  - [Prompt Engineering - Examples of Prompts](https://www.promptingguide.ai/introduction/examples)\n- [Prompt Engineering - Techniques](https://www.promptingguide.ai/techniques)\n  - [Prompt Engineering - Zero-Shot Prompting](https://www.promptingguide.ai/techniques/zeroshot)\n  - [Prompt Engineering - Few-Shot Prompting](https://www.promptingguide.ai/techniques/fewshot)\n  - [Prompt Engineering - Chain-of-Thought Prompting](https://www.promptingguide.ai/techniques/cot)\n  - [Prompt Engineering - Self-Consistency](https://www.promptingguide.ai/techniques/consistency)\n  - [Prompt Engineering - Generate Knowledge Prompting](https://www.promptingguide.ai/techniques/knowledge)\n  - [Prompt Engineering - Prompt Chaining](https://www.promptingguide.ai/techniques/prompt_chaining)\n  - [Prompt Engineering - Tree of Thoughts (ToT)](https://www.promptingguide.ai/techniques/tot)\n  - [Prompt Engineering - Retrieval Augmented Generation](https://www.promptingguide.ai/techniques/rag)\n  - [Prompt Engineering - Automatic Reasoning and Tool-use (ART)](https://www.promptingguide.ai/techniques/art)\n  - [Prompt Engineering - Automatic Prompt Engineer](https://www.promptingguide.ai/techniques/ape)\n  - [Prompt Engineering - Active-Prompt](https://www.promptingguide.ai/techniques/activeprompt)\n  - [Prompt Engineering - Directional Stimulus Prompting](https://www.promptingguide.ai/techniques/dsp)\n  - [Prompt Engineering - Program-Aided Language Models](https://www.promptingguide.ai/techniques/pal)\n  - [Prompt Engineering - ReAct Prompting](https://www.promptingguide.ai/techniques/react)\n  - [Prompt Engineering - Multimodal CoT Prompting](https://www.promptingguide.ai/techniques/multimodalcot)\n  - [Prompt Engineering - Graph Prompting](https://www.promptingguide.ai/techniques/graph)\n- [Prompt Engineering - Applications](https://www.promptingguide.ai/applications)\n  - [Prompt Engineering - Function Calling](https://www.promptingguide.ai/applications/function_calling)\n  - [Prompt Engineering - Generating Data](https://www.promptingguide.ai/applications/generating)\n  - [Prompt Engineering - Generating Synthetic Dataset for RAG](https://www.promptingguide.ai/applications/synthetic_rag)\n  - [Prompt Eng...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "AutoGPT",
    "full_name": "Significant-Gravitas/AutoGPT",
    "description": "AutoGPT is the vision of accessible AI for everyone, to use and to build on. Our mission is to provide the tools, so that you can focus on what matters.",
    "url": "https://github.com/Significant-Gravitas/AutoGPT",
    "stars": 174797,
    "forks": 45619,
    "created_at": "2023-03-16",
    "updated_at": "2025-04-26",
    "topics": [
      "ai",
      "gpt-4",
      "openai",
      "python",
      "artificial-intelligence",
      "autonomous-agents"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 2921679,
      "TypeScript": 1304132,
      "Dart": 203562,
      "PLpgSQL": 44427,
      "Jinja": 31626,
      "C++": 23419,
      "CMake": 18862,
      "JavaScript": 14039,
      "MDX": 12901,
      "CSS": 10849,
      "Shell": 5870,
      "Dockerfile": 4757,
      "HTML": 3023,
      "Ruby": 2803,
      "Swift": 2384,
      "C": 1425,
      "Elixir": 1068,
      "Batchfile": 478,
      "Kotlin": 140,
      "Objective-C": 38
    },
    "readme_content": "# AutoGPT: Build, Deploy, and Run AI Agents\n\n[![Discord Follow](https://dcbadge.vercel.app/api/server/autogpt?style=flat)](https://discord.gg/autogpt) &ensp;\n[![Twitter Follow](https://img.shields.io/twitter/follow/Auto_GPT?style=social)](https://twitter.com/Auto_GPT) &ensp;\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n**AutoGPT** is a powerful platform that allows you to create, deploy, and manage continuous AI agents that automate complex workflows. \n\n## Hosting Options \n   - Download to self-host\n   - [Join the Waitlist](https://bit.ly/3ZDijAI) for the cloud-hosted beta  \n\n## How to Setup for Self-Hosting\n> [!NOTE]\n> Setting up and hosting the AutoGPT Platform yourself is a technical process. \n> If you'd rather something that just works, we recommend [joining the waitlist](https://bit.ly/3ZDijAI) for the cloud-hosted beta.\n\n### Updated Setup Instructions:\nWe’ve moved to a fully maintained and regularly updated documentation site.\n\n👉 [Follow the official self-hosting guide here](https://docs.agpt.co/platform/getting-started/)\n\n\nThis tutorial assumes you have Docker, VSCode, git and npm installed.\n\n### 🧱 AutoGPT Frontend\n\nThe AutoGPT frontend is where users interact with our powerful AI automation platform. It offers multiple ways to engage with and leverage our AI agents. This is the interface where you'll bring your AI automation ideas to life:\n\n   **Agent Builder:** For those who want to customize, our intuitive, low-code interface allows you to design and configure your own AI agents. \n   \n   **Workflow Management:** Build, modify, and optimize your automation workflows with ease. You build your agent by connecting blocks, where each block     performs a single action.\n   \n   **Deployment Controls:** Manage the lifecycle of your agents, from testing to production.\n   \n   **Ready-to-Use Agents:** Don't want to build? Simply select from our library of pre-configured agents and put them to work immediately.\n   \n   **Agent Interaction:** Whether you've built your own or are using pre-configured agents, easily run and interact with them through our user-friendly      interface.\n\n   **Monitoring and Analytics:** Keep track of your agents' performance and gain insights to continually improve your automation processes.\n\n[Read this guide](https://docs.agpt.co/platform/new_blocks/) to learn how to build your own custom blocks.\n\n### 💽 AutoGPT Server\n\nThe AutoGPT Server is the powerhouse of our platform This is where your agents run. Once deployed, agents can be triggered by external sources and can operate continuously. It contains all the essential components that make AutoGPT run smoothly.\n\n   **Source Code:** The core logic that drives our agents and automation processes.\n   \n   **Infrastructure:** Robust systems that ensure reliable and scalable performance.\n   \n   **Marketplace:** A comprehensive marketplace where you can find and deploy a wide range of pre-built agents.\n\n### 🐙 Example Agents\n\nHere are two examples of what you can do with AutoGPT:\n\n1. **Generate Viral Videos from Trending Topics**\n   - This agent reads topics on Reddit.\n   - It identifies trending topics.\n   - It then automatically creates a short-form video based on the content. \n\n2. **Identify Top Quotes from Videos for Social Media**\n   - This agent subscribes to your YouTube channel.\n   - When you post a new video, it transcribes it.\n   - It uses AI to identify the most impactful quotes to generate a summary.\n   - Then, it writes a post to automatically publish to your social media. \n\nThese examples show just a glimpse of what you can achieve with AutoGPT! You can create customized workflows to build agents for any use case.\n\n---\n### Mission and Licencing\nOur mission is to provide the tools, so that you can focus on what matters:\n\n- 🏗️ **Building** - Lay the foundation for something amazing.\n- 🧪 **Testing** - Fine-tune your agent to perfection.\n- 🤝 **Delegating** - Let AI work for you, and have your ideas come to life.\n\nBe part of the revolution! **AutoGPT** is here to stay, at the forefront of AI innovation.\n\n**📖 [Documentation](https://docs.agpt.co)**\n&ensp;|&ensp;\n**🚀 [Contributing](CONTRIBUTING.md)**\n\n**Licensing:**\n\nMIT License: The majority of the AutoGPT repository is under the MIT License.\n\nPolyform Shield License: This license applies to the autogpt_platform folder. \n\nFor more information, see https://agpt.co/blog/introducing-the-autogpt-platform\n\n---\n## 🤖 AutoGPT Classic\n> Below is information about the classic version of AutoGPT.\n\n**🛠️ [Build your own Agent - Quickstart](classic/FORGE-QUICKSTART.md)**\n\n### 🏗️ Forge\n\n**Forge your own agent!** &ndash; Forge is a ready-to-go toolkit to build your own agent application. It handles most of the boilerplate code, letting you channel all your creativity into the things that set *your* agent apart. All tutorials are located [here](https://medium.com/@aiedge/autogpt-forge-e3de53cc58ec). Components from [`forge`](/classic/fo...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "OpenHands",
    "full_name": "All-Hands-AI/OpenHands",
    "description": "🙌 OpenHands: Code Less, Make More",
    "url": "https://github.com/All-Hands-AI/OpenHands",
    "stars": 53468,
    "forks": 5972,
    "created_at": "2024-03-13",
    "updated_at": "2025-04-26",
    "topics": [
      "agent",
      "artificial-intelligence",
      "llm",
      "chatgpt",
      "claude-ai",
      "cli",
      "developer-tools",
      "gpt",
      "openai"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 3455369,
      "TypeScript": 792679,
      "Shell": 80010,
      "JavaScript": 54168,
      "Jinja": 35541,
      "Makefile": 13364,
      "Dockerfile": 8780,
      "CSS": 2248,
      "HTML": 1849
    },
    "readme_content": "<a name=\"readme-top\"></a>\n\n<div align=\"center\">\n  <img src=\"./docs/static/img/logo.png\" alt=\"Logo\" width=\"200\">\n  <h1 align=\"center\">OpenHands: Code Less, Make More</h1>\n</div>\n\n\n<div align=\"center\">\n  <a href=\"https://github.com/All-Hands-AI/OpenHands/graphs/contributors\"><img src=\"https://img.shields.io/github/contributors/All-Hands-AI/OpenHands?style=for-the-badge&color=blue\" alt=\"Contributors\"></a>\n  <a href=\"https://github.com/All-Hands-AI/OpenHands/stargazers\"><img src=\"https://img.shields.io/github/stars/All-Hands-AI/OpenHands?style=for-the-badge&color=blue\" alt=\"Stargazers\"></a>\n  <a href=\"https://codecov.io/github/All-Hands-AI/OpenHands?branch=main\"><img alt=\"CodeCov\" src=\"https://img.shields.io/codecov/c/github/All-Hands-AI/OpenHands?style=for-the-badge&color=blue\"></a>\n  <a href=\"https://github.com/All-Hands-AI/OpenHands/blob/main/LICENSE\"><img src=\"https://img.shields.io/github/license/All-Hands-AI/OpenHands?style=for-the-badge&color=blue\" alt=\"MIT License\"></a>\n  <br/>\n  <a href=\"https://join.slack.com/t/openhands-ai/shared_invite/zt-2ngejmfw6-9gW4APWOC9XUp1n~SiQ6iw\"><img src=\"https://img.shields.io/badge/Slack-Join%20Us-red?logo=slack&logoColor=white&style=for-the-badge\" alt=\"Join our Slack community\"></a>\n  <a href=\"https://discord.gg/ESHStjSjD4\"><img src=\"https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&logoColor=white&style=for-the-badge\" alt=\"Join our Discord community\"></a>\n  <a href=\"https://github.com/All-Hands-AI/OpenHands/blob/main/CREDITS.md\"><img src=\"https://img.shields.io/badge/Project-Credits-blue?style=for-the-badge&color=FFE165&logo=github&logoColor=white\" alt=\"Credits\"></a>\n  <br/>\n  <a href=\"https://docs.all-hands.dev/modules/usage/getting-started\"><img src=\"https://img.shields.io/badge/Documentation-000?logo=googledocs&logoColor=FFE165&style=for-the-badge\" alt=\"Check out the documentation\"></a>\n  <a href=\"https://arxiv.org/abs/2407.16741\"><img src=\"https://img.shields.io/badge/Paper%20on%20Arxiv-000?logoColor=FFE165&logo=arxiv&style=for-the-badge\" alt=\"Paper on Arxiv\"></a>\n  <a href=\"https://docs.google.com/spreadsheets/d/1wOUdFCMyY6Nt0AIqF705KN4JKOWgeI4wUGUP60krXXs/edit?gid=0#gid=0\"><img src=\"https://img.shields.io/badge/Benchmark%20score-000?logoColor=FFE165&logo=huggingface&style=for-the-badge\" alt=\"Evaluation Benchmark Score\"></a>\n  <hr>\n</div>\n\nWelcome to OpenHands (formerly OpenDevin), a platform for software development agents powered by AI.\n\nOpenHands agents can do anything a human developer can: modify code, run commands, browse the web,\ncall APIs, and yes—even copy code snippets from StackOverflow.\n\nLearn more at [docs.all-hands.dev](https://docs.all-hands.dev), or [sign up for OpenHands Cloud](https://app.all-hands.dev) to get started.\n\n> [!IMPORTANT]\n> Using OpenHands for work? We'd love to chat! Fill out\n> [this short form](https://docs.google.com/forms/d/e/1FAIpQLSet3VbGaz8z32gW9Wm-Grl4jpt5WgMXPgJ4EDPVmCETCBpJtQ/viewform)\n> to join our Design Partner program, where you'll get early access to commercial features and the opportunity to provide input on our product roadmap.\n\n![App screenshot](./docs/static/img/screenshot.png)\n\n## ☁️ OpenHands Cloud\nThe easiest way to get started with OpenHands is on [OpenHands Cloud](https://app.all-hands.dev),\nwhich comes with $50 in free credits for new users.\n\n## 💻 Running OpenHands Locally\n\nOpenHands can also run on your local system using Docker.\nSee the [Running OpenHands](https://docs.all-hands.dev/modules/usage/installation) guide for\nsystem requirements and more information.\n\n> [!WARNING]\n> On a public network? See our [Hardened Docker Installation Guide](https://docs.all-hands.dev/modules/usage/runtimes/docker#hardened-docker-installation)\n> to secure your deployment by restricting network binding and implementing additional security measures.\n\n\n```bash\ndocker pull docker.all-hands.dev/all-hands-ai/runtime:0.34-nikolaik\n\ndocker run -it --rm --pull=always \\\n    -e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.34-nikolaik \\\n    -e LOG_ALL_EVENTS=true \\\n    -v /var/run/docker.sock:/var/run/docker.sock \\\n    -v ~/.openhands-state:/.openhands-state \\\n    -p 3000:3000 \\\n    --add-host host.docker.internal:host-gateway \\\n    --name openhands-app \\\n    docker.all-hands.dev/all-hands-ai/openhands:0.34\n```\n\nYou'll find OpenHands running at [http://localhost:3000](http://localhost:3000)!\n\nWhen you open the application, you'll be asked to choose an LLM provider and add an API key.\n[Anthropic's Claude 3.5 Sonnet](https://www.anthropic.com/api) (`anthropic/claude-3-5-sonnet-20241022`)\nworks best, but you have [many options](https://docs.all-hands.dev/modules/usage/llms).\n\n## 💡 Other ways to run OpenHands\n\n> [!CAUTION]\n> OpenHands is meant to be run by a single user on their local workstation.\n> It is not appropriate for multi-tenant deployments where multiple users share the same instance. There is no built-in authentication, isolation, or scalability.\n>\n> If you're interested in running...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "Deep-Live-Cam",
    "full_name": "hacksider/Deep-Live-Cam",
    "description": "real time face swap and one-click video deepfake with only a single image",
    "url": "https://github.com/hacksider/Deep-Live-Cam",
    "stars": 50709,
    "forks": 7536,
    "created_at": "2023-09-24",
    "updated_at": "2025-04-26",
    "topics": [
      "ai-face",
      "deepfake",
      "video-deepfake",
      "deepfake-webcam",
      "fake-webcam",
      "realtime-deepfake",
      "realtime-face-changer",
      "faceswap",
      "webcam",
      "ai-deep-fake",
      "ai-webcam",
      "ai",
      "artificial-intelligence",
      "webcamera",
      "deep-fake",
      "real-time-deepfake",
      "gan",
      "realtime"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 104119,
      "Batchfile": 79
    },
    "readme_content": "<h1 align=\"center\">Deep-Live-Cam</h1>\n\n<p align=\"center\">\n  Real-time face swap and video deepfake with a single click and only a single image.\n</p>\n\n<p align=\"center\">\n<a href=\"https://trendshift.io/repositories/11395\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/11395\" alt=\"hacksider%2FDeep-Live-Cam | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n</p>\n\n<p align=\"center\">\n  <img src=\"media/demo.gif\" alt=\"Demo GIF\" width=\"800\">\n</p>\n\n##  Disclaimer\n\nThis deepfake software is designed to be a productive tool for the AI-generated media industry. It can assist artists in animating custom characters, creating engaging content, and even using models for clothing design.\n\nWe are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to the law and ethics. We may shut down the project or add watermarks if legally required.\n\n- Ethical Use: Users are expected to use this software responsibly and legally. If using a real person's face, obtain their consent and clearly label any output as a deepfake when sharing online.\n\n- Content Restrictions: The software includes built-in checks to prevent processing inappropriate media, such as nudity, graphic content, or sensitive material.\n\n- Legal Compliance: We adhere to all relevant laws and ethical guidelines. If legally required, we may shut down the project or add watermarks to the output.\n\n- User Responsibility: We are not responsible for end-user actions. Users must ensure their use of the software aligns with ethical standards and legal requirements.\n\nBy using this software, you agree to these terms and commit to using it in a manner that respects the rights and dignity of others.\n\nUsers are expected to use this software responsibly and legally. If using a real person's face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user actions.\n\n## Exclusive v2.0 Quick Start - Pre-built (Windows / Nvidia)\n\n  <a href=\"https://deeplivecam.net/index.php/quickstart\"> <img src=\"https://github.com/user-attachments/assets/7d993b32-e3e8-4cd3-bbfb-a549152ebdd5\" width=\"285\" height=\"77\" />\n\n##### This is the fastest build you can get if you have a discrete NVIDIA GPU.\n \n###### These Pre-builts are perfect for non-technical users or those who don't have time to, or can't manually install all the requirements. Just a heads-up: this is an open-source project, so you can also install it manually. This will be 60 days ahead on the open source version.\n\n## TLDR; Live Deepfake in just 3 Clicks\n![easysteps](https://github.com/user-attachments/assets/af825228-852c-411b-b787-ffd9aac72fc6)\n1. Select a face\n2. Select which camera to use\n3. Press live!\n\n## Features & Uses - Everything is in real-time\n\n### Mouth Mask\n\n**Retain your original mouth for accurate movement using Mouth Mask**\n\n<p align=\"center\">\n  <img src=\"media/ludwig.gif\" alt=\"resizable-gif\">\n</p>\n\n### Face Mapping\n\n**Use different faces on multiple subjects simultaneously**\n\n<p align=\"center\">\n  <img src=\"media/streamers.gif\" alt=\"face_mapping_source\">\n</p>\n\n### Your Movie, Your Face\n\n**Watch movies with any face in real-time**\n\n<p align=\"center\">\n  <img src=\"media/movie.gif\" alt=\"movie\">\n</p>\n\n### Live Show\n\n**Run Live shows and performances**\n\n<p align=\"center\">\n  <img src=\"media/live_show.gif\" alt=\"show\">\n</p>\n\n### Memes\n\n**Create Your Most Viral Meme Yet**\n\n<p align=\"center\">\n  <img src=\"media/meme.gif\" alt=\"show\" width=\"450\"> \n  <br>\n  <sub>Created using Many Faces feature in Deep-Live-Cam</sub>\n</p>\n\n### Omegle\n\n**Surprise people on Omegle**\n\n<p align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/2e9b9b82-fa04-4b70-9f56-b1f68e7672d0\" width=\"450\" controls></video>\n</p>\n\n## Installation (Manual)\n\n**Please be aware that the installation requires technical skills and is not for beginners. Consider downloading the prebuilt version.**\n\n<details>\n<summary>Click to see the process</summary>\n\n### Installation\n\nThis is more likely to work on your computer but will be slower as it utilizes the CPU.\n\n**1. Set up Your Platform**\n\n-   Python (3.10 recommended)\n-   pip\n-   git\n-   [ffmpeg](https://www.youtube.com/watch?v=OlNWCpFdVMA) - ```iex (irm ffmpeg.tc.ht)```\n-   [Visual Studio 2022 Runtimes (Windows)](https://visualstudio.microsoft.com/visual-cpp-build-tools/)\n\n**2. Clone the Repository**\n\n```bash\ngit clone https://github.com/hacksider/Deep-Live-Cam.git\ncd Deep-Live-Cam\n```\n\n**3. Download the Models**\n\n1. [GFPGANv1.4](https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth)\n2. [inswapper\\_128\\_fp16.onnx](https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx)\n\nPlace these files in the \"**models**\" folder.\n\n*...",
    "owner_type": "User",
    "is_archived": false
  },
  {
    "name": "LLMs-from-scratch",
    "full_name": "rasbt/LLMs-from-scratch",
    "description": "Implement a ChatGPT-like LLM in PyTorch from scratch, step by step",
    "url": "https://github.com/rasbt/LLMs-from-scratch",
    "stars": 46420,
    "forks": 6556,
    "created_at": "2023-07-23",
    "updated_at": "2025-04-26",
    "topics": [
      "chatgpt",
      "gpt",
      "large-language-models",
      "llm",
      "python",
      "pytorch",
      "ai",
      "artificial-intelligence",
      "language-model",
      "transformer"
    ],
    "primary_language": "Jupyter Notebook",
    "languages": {
      "Jupyter Notebook": 2295066,
      "Python": 586422,
      "Dockerfile": 532
    },
    "readme_content": "# Build a Large Language Model (From Scratch)\n\nThis repository contains the code for developing, pretraining, and finetuning a GPT-like LLM and is the official code repository for the book [Build a Large Language Model (From Scratch)](https://amzn.to/4fqvn0D).\n\n<br>\n<br>\n\n<a href=\"https://amzn.to/4fqvn0D\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover.jpg?123\" width=\"250px\"></a>\n\n<br>\n\nIn [*Build a Large Language Model (From Scratch)*](http://mng.bz/orYv), you'll learn and understand how large language models (LLMs) work from the inside out by coding them from the ground up, step by step. In this book, I'll guide you through creating your own LLM, explaining each stage with clear text, diagrams, and examples.\n\nThe method described in this book for training and developing your own small-but-functional model for educational purposes mirrors the approach used in creating large-scale foundational models such as those behind ChatGPT. In addition, this book includes code for loading the weights of larger pretrained models for finetuning.\n\n- Link to the official [source code repository](https://github.com/rasbt/LLMs-from-scratch)\n- [Link to the book at Manning (the publisher's website)](http://mng.bz/orYv)\n- [Link to the book page on Amazon.com](https://www.amazon.com/gp/product/1633437167)\n- ISBN 9781633437166\n\n<a href=\"http://mng.bz/orYv#reviews\"><img src=\"https://sebastianraschka.com//images/LLMs-from-scratch-images/other/reviews.png\" width=\"220px\"></a>\n\n\n<br>\n<br>\n\nTo download a copy of this repository, click on the [Download ZIP](https://github.com/rasbt/LLMs-from-scratch/archive/refs/heads/main.zip) button or execute the following command in your terminal:\n\n```bash\ngit clone --depth 1 https://github.com/rasbt/LLMs-from-scratch.git\n```\n\n<br>\n\n(If you downloaded the code bundle from the Manning website, please consider visiting the official code repository on GitHub at [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch) for the latest updates.)\n\n<br>\n<br>\n\n\n# Table of Contents\n\nPlease note that this `README.md` file is a Markdown (`.md`) file. If you have downloaded this code bundle from the Manning website and are viewing it on your local computer, I recommend using a Markdown editor or previewer for proper viewing. If you haven't installed a Markdown editor yet, [MarkText](https://www.marktext.cc) is a good free option.\n\nYou can alternatively view this and other files on GitHub at [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch) in your browser, which renders Markdown automatically.\n\n<br>\n<br>\n<!--  -->\n\n> **Tip:**\n> If you're seeking guidance on installing Python and Python packages and setting up your code environment, I suggest reading the [README.md](setup/README.md) file located in the [setup](setup) directory.\n\n<br>\n<br>\n\n[![Code tests Linux](https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-linux-uv.yml/badge.svg)](https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-linux-uv.yml)\n[![Code tests Windows](https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-windows-uv-pip.yml/badge.svg)](https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-windows-uv-pip.yml)\n[![Code tests macOS](https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-macos-uv.yml/badge.svg)](https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-macos-uv.yml)\n\n\n\n\n<br>\n\n| Chapter Title                                              | Main Code (for Quick Access)                                                                                                    | All Code + Supplementary      |\n|------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------|-------------------------------|\n| [Setup recommendations](setup)                             | -                                                                                                                               | -                             |\n| Ch 1: Understanding Large Language Models                  | No code                                                                                                                         | -                             |\n| Ch 2: Working with Text Data                               | - [ch02.ipynb](ch02/01_main-chapter-code/ch02.ipynb)<br/>- [dataloader.ipynb](ch02/01_main-chapter-code/dataloader.ipynb) (summary)<br/>- [exercise-solutions.ipynb](ch02/01_main-chapter-code/exercise-solutions.ipynb)               | [./ch02](./ch02)            |\n| Ch 3: Coding Attention Mechanisms                          | - [ch03.ipynb](ch03/01_main-chapter-code/ch03.ipynb)<br/>- [multihead-attention.ipynb](ch03/01_main-chapter-code/multihead-attention.ipynb) (summary) <br/>- [exercise-solutions.ipynb](c...",
    "owner_type": "User",
    "is_archived": false
  },
  {
    "name": "kong",
    "full_name": "Kong/kong",
    "description": "🦍 The Cloud-Native API Gateway and AI Gateway.",
    "url": "https://github.com/Kong/kong",
    "stars": 40688,
    "forks": 4915,
    "created_at": "2014-11-17",
    "updated_at": "2025-04-26",
    "topics": [
      "api-gateway",
      "nginx",
      "luajit",
      "microservices",
      "api-management",
      "serverless",
      "apis",
      "consul",
      "docker",
      "reverse-proxy",
      "cloud-native",
      "microservice",
      "kong",
      "devops",
      "kubernetes",
      "kubernetes-ingress-controller",
      "kubernetes-ingress",
      "ai",
      "artificial-intelligence",
      "ai-gateway"
    ],
    "primary_language": "Lua",
    "languages": {
      "Lua": 9932935,
      "Perl": 587765,
      "Raku": 343862,
      "Starlark": 109185,
      "Shell": 96692,
      "Python": 46550,
      "Makefile": 9727,
      "CSS": 5812,
      "Dockerfile": 3847,
      "Linker Script": 652
    },
    "readme_content": "[![][kong-logo]][kong-url]\n\n![Stars](https://img.shields.io/github/stars/Kong/kong?style=flat-square) ![GitHub commit activity](https://img.shields.io/github/commit-activity/m/Kong/kong?style=flat-square) ![Docker Pulls](https://img.shields.io/docker/pulls/_/kong?style=flat-square) [![Build Status][badge-action-image]][badge-action-url] ![Version](https://img.shields.io/github/v/release/Kong/kong?color=green&label=Version&style=flat-square)  ![License](https://img.shields.io/badge/License-Apache%202.0-blue?style=flat-square)  ![Twitter Follow](https://img.shields.io/twitter/follow/thekonginc?style=social)\n\n\n**Kong** or **Kong API Gateway** is a cloud-native, platform-agnostic, scalable API Gateway distinguished for its high performance and extensibility via plugins. It also provides advanced AI capabilities with multi-LLM support.\n\nBy providing functionality for proxying, routing, load balancing, health checking, authentication (and [more](#features)), Kong serves as the central layer for orchestrating microservices or conventional API traffic with ease.\n\nKong runs natively on Kubernetes thanks to its official [Kubernetes Ingress Controller](https://github.com/Kong/kubernetes-ingress-controller).\n\n---\n\n[Installation](https://konghq.com/install/#kong-community) | [Documentation](https://docs.konghq.com) | [Discussions](https://github.com/Kong/kong/discussions) | [Forum](https://discuss.konghq.com) | [Blog](https://konghq.com/blog) | [Builds][kong-master-builds] | [Cloud Hosted Kong](https://konghq.com/kong-konnect/)\n\n---\n\n## Getting Started\n\nIf you prefer to use a cloud-hosted Kong, you can [sign up for a free trial of Kong Konnect](https://konghq.com/products/kong-konnect/register?utm_medium=Referral&utm_source=Github&utm_campaign=kong-gateway&utm_content=konnect-promo-in-gateway&utm_term=get-started) and get started in minutes. If not, you can follow the instructions below to get started with Kong on your own infrastructure.\n\nLet’s test drive Kong by adding authentication to an API in under 5 minutes.\n\nWe suggest using the docker-compose distribution via the instructions below, but there is also a [docker installation](https://docs.konghq.com/gateway/latest/install/docker/#install-kong-gateway-in-db-less-mode) procedure if you’d prefer to run the Kong API Gateway in DB-less mode.\n\nWhether you’re running in the cloud, on bare metal, or using containers, you can find every supported distribution on our [official installation](https://konghq.com/install/#kong-community) page.\n\n1) To start, clone the Docker repository and navigate to the compose folder.\n```cmd\n  $ git clone https://github.com/Kong/docker-kong\n  $ cd docker-kong/compose/\n```\n\n2) Start the Gateway stack using:\n```cmd\n  $ KONG_DATABASE=postgres docker-compose --profile database up\n```\n\nThe Gateway is now available on the following ports on localhost:\n\n- `:8000` - send traffic to your service via Kong\n- `:8001` - configure Kong using Admin API or via [decK](https://github.com/kong/deck)\n- `:8002` - access Kong's management Web UI ([Kong Manager](https://github.com/Kong/kong-manager)) on [localhost:8002](http://localhost:8002)\n\nNext, follow the [quick start guide](https://docs.konghq.com/gateway-oss/latest/getting-started/configuring-a-service/\n) to tour the Gateway features.\n\n## Features\n\nBy centralizing common API functionality across all your organization's services, the Kong API Gateway creates more freedom for engineering teams to focus on the challenges that matter most.\n\nThe top Kong features include:\n- Advanced routing, load balancing, health checking - all configurable via a RESTful admin API or declarative configuration.\n- Authentication and authorization for APIs using methods like JWT, basic auth, OAuth, ACLs and more.\n- Proxy, SSL/TLS termination, and connectivity support for L4 or L7 traffic.\n- Plugins for enforcing traffic controls, rate limiting, req/res transformations, logging, monitoring and including a plugin developer hub.\n- Plugins for AI traffic to support multi-LLM implementations and no-code AI use cases, with advanced AI prompt engineering, AI observability, AI security and more.\n- Sophisticated deployment models like Declarative Databaseless Deployment and Hybrid Deployment (control plane/data plane separation) without any vendor lock-in.\n- Native [ingress controller](https://github.com/Kong/kubernetes-ingress-controller) support for serving Kubernetes.\n\n[![][kong-benefits]][kong-url]\n\n### Plugin Hub\nPlugins provide advanced functionality that extends the use of the Gateway. Many of the Kong Inc. and community-developed plugins like AWS Lambda, Correlation ID, and Response Transformer are showcased at the [Plugin Hub](https://docs.konghq.com/hub/).\n\nContribute to the Plugin Hub and ensure your next innovative idea is published and available to the broader community!\n\n## Contributing\n\nWe ❤️ pull requests, and we’re continually working hard to make it as easy as possible for developers to contribute. Before beginning develo...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "Flowise",
    "full_name": "FlowiseAI/Flowise",
    "description": "Drag & drop UI to build your customized LLM flow",
    "url": "https://github.com/FlowiseAI/Flowise",
    "stars": 37669,
    "forks": 19586,
    "created_at": "2023-03-31",
    "updated_at": "2025-04-26",
    "topics": [
      "artificial-intelligence",
      "chatgpt",
      "large-language-models",
      "low-code",
      "no-code",
      "javascript",
      "react",
      "typescript",
      "langchain",
      "llamaindex",
      "openai",
      "rag",
      "chatbot",
      "workflow-automation"
    ],
    "primary_language": "TypeScript",
    "languages": {
      "TypeScript": 3178767,
      "JavaScript": 1890414,
      "CSS": 11939,
      "SCSS": 6240,
      "HTML": 5645,
      "Dockerfile": 1505,
      "Shell": 195,
      "Batchfile": 60
    },
    "readme_content": "<!-- markdownlint-disable MD030 -->\n\n<img width=\"100%\" src=\"https://github.com/FlowiseAI/Flowise/blob/main/images/flowise.png?raw=true\"></a>\n\n# Flowise - Build LLM Apps Easily\n\n[![Release Notes](https://img.shields.io/github/release/FlowiseAI/Flowise)](https://github.com/FlowiseAI/Flowise/releases)\n[![Discord](https://img.shields.io/discord/1087698854775881778?label=Discord&logo=discord)](https://discord.gg/jbaHfsRVBW)\n[![Twitter Follow](https://img.shields.io/twitter/follow/FlowiseAI?style=social)](https://twitter.com/FlowiseAI)\n[![GitHub star chart](https://img.shields.io/github/stars/FlowiseAI/Flowise?style=social)](https://star-history.com/#FlowiseAI/Flowise)\n[![GitHub fork](https://img.shields.io/github/forks/FlowiseAI/Flowise?style=social)](https://github.com/FlowiseAI/Flowise/fork)\n\nEnglish | [繁體中文](./i18n/README-TW.md) | [简体中文](./i18n/README-ZH.md) | [日本語](./i18n/README-JA.md) | [한국어](./i18n/README-KR.md)\n\n<h3>Drag & drop UI to build your customized LLM flow</h3>\n<a href=\"https://github.com/FlowiseAI/Flowise\">\n<img width=\"100%\" src=\"https://github.com/FlowiseAI/Flowise/blob/main/images/flowise.gif?raw=true\"></a>\n\n## ⚡Quick Start\n\nDownload and Install [NodeJS](https://nodejs.org/en/download) >= 18.15.0\n\n1. Install Flowise\n    ```bash\n    npm install -g flowise\n    ```\n2. Start Flowise\n\n    ```bash\n    npx flowise start\n    ```\n\n    With username & password\n\n    ```bash\n    npx flowise start --FLOWISE_USERNAME=user --FLOWISE_PASSWORD=1234\n    ```\n\n3. Open [http://localhost:3000](http://localhost:3000)\n\n## 🐳 Docker\n\n### Docker Compose\n\n1. Clone the Flowise project\n2. Go to `docker` folder at the root of the project\n3. Copy `.env.example` file, paste it into the same location, and rename to `.env` file\n4. `docker compose up -d`\n5. Open [http://localhost:3000](http://localhost:3000)\n6. You can bring the containers down by `docker compose stop`\n\n### Docker Image\n\n1. Build the image locally:\n    ```bash\n    docker build --no-cache -t flowise .\n    ```\n2. Run image:\n\n    ```bash\n    docker run -d --name flowise -p 3000:3000 flowise\n    ```\n\n3. Stop image:\n    ```bash\n    docker stop flowise\n    ```\n\n## 👨‍💻 Developers\n\nFlowise has 3 different modules in a single mono repository.\n\n-   `server`: Node backend to serve API logics\n-   `ui`: React frontend\n-   `components`: Third-party nodes integrations\n-   `api-documentation`: Auto-generated swagger-ui API docs from express\n\n### Prerequisite\n\n-   Install [PNPM](https://pnpm.io/installation)\n    ```bash\n    npm i -g pnpm\n    ```\n\n### Setup\n\n1.  Clone the repository\n\n    ```bash\n    git clone https://github.com/FlowiseAI/Flowise.git\n    ```\n\n2.  Go into repository folder\n\n    ```bash\n    cd Flowise\n    ```\n\n3.  Install all dependencies of all modules:\n\n    ```bash\n    pnpm install\n    ```\n\n4.  Build all the code:\n\n    ```bash\n    pnpm build\n    ```\n\n    <details>\n    <summary>Exit code 134 (JavaScript heap out of memory)</summary>  \n      If you get this error when running the above `build` script, try increasing the Node.js heap size and run the script again:\n\n        export NODE_OPTIONS=\"--max-old-space-size=4096\"\n        pnpm build\n\n    </details>\n\n5.  Start the app:\n\n    ```bash\n    pnpm start\n    ```\n\n    You can now access the app on [http://localhost:3000](http://localhost:3000)\n\n6.  For development build:\n\n    -   Create `.env` file and specify the `VITE_PORT` (refer to `.env.example`) in `packages/ui`\n    -   Create `.env` file and specify the `PORT` (refer to `.env.example`) in `packages/server`\n    -   Run\n\n        ```bash\n        pnpm dev\n        ```\n\n    Any code changes will reload the app automatically on [http://localhost:8080](http://localhost:8080)\n\n## 🔒 Authentication\n\nTo enable app level authentication, add `FLOWISE_USERNAME` and `FLOWISE_PASSWORD` to the `.env` file in `packages/server`:\n\n```\nFLOWISE_USERNAME=user\nFLOWISE_PASSWORD=1234\n```\n\n## 🌱 Env Variables\n\nFlowise support different environment variables to configure your instance. You can specify the following variables in the `.env` file inside `packages/server` folder. Read [more](https://github.com/FlowiseAI/Flowise/blob/main/CONTRIBUTING.md#-env-variables)\n\n## 📖 Documentation\n\n[Flowise Docs](https://docs.flowiseai.com/)\n\n## 🌐 Self Host\n\nDeploy Flowise self-hosted in your existing infrastructure, we support various [deployments](https://docs.flowiseai.com/configuration/deployment)\n\n-   [AWS](https://docs.flowiseai.com/configuration/deployment/aws)\n-   [Azure](https://docs.flowiseai.com/configuration/deployment/azure)\n-   [Digital Ocean](https://docs.flowiseai.com/configuration/deployment/digital-ocean)\n-   [GCP](https://docs.flowiseai.com/configuration/deployment/gcp)\n-   [Alibaba Cloud](https://computenest.console.aliyun.com/service/instance/create/default?type=user&ServiceName=Flowise社区版)\n-   <details>\n      <summary>Others</summary>\n\n    -   [Railway](https://docs.flowiseai.com/configuration/deployment/railway)\n\n        [![Deploy on Railway](https://railway.app/button.svg)](https://r...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "AI-For-Beginners",
    "full_name": "microsoft/AI-For-Beginners",
    "description": "12 Weeks, 24 Lessons, AI for All!",
    "url": "https://github.com/microsoft/AI-For-Beginners",
    "stars": 37201,
    "forks": 6807,
    "created_at": "2021-03-03",
    "updated_at": "2025-04-26",
    "topics": [
      "deep-learning",
      "artificial-intelligence",
      "machine-learning",
      "ai",
      "computer-vision",
      "nlp",
      "cnn",
      "rnn",
      "gan",
      "microsoft-for-beginners"
    ],
    "primary_language": "Jupyter Notebook",
    "languages": {
      "Jupyter Notebook": 26193652,
      "Python": 36131,
      "HTML": 13907,
      "Vue": 5103,
      "JavaScript": 2433,
      "Dockerfile": 1006,
      "Shell": 921
    },
    "readme_content": "[![GitHub license](https://img.shields.io/github/license/microsoft/AI-For-Beginners.svg)](https://github.com/microsoft/AI-For-Beginners/blob/main/LICENSE)\n[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/AI-For-Beginners.svg)](https://GitHub.com/microsoft/AI-For-Beginners/graphs/contributors/)\n[![GitHub issues](https://img.shields.io/github/issues/microsoft/AI-For-Beginners.svg)](https://GitHub.com/microsoft/AI-For-Beginners/issues/)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/AI-For-Beginners.svg)](https://GitHub.com/microsoft/AI-For-Beginners/pulls/)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)\n\n[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/AI-For-Beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/AI-For-Beginners/watchers/)\n[![GitHub forks](https://img.shields.io/github/forks/microsoft/AI-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/AI-For-Beginners/network/)\n[![GitHub stars](https://img.shields.io/github/stars/microsoft/AI-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/AI-For-Beginners/stargazers/)\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/microsoft/ai-for-beginners/HEAD)\n[![Gitter](https://badges.gitter.im/Microsoft/ai-for-beginners.svg)](https://gitter.im/Microsoft/ai-for-beginners?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n\n[![](https://dcbadge.vercel.app/api/server/ByRwuEEgH4)](https://discord.gg/zxKYvhSnVp?WT.mc_id=academic-000002-leestott)\n\n# Artificial Intelligence for Beginners - A Curriculum\n\n|![ Sketchnote by [(@girlie_mac)](https://twitter.com/girlie_mac) ](./lessons/sketchnotes/ai-overview.png)|\n|:---:|\n| AI For Beginners - _Sketchnote by [@girlie_mac](https://twitter.com/girlie_mac)_ |\n\nExplore the world of **Artificial Intelligence** (AI) with our 12-week, 24-lesson curriculum!  It includes practical lessons, quizzes, and labs. The curriculum is beginner-friendly and covers tools like TensorFlow and PyTorch, as well as ethics in AI\n\n## What you will learn\n\n**[Mindmap of the Course](http://soshnikov.com/courses/ai-for-beginners/mindmap.html)**\n\nIn this curriculum, you will learn:\n\n* Different approaches to Artificial Intelligence, including the \"good old\" symbolic approach with **Knowledge Representation** and reasoning ([GOFAI](https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence)).\n* **Neural Networks** and **Deep Learning**, which are at the core of modern AI. We will illustrate the concepts behind these important topics using code in two of the most popular frameworks - [TensorFlow](http://Tensorflow.org) and [PyTorch](http://pytorch.org).\n* **Neural Architectures** for working with images and text. We will cover recent models but may be a bit lacking in the state-of-the-art.\n* Less popular AI approaches, such as **Genetic Algorithms** and **Multi-Agent Systems**.\n\nWhat we will not cover in this curriculum:\n\n> [Find all additional resources for this course in our Microsoft Learn collection](https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum)\n\n* Business cases of using **AI in Business**. Consider taking [Introduction to AI for business users](https://docs.microsoft.com/learn/paths/introduction-ai-for-business-users/?WT.mc_id=academic-77998-bethanycheum) learning path on Microsoft Learn, or [AI Business School](https://www.microsoft.com/ai/ai-business-school/?WT.mc_id=academic-77998-bethanycheum), developed in cooperation with [INSEAD](https://www.insead.edu/).\n* **Classic Machine Learning**, which is well described in our [Machine Learning for Beginners Curriculum](http://github.com/Microsoft/ML-for-Beginners).\n* Practical AI applications built using **[Cognitive Services](https://azure.microsoft.com/services/cognitive-services/?WT.mc_id=academic-77998-bethanycheum)**. For this, we recommend that you start with modules Microsoft Learn for [vision](https://docs.microsoft.com/learn/paths/create-computer-vision-solutions-azure-cognitive-services/?WT.mc_id=academic-77998-bethanycheum), [natural language processing](https://docs.microsoft.com/learn/paths/explore-natural-language-processing/?WT.mc_id=academic-77998-bethanycheum), **[Generative AI with Azure OpenAI Service](https://learn.microsoft.com/en-us/training/paths/develop-ai-solutions-azure-openai/?WT.mc_id=academic-77998-bethanycheum)** and others.\n* Specific ML **Cloud Frameworks**, such as [Azure Machine Learning](https://azure.microsoft.com/services/machine-learning/?WT.mc_id=academic-77998-bethanycheum), [Microsoft Fabric](https://learn.microsoft.com/en-us/training/paths/get-started-fabric/?WT.mc_id=academic-77998-bethanycheum), or [Azure Databricks](https://docs.microsoft.com/learn/paths/data-engineer-azure-databricks?WT.mc_id=academic-77998-bethanycheum). Consider using [Build and operate machine l...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "DragGAN",
    "full_name": "XingangPan/DragGAN",
    "description": "Official Code for DragGAN (SIGGRAPH 2023)",
    "url": "https://github.com/XingangPan/DragGAN",
    "stars": 35912,
    "forks": 3446,
    "created_at": "2023-05-18",
    "updated_at": "2025-04-26",
    "topics": [
      "artificial-intelligence",
      "generative-adversarial-network",
      "generative-models",
      "image-manipulation"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 1193439,
      "Cuda": 250786,
      "C++": 68635,
      "Dockerfile": 689,
      "Batchfile": 524,
      "Shell": 502
    },
    "readme_content": "<p align=\"center\">\n\n  <h1 align=\"center\">Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold</h1>\n  <p align=\"center\">\n    <a href=\"https://xingangpan.github.io/\"><strong>Xingang Pan</strong></a>\n    ·\n    <a href=\"https://ayushtewari.com/\"><strong>Ayush Tewari</strong></a>\n    ·\n    <a href=\"https://people.mpi-inf.mpg.de/~tleimkue/\"><strong>Thomas Leimkühler</strong></a>\n    ·\n    <a href=\"https://lingjie0206.github.io/\"><strong>Lingjie Liu</strong></a>\n    ·\n    <a href=\"https://www.meka.page/\"><strong>Abhimitra Meka</strong></a>\n    ·\n    <a href=\"http://www.mpi-inf.mpg.de/~theobalt/\"><strong>Christian Theobalt</strong></a>\n  </p>\n  <h2 align=\"center\">SIGGRAPH 2023 Conference Proceedings</h2>\n  <div align=\"center\">\n    <img src=\"DragGAN.gif\", width=\"600\">\n  </div>\n\n  <p align=\"center\">\n  <br>\n    <a href=\"https://pytorch.org/get-started/locally/\"><img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white\"></a>\n    <a href=\"https://twitter.com/XingangP\"><img alt='Twitter' src=\"https://img.shields.io/twitter/follow/XingangP?label=%40XingangP\"></a>\n    <a href=\"https://arxiv.org/abs/2305.10973\">\n      <img src='https://img.shields.io/badge/Paper-PDF-green?style=for-the-badge&logo=adobeacrobatreader&logoWidth=20&logoColor=white&labelColor=66cc00&color=94DD15' alt='Paper PDF'>\n    </a>\n    <a href='https://vcai.mpi-inf.mpg.de/projects/DragGAN/'>\n      <img src='https://img.shields.io/badge/DragGAN-Page-orange?style=for-the-badge&logo=Google%20chrome&logoColor=white&labelColor=D35400' alt='Project Page'></a>\n    <a href=\"https://colab.research.google.com/drive/1mey-IXPwQC_qSthI5hO-LTX7QL4ivtPh?usp=sharing\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n  </p>\n</p>\n\n## Web Demos\n\n[![Open in OpenXLab](https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg)](https://openxlab.org.cn/apps/detail/XingangPan/DragGAN)\n\n<p align=\"left\">\n  <a href=\"https://huggingface.co/spaces/radames/DragGan\"><img alt=\"Huggingface\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DragGAN-orange\"></a>\n</p>\n\n## Requirements\n\nIf you have CUDA graphic card, please follow the requirements of [NVlabs/stylegan3](https://github.com/NVlabs/stylegan3#requirements).  \n\nThe usual installation steps involve the following commands, they should set up the correct CUDA version and all the python packages\n\n```\nconda env create -f environment.yml\nconda activate stylegan3\n```\n\nThen install the additional requirements\n\n```\npip install -r requirements.txt\n```\n\nOtherwise (for GPU acceleration on MacOS with Silicon Mac M1/M2, or just CPU) try the following:\n\n```sh\ncat environment.yml | \\\n  grep -v -E 'nvidia|cuda' > environment-no-nvidia.yml && \\\n    conda env create -f environment-no-nvidia.yml\nconda activate stylegan3\n\n# On MacOS\nexport PYTORCH_ENABLE_MPS_FALLBACK=1\n```\n\n## Run Gradio visualizer in Docker \n\nProvided docker image is based on NGC PyTorch repository. To quickly try out visualizer in Docker, run the following:  \n\n```sh\n# before you build the docker container, make sure you have cloned this repo, and downloaded the pretrained model by `python scripts/download_model.py`.\ndocker build . -t draggan:latest  \ndocker run -p 7860:7860 -v \"$PWD\":/workspace/src -it draggan:latest bash\n# (Use GPU)if you want to utilize your Nvidia gpu to accelerate in docker, please add command tag `--gpus all`, like:\n#   docker run --gpus all  -p 7860:7860 -v \"$PWD\":/workspace/src -it draggan:latest bash\n\ncd src && python visualizer_drag_gradio.py --listen\n```\nNow you can open a shared link from Gradio (printed in the terminal console).   \nBeware the Docker image takes about 25GB of disk space!\n\n## Download pre-trained StyleGAN2 weights\n\nTo download pre-trained weights, simply run:\n\n```\npython scripts/download_model.py\n```\nIf you want to try StyleGAN-Human and the Landscapes HQ (LHQ) dataset, please download weights from these links: [StyleGAN-Human](https://drive.google.com/file/d/1dlFEHbu-WzQWJl7nBBZYcTyo000H9hVm/view?usp=sharing), [LHQ](https://drive.google.com/file/d/16twEf0T9QINAEoMsWefoWiyhcTd-aiWc/view?usp=sharing), and put them under `./checkpoints`.\n\nFeel free to try other pretrained StyleGAN.\n\n## Run DragGAN GUI\n\nTo start the DragGAN GUI, simply run:\n```sh\nsh scripts/gui.sh\n```\nIf you are using windows, you can run:\n```\n.\\scripts\\gui.bat\n```\n\nThis GUI supports editing GAN-generated images. To edit a real image, you need to first perform GAN inversion using tools like [PTI](https://github.com/danielroich/PTI). Then load the new latent code and model weights to the GUI.\n\nYou can run DragGAN Gradio demo as well, this is universal for both windows and linux:\n```sh\npython visualizer_drag_gradio.py\n```\n\n## Acknowledgement\n\nThis code is developed based on [StyleGAN3](https://github.com/NVlabs/stylegan3). Part of the code is borrowed from [StyleGAN-Human](https://github.com/stylegan-human/StyleGAN-Human).\n\n(cheers to the communit...",
    "owner_type": "User",
    "is_archived": false
  },
  {
    "name": "spaCy",
    "full_name": "explosion/spaCy",
    "description": "💫 Industrial-strength Natural Language Processing (NLP) in Python",
    "url": "https://github.com/explosion/spaCy",
    "stars": 31463,
    "forks": 4497,
    "created_at": "2014-07-03",
    "updated_at": "2025-04-26",
    "topics": [
      "natural-language-processing",
      "data-science",
      "machine-learning",
      "python",
      "cython",
      "nlp",
      "artificial-intelligence",
      "ai",
      "spacy",
      "nlp-library",
      "neural-network",
      "neural-networks",
      "deep-learning",
      "named-entity-recognition",
      "entity-linking",
      "text-classification",
      "tokenization"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 4024898,
      "MDX": 2351173,
      "Cython": 804389,
      "JavaScript": 197404,
      "Sass": 57825,
      "TypeScript": 26865,
      "Jinja": 16352,
      "C": 9571,
      "HTML": 6927,
      "Makefile": 1512,
      "Shell": 981,
      "Dockerfile": 425,
      "C++": 187
    },
    "readme_content": "<a href=\"https://explosion.ai\"><img src=\"https://explosion.ai/assets/img/logo.svg\" width=\"125\" height=\"125\" align=\"right\" /></a>\n\n# spaCy: Industrial-strength NLP\n\nspaCy is a library for **advanced Natural Language Processing** in Python and\nCython. It's built on the very latest research, and was designed from day one to\nbe used in real products.\n\nspaCy comes with [pretrained pipelines](https://spacy.io/models) and currently\nsupports tokenization and training for **70+ languages**. It features\nstate-of-the-art speed and **neural network models** for tagging, parsing,\n**named entity recognition**, **text classification** and more, multi-task\nlearning with pretrained **transformers** like BERT, as well as a\nproduction-ready [**training system**](https://spacy.io/usage/training) and easy\nmodel packaging, deployment and workflow management. spaCy is commercial\nopen-source software, released under the\n[MIT license](https://github.com/explosion/spaCy/blob/master/LICENSE).\n\n💫 **Version 3.8 out now!**\n[Check out the release notes here.](https://github.com/explosion/spaCy/releases)\n\n[![tests](https://github.com/explosion/spaCy/actions/workflows/tests.yml/badge.svg)](https://github.com/explosion/spaCy/actions/workflows/tests.yml)\n[![Current Release Version](https://img.shields.io/github/release/explosion/spacy.svg?style=flat-square&logo=github)](https://github.com/explosion/spaCy/releases)\n[![pypi Version](https://img.shields.io/pypi/v/spacy.svg?style=flat-square&logo=pypi&logoColor=white)](https://pypi.org/project/spacy/)\n[![conda Version](https://img.shields.io/conda/vn/conda-forge/spacy.svg?style=flat-square&logo=conda-forge&logoColor=white)](https://anaconda.org/conda-forge/spacy)\n[![Python wheels](https://img.shields.io/badge/wheels-%E2%9C%93-4c1.svg?longCache=true&style=flat-square&logo=python&logoColor=white)](https://github.com/explosion/wheelwright/releases)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/ambv/black)\n<br />\n[![PyPi downloads](https://static.pepy.tech/personalized-badge/spacy?period=total&units=international_system&left_color=grey&right_color=orange&left_text=pip%20downloads)](https://pypi.org/project/spacy/)\n[![Conda downloads](https://img.shields.io/conda/dn/conda-forge/spacy?label=conda%20downloads)](https://anaconda.org/conda-forge/spacy)\n\n## 📖 Documentation\n\n| Documentation                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                              |\n| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| ⭐️ **[spaCy 101]**                                                                                                                                                                                                       | New to spaCy? Here's everything you need to know!                                                                                                                                                                                                                                                                                            |\n| 📚 **[Usage Guides]**                                                                                                                                                                                                     | How to use spaCy and its features.                                                                                                                                                                                                                                                                                                           |\n| 🚀 **[New in v3.0]**                                                                                                                                                                                                      | New features, backwards incompatibilities and migration guide.                                                                                                    ...",
    "owner_type": "Organization",
    "is_archived": false
  },
  {
    "name": "fairseq",
    "full_name": "facebookresearch/fairseq",
    "description": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python.",
    "url": "https://github.com/facebookresearch/fairseq",
    "stars": 31367,
    "forks": 6504,
    "created_at": "2017-08-29",
    "updated_at": "2025-04-26",
    "topics": [
      "python",
      "pytorch",
      "artificial-intelligence"
    ],
    "primary_language": "Python",
    "languages": {
      "Python": 4295063,
      "Cuda": 38178,
      "C++": 21106,
      "Cython": 13294,
      "Lua": 4210,
      "Shell": 2182
    },
    "readme_content": "<p align=\"center\">\n  <img src=\"docs/fairseq_logo.png\" width=\"150\">\n  <br />\n  <br />\n  <a href=\"https://opensource.fb.com/support-ukraine\"><img alt=\"Support Ukraine\" src=\"https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB\" /></a>\n  <a href=\"https://github.com/pytorch/fairseq/blob/main/LICENSE\"><img alt=\"MIT License\" src=\"https://img.shields.io/badge/license-MIT-blue.svg\" /></a>\n  <a href=\"https://github.com/pytorch/fairseq/releases\"><img alt=\"Latest Release\" src=\"https://img.shields.io/github/release/pytorch/fairseq.svg\" /></a>\n  <a href=\"https://github.com/pytorch/fairseq/actions?query=workflow:build\"><img alt=\"Build Status\" src=\"https://github.com/pytorch/fairseq/workflows/build/badge.svg\" /></a>\n  <a href=\"https://fairseq.readthedocs.io/en/latest/?badge=latest\"><img alt=\"Documentation Status\" src=\"https://readthedocs.org/projects/fairseq/badge/?version=latest\" /></a>\n  <a href=\"https://app.circleci.com/pipelines/github/facebookresearch/fairseq/\"><img alt=\"CicleCI Status\" src=\"https://circleci.com/gh/facebookresearch/fairseq.svg?style=shield\" /></a>\n</p>\n\n--------------------------------------------------------------------------------\n\nFairseq(-py) is a sequence modeling toolkit that allows researchers and\ndevelopers to train custom models for translation, summarization, language\nmodeling and other text generation tasks.\n\nWe provide reference implementations of various sequence modeling papers:\n\n<details><summary>List of implemented papers</summary><p>\n\n* **Convolutional Neural Networks (CNN)**\n  + [Language Modeling with Gated Convolutional Networks (Dauphin et al., 2017)](examples/language_model/conv_lm/README.md)\n  + [Convolutional Sequence to Sequence Learning (Gehring et al., 2017)](examples/conv_seq2seq/README.md)\n  + [Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al., 2018)](https://github.com/pytorch/fairseq/tree/classic_seqlevel)\n  + [Hierarchical Neural Story Generation (Fan et al., 2018)](examples/stories/README.md)\n  + [wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al., 2019)](examples/wav2vec/README.md)\n* **LightConv and DynamicConv models**\n  + [Pay Less Attention with Lightweight and Dynamic Convolutions (Wu et al., 2019)](examples/pay_less_attention_paper/README.md)\n* **Long Short-Term Memory (LSTM) networks**\n  + Effective Approaches to Attention-based Neural Machine Translation (Luong et al., 2015)\n* **Transformer (self-attention) networks**\n  + Attention Is All You Need (Vaswani et al., 2017)\n  + [Scaling Neural Machine Translation (Ott et al., 2018)](examples/scaling_nmt/README.md)\n  + [Understanding Back-Translation at Scale (Edunov et al., 2018)](examples/backtranslation/README.md)\n  + [Adaptive Input Representations for Neural Language Modeling (Baevski and Auli, 2018)](examples/language_model/README.adaptive_inputs.md)\n  + [Lexically constrained decoding with dynamic beam allocation (Post & Vilar, 2018)](examples/constrained_decoding/README.md)\n  + [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (Dai et al., 2019)](examples/truncated_bptt/README.md)\n  + [Adaptive Attention Span in Transformers (Sukhbaatar et al., 2019)](examples/adaptive_span/README.md)\n  + [Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al., 2019)](examples/translation_moe/README.md)\n  + [RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019)](examples/roberta/README.md)\n  + [Facebook FAIR's WMT19 News Translation Task Submission (Ng et al., 2019)](examples/wmt19/README.md)\n  + [Jointly Learning to Align and Translate with Transformer Models (Garg et al., 2019)](examples/joint_alignment_translation/README.md )\n  + [Multilingual Denoising Pre-training for Neural Machine Translation (Liu et at., 2020)](examples/mbart/README.md)\n  + [Neural Machine Translation with Byte-Level Subwords (Wang et al., 2020)](examples/byte_level_bpe/README.md)\n  + [Unsupervised Quality Estimation for Neural Machine Translation (Fomicheva et al., 2020)](examples/unsupervised_quality_estimation/README.md)\n  + [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (Baevski et al., 2020)](examples/wav2vec/README.md)\n  + [Generating Medical Reports from Patient-Doctor Conversations Using Sequence-to-Sequence Models (Enarvi et al., 2020)](examples/pointer_generator/README.md)\n  + [Linformer: Self-Attention with Linear Complexity (Wang et al., 2020)](examples/linformer/README.md)\n  + [Cross-lingual Retrieval for Iterative Self-Supervised Training (Tran et al., 2020)](examples/criss/README.md)\n  + [Deep Transformers with Latent Depth (Li et al., 2020)](examples/latent_depth/README.md)\n  + [Unsupervised Cross-lingual Representation Learning for Speech Recognition (Conneau et al., 2020)](https://arxiv.org/abs/2006.13979)\n  + [Self-training and Pre-training are Complementary for Speech Recognition (Xu et al., 2020)](https://arxiv.org/abs/2010.11430)\n  + [Ro...",
    "owner_type": "Organization",
    "is_archived": false
  }
]